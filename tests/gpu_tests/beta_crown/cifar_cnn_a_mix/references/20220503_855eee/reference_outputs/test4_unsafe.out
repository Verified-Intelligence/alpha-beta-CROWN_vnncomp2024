Building native CUDA modules...
/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
model:
  path: cifar_cnn_a_mix.model
  name: cnn_4layer
data:
  start: 48
  end: 49
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR_SDP
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: 0.00784313725
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 4096
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 20
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 200
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  use_auto_attack: false
  use_diversed_pgd: false
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 20:59:41 2022 on diablo.cs.ucla.edu
Sequential(
  (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (3): ReLU()
  (4): Flatten()
  (5): Linear(in_features=2048, out_features=100, bias=True)
  (6): ReLU()
  (7): Linear(in_features=100, out_features=10, bias=True)
)
############################
Sampled data loaded. Data already preprocessed!
Shape: torch.Size([200, 3, 32, 32]) torch.Size([200]) torch.Size([200])
X range: tensor(2.1256) tensor(-1.9889) tensor(-0.0131)
############################
epsilon after preprocessing: tensor([[[[0.0317]],

         [[0.0322]],

         [[0.0300]]]]), data_max = tensor([[[[2.0587]],

         [[2.1256]],

         [[2.1154]]]]), data_min = tensor([[[[-1.9889]],

         [[-1.9807]],

         [[-1.7076]]]])
Task length: 1
saving results to Verified_ret_[cnn_4layer]_start=48_end=49_iter=20_b=4096_timeout=200_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=skip.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 48 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label 5, correct label 5, image norm 2560.0009765625, logits tensor([-30.8774, -33.2365, -27.2401, -26.1453, -28.3847, -25.2354, -28.4432,
        -26.1824, -33.1786, -31.0787], device='cuda:0',
       grad_fn=<SelectBackward>)
Model prediction is: tensor([[-30.8774, -33.2365, -27.2401, -26.1453, -28.3847, -25.2354, -28.4432,
         -26.1824, -33.1786, -31.0787]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[ 2.0232,  3.4012, -0.3539, -0.5235,  0.1777, -0.3920, -1.6333,  3.6881,
          1.5767]], device='cuda:0') None
best_l after optimization: -9.469024658203125 with beta sum per layer: []
alpha/beta optimization time: 7.7851386070251465
initial alpha-CROWN bounds: tensor([[ 2.2256,  3.6606, -0.2408, -0.4529,  0.3186, -0.2533, -1.4725,  3.9219,
          1.7619]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-1.4725, device='cuda:0', grad_fn=<MinBackward1>)
##### [0:48] Tested against 7 ######
Model prediction is: tensor([[-30.8774, -33.2365, -27.2401, -26.1453, -28.3847, -25.2354, -28.4432,
         -26.1824, -33.1786, -31.0787]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /10 start_node /11
setting alpha for layer /10 start_node /21
not setting layer /10 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 16, 16, 16]) != torch.Size([2, 9, 1, 16, 16, 16]))
setting alpha for layer /12 start_node /21
not setting layer /12 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
not setting layer /22 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /9 torch.Size([1, 16, 16, 16])
1 /11 torch.Size([1, 32, 8, 8])
2 /21 torch.Size([1, 100])
best_l after optimization: 1.4721137285232544 with beta sum per layer: []
alpha/beta optimization time: 2.190800905227661
alpha-CROWN with fixed intermediate bounds: tensor([[-1.4721]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-1.472113847732544
layer 0 size torch.Size([4096]) unstable 694
layer 1 size torch.Size([2048]) unstable 307
layer 2 size torch.Size([100]) unstable 44
-----------------
# of unstable neurons: 1045
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  8
batch:  torch.Size([1, 16, 16, 16]) post split depth:  8
splitting decisions: 
split level 0: [2, 42] 
split level 1: [2, 94] 
split level 2: [2, 54] 
split level 3: [2, 73] 
split level 4: [2, 61] 
split level 5: [2, 4] 
split level 6: [2, 8] 
split level 7: [2, 20] 
regular batch size: 2*128, diving batch size 1*0
best_l after optimization: 14.188080787658691 with beta sum per layer: [0.0, 0.0, 26.40873908996582]
alpha/beta optimization time: 0.30176258087158203
This batch time : update_bounds func: 0.3478	 prepare: 0.0184	 bound: 0.3021	 transfer: 0.0118	 finalize: 0.0148
Accumulated time: update_bounds func: 0.3478	 prepare: 0.0184	 bound: 0.3021	 transfer: 0.0118	 finalize: 0.0148
batch bounding time:  0.3481166362762451
Current worst splitting domains [lb, ub] (depth):
[-0.60511,   inf] (9), [-0.59674,   inf] (9), [-0.57906,   inf] (9), [-0.57326,   inf] (9), [-0.55187,   inf] (9), [-0.53646,   inf] (9), [-0.52249,   inf] (9), [-0.51117,   inf] (9), [-0.49394,   inf] (9), [-0.49176,   inf] (9), [-0.48861,   inf] (9), [-0.48793,   inf] (9), [-0.48683,   inf] (9), [-0.47982,   inf] (9), [-0.47882,   inf] (9), [-0.46759,   inf] (9), [-0.45417,   inf] (9), [-0.45348,   inf] (9), [-0.45322,   inf] (9), [-0.45108,   inf] (9), 
length of domains: 127
Total time: 0.6546	 pickout: 0.0009	 decision: 0.2673	 get_bound: 0.3807	 add_domain: 0.0056
Current lb:-0.6051108241081238
256 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.685455322265625

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([127, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([127, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 88] [2, 88] [2, 88] [2, 88] [2, 88] [2, 88] [2, 88] [2, 88] [2, 88] [2, 88] 
regular batch size: 2*127, diving batch size 1*0
best_l after optimization: 46.340579986572266 with beta sum per layer: [0.0, 0.0, 36.54033660888672]
alpha/beta optimization time: 0.2807443141937256
This batch time : update_bounds func: 0.3329	 prepare: 0.0222	 bound: 0.2811	 transfer: 0.0134	 finalize: 0.0157
Accumulated time: update_bounds func: 0.6807	 prepare: 0.0405	 bound: 0.5832	 transfer: 0.0134	 finalize: 0.0305
batch bounding time:  0.33335208892822266
Current worst splitting domains [lb, ub] (depth):
[-0.57790,   inf] (11), [-0.56962,   inf] (11), [-0.55125,   inf] (11), [-0.54524,   inf] (11), [-0.52305,   inf] (11), [-0.50754,   inf] (11), [-0.49318,   inf] (11), [-0.48125,   inf] (11), [-0.47797,   inf] (11), [-0.47425,   inf] (11), [-0.46419,   inf] (11), [-0.46198,   inf] (11), [-0.45832,   inf] (11), [-0.45672,   inf] (11), [-0.45670,   inf] (11), [-0.45542,   inf] (11), [-0.45402,   inf] (11), [-0.44827,   inf] (11), [-0.44726,   inf] (11), [-0.43657,   inf] (11), 
length of domains: 201
Total time: 0.4255	 pickout: 0.0214	 decision: 0.0608	 get_bound: 0.3337	 add_domain: 0.0095
Current lb:-0.5778962969779968
510 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.112937927246094

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([201, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([201, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 25] [2, 25] [2, 25] [2, 25] [2, 25] [2, 25] [2, 25] [2, 25] [2, 25] [2, 25] 
regular batch size: 2*201, diving batch size 1*0
best_l after optimization: 67.5621337890625 with beta sum per layer: [0.0, 0.0, 61.110076904296875]
alpha/beta optimization time: 0.3350045680999756
This batch time : update_bounds func: 0.4074	 prepare: 0.0349	 bound: 0.3354	 transfer: 0.0136	 finalize: 0.0226
Accumulated time: update_bounds func: 1.0881	 prepare: 0.0755	 bound: 0.9185	 transfer: 0.0136	 finalize: 0.0531
batch bounding time:  0.40789103507995605
Current worst splitting domains [lb, ub] (depth):
[-0.55930,   inf] (13), [-0.55067,   inf] (13), [-0.53259,   inf] (13), [-0.52610,   inf] (13), [-0.50398,   inf] (13), [-0.48677,   inf] (13), [-0.47453,   inf] (13), [-0.46173,   inf] (13), [-0.46124,   inf] (13), [-0.45580,   inf] (13), [-0.45289,   inf] (13), [-0.45147,   inf] (13), [-0.44544,   inf] (13), [-0.44325,   inf] (13), [-0.44170,   inf] (13), [-0.43976,   inf] (13), [-0.43685,   inf] (13), [-0.43665,   inf] (13), [-0.43663,   inf] (13), [-0.43261,   inf] (13), 
length of domains: 327
Total time: 0.5401	 pickout: 0.0291	 decision: 0.0843	 get_bound: 0.4085	 add_domain: 0.0182
Current lb:-0.5593017935752869
912 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.657010078430176

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([327, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([327, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 76] [2, 76] [2, 76] [2, 76] [2, 76] [2, 76] [2, 76] [2, 76] [2, 76] [2, 76] 
regular batch size: 2*327, diving batch size 1*0
best_l after optimization: 42.664527893066406 with beta sum per layer: [0.0, 0.0, 136.81654357910156]
alpha/beta optimization time: 0.4415297508239746
This batch time : update_bounds func: 0.6198	 prepare: 0.0560	 bound: 0.4419	 transfer: 0.0260	 finalize: 0.0943
Accumulated time: update_bounds func: 1.7079	 prepare: 0.1315	 bound: 1.3604	 transfer: 0.0260	 finalize: 0.1474
batch bounding time:  0.6206235885620117
Current worst splitting domains [lb, ub] (depth):
[-0.54233,   inf] (15), [-0.53316,   inf] (15), [-0.51484,   inf] (15), [-0.50830,   inf] (15), [-0.48603,   inf] (15), [-0.46907,   inf] (15), [-0.45694,   inf] (15), [-0.44365,   inf] (15), [-0.44316,   inf] (15), [-0.43705,   inf] (15), [-0.43466,   inf] (15), [-0.43256,   inf] (15), [-0.42667,   inf] (15), [-0.42404,   inf] (15), [-0.42282,   inf] (15), [-0.42178,   inf] (15), [-0.41889,   inf] (15), [-0.41858,   inf] (15), [-0.41791,   inf] (15), [-0.41364,   inf] (15), 
length of domains: 362
Total time: 0.8080	 pickout: 0.0544	 decision: 0.1113	 get_bound: 0.6217	 add_domain: 0.0206
Current lb:-0.5423263907432556
1566 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.473215103149414

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([362, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([362, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 36] [2, 36] [1, 541] [1, 269] [2, 36] [2, 36] [1, 269] [1, 269] [1, 269] [2, 36] 
regular batch size: 2*362, diving batch size 1*0
best_l after optimization: 100.91488647460938 with beta sum per layer: [0.0, 2.3684065341949463, 132.29348754882812]
alpha/beta optimization time: 0.4637641906738281
This batch time : update_bounds func: 0.5915	 prepare: 0.0631	 bound: 0.4641	 transfer: 0.0215	 finalize: 0.0411
Accumulated time: update_bounds func: 2.2993	 prepare: 0.1946	 bound: 1.8245	 transfer: 0.0215	 finalize: 0.1885
batch bounding time:  0.5921728610992432
Current worst splitting domains [lb, ub] (depth):
[-0.52885,   inf] (17), [-0.51933,   inf] (17), [-0.49871,   inf] (17), [-0.49801,   inf] (17), [-0.49370,   inf] (17), [-0.49328,   inf] (17), [-0.47180,   inf] (17), [-0.45446,   inf] (17), [-0.44237,   inf] (17), [-0.44235,   inf] (17), [-0.42792,   inf] (17), [-0.42755,   inf] (17), [-0.42471,   inf] (17), [-0.42207,   inf] (17), [-0.42198,   inf] (17), [-0.41762,   inf] (17), [-0.41641,   inf] (17), [-0.41316,   inf] (17), [-0.41010,   inf] (17), [-0.40806,   inf] (17), 
length of domains: 597
Total time: 0.8066	 pickout: 0.0552	 decision: 0.1237	 get_bound: 0.5932	 add_domain: 0.0345
Current lb:-0.5288532376289368
2290 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.286394834518433

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([597, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([597, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 269] [1, 269] [1, 269] [1, 269] [1, 541] [1, 541] [1, 269] [1, 269] [2, 36] [2, 36] 
regular batch size: 2*597, diving batch size 1*0
best_l after optimization: 132.6081085205078 with beta sum per layer: [0.0, 14.827335357666016, 241.48794555664062]
alpha/beta optimization time: 0.668604850769043
This batch time : update_bounds func: 0.8849	 prepare: 0.1099	 bound: 0.6690	 transfer: 0.0341	 finalize: 0.0690
Accumulated time: update_bounds func: 3.1842	 prepare: 0.3044	 bound: 2.4935	 transfer: 0.0341	 finalize: 0.2575
batch bounding time:  0.8859832286834717
Current worst splitting domains [lb, ub] (depth):
[-0.51599,   inf] (19), [-0.51587,   inf] (19), [-0.50587,   inf] (19), [-0.50516,   inf] (19), [-0.48489,   inf] (19), [-0.48452,   inf] (19), [-0.48385,   inf] (19), [-0.48342,   inf] (19), [-0.47771,   inf] (19), [-0.47753,   inf] (19), [-0.47689,   inf] (19), [-0.47676,   inf] (19), [-0.45909,   inf] (19), [-0.45891,   inf] (19), [-0.44045,   inf] (19), [-0.44036,   inf] (19), [-0.42869,   inf] (19), [-0.42867,   inf] (19), [-0.41455,   inf] (19), [-0.41405,   inf] (19), 
length of domains: 883
Total time: 1.2850	 pickout: 0.0927	 decision: 0.2503	 get_bound: 0.8878	 add_domain: 0.0541
Current lb:-0.5159915685653687
3484 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.582361459732056

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([883, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([883, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 541] [1, 541] [1, 541] [1, 541] [2, 36] [2, 36] [2, 36] [2, 36] [2, 36] [2, 36] 
regular batch size: 2*883, diving batch size 1*0
best_l after optimization: 142.30569458007812 with beta sum per layer: [0.0, 73.82933807373047, 425.7000732421875]
alpha/beta optimization time: 0.8993988037109375
This batch time : update_bounds func: 1.2957	 prepare: 0.1658	 bound: 0.8997	 transfer: 0.0575	 finalize: 0.1680
Accumulated time: update_bounds func: 4.4799	 prepare: 0.4702	 bound: 3.3932	 transfer: 0.0575	 finalize: 0.4256
batch bounding time:  1.297365427017212
Current worst splitting domains [lb, ub] (depth):
[-0.50384,   inf] (21), [-0.50380,   inf] (21), [-0.50283,   inf] (21), [-0.50275,   inf] (21), [-0.49388,   inf] (21), [-0.49294,   inf] (21), [-0.49220,   inf] (21), [-0.49137,   inf] (21), [-0.47212,   inf] (21), [-0.47187,   inf] (21), [-0.47085,   inf] (21), [-0.47035,   inf] (21), [-0.46499,   inf] (21), [-0.46470,   inf] (21), [-0.46428,   inf] (21), [-0.46398,   inf] (21), [-0.44953,   inf] (21), [-0.44937,   inf] (21), [-0.43111,   inf] (21), [-0.43097,   inf] (21), 
length of domains: 1346
Total time: 1.8699	 pickout: 0.1420	 decision: 0.3384	 get_bound: 1.3002	 add_domain: 0.0893
Current lb:-0.5038401484489441
5250 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.469250917434692

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1346, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([1346, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 780] [1, 780] [1, 780] [1, 780] [1, 780] [1, 780] [1, 780] [1, 780] [2, 92] [2, 92] 
regular batch size: 2*1346, diving batch size 1*0
best_l after optimization: 124.58387756347656 with beta sum per layer: [0.0, 177.76620483398438, 696.074951171875]
alpha/beta optimization time: 1.2945506572723389
This batch time : update_bounds func: 1.9247	 prepare: 0.2906	 bound: 1.2951	 transfer: 0.0982	 finalize: 0.2325
Accumulated time: update_bounds func: 6.4046	 prepare: 0.7608	 bound: 4.6884	 transfer: 0.0982	 finalize: 0.6581
batch bounding time:  1.9270076751708984
Current worst splitting domains [lb, ub] (depth):
[-0.49474,   inf] (23), [-0.49373,   inf] (23), [-0.49369,   inf] (23), [-0.49264,   inf] (23), [-0.48806,   inf] (23), [-0.48792,   inf] (23), [-0.48722,   inf] (23), [-0.48700,   inf] (23), [-0.48392,   inf] (23), [-0.48223,   inf] (23), [-0.48207,   inf] (23), [-0.48067,   inf] (23), [-0.47731,   inf] (23), [-0.47728,   inf] (23), [-0.47587,   inf] (23), [-0.47569,   inf] (23), [-0.46271,   inf] (23), [-0.46241,   inf] (23), [-0.46155,   inf] (23), [-0.46127,   inf] (23), 
length of domains: 1847
Total time: 2.8200	 pickout: 0.2521	 decision: 0.5149	 get_bound: 1.9310	 add_domain: 0.1219
Current lb:-0.4947395920753479
7942 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 12.31600308418274

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1847, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([1847, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 92] [2, 92] [2, 92] [2, 92] [2, 92] [2, 92] [2, 92] [2, 92] [1, 1500] [1, 1500] 
regular batch size: 2*1847, diving batch size 1*0
best_l after optimization: 345.447265625 with beta sum per layer: [0.0, 422.06689453125, 788.9913330078125]
alpha/beta optimization time: 1.7673213481903076
This batch time : update_bounds func: 2.5421	 prepare: 0.3474	 bound: 1.7677	 transfer: 0.1361	 finalize: 0.2811
Accumulated time: update_bounds func: 8.9467	 prepare: 1.1082	 bound: 6.4561	 transfer: 0.1361	 finalize: 0.9392
batch bounding time:  2.5451478958129883
Current worst splitting domains [lb, ub] (depth):
[-0.48583,   inf] (25), [-0.48487,   inf] (25), [-0.48482,   inf] (25), [-0.48378,   inf] (25), [-0.47922,   inf] (25), [-0.47905,   inf] (25), [-0.47843,   inf] (25), [-0.47819,   inf] (25), [-0.47338,   inf] (25), [-0.47306,   inf] (25), [-0.47201,   inf] (25), [-0.47163,   inf] (25), [-0.47042,   inf] (25), [-0.46979,   inf] (25), [-0.46813,   inf] (25), [-0.46658,   inf] (25), [-0.46635,   inf] (25), [-0.46533,   inf] (25), [-0.46530,   inf] (25), [-0.46528,   inf] (25), 
length of domains: 2991
Total time: 3.7978	 pickout: 0.3270	 decision: 0.6965	 get_bound: 2.5508	 add_domain: 0.2236
Current lb:-0.4858330488204956
11636 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 16.147953033447266

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2991, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([2991, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1470] [1, 1470] [1, 1470] [1, 1470] [1, 1470] [1, 1470] [1, 1470] [1, 1470] [1, 1500] [2, 92] 
regular batch size: 2*2991, diving batch size 1*0
best_l after optimization: 746.4544677734375 with beta sum per layer: [0.0, 1106.742431640625, 1118.580078125]
alpha/beta optimization time: 2.760774850845337
This batch time : update_bounds func: 4.0828	 prepare: 0.5594	 bound: 2.7613	 transfer: 0.2198	 finalize: 0.5278
Accumulated time: update_bounds func: 13.0295	 prepare: 1.6676	 bound: 9.2173	 transfer: 0.2198	 finalize: 1.4670
batch bounding time:  4.088234186172485
Current worst splitting domains [lb, ub] (depth):
[-0.48055,   inf] (27), [-0.47962,   inf] (27), [-0.47957,   inf] (27), [-0.47855,   inf] (27), [-0.47398,   inf] (27), [-0.47377,   inf] (27), [-0.47318,   inf] (27), [-0.47293,   inf] (27), [-0.46441,   inf] (27), [-0.46327,   inf] (27), [-0.46296,   inf] (27), [-0.46296,   inf] (27), [-0.46175,   inf] (27), [-0.46162,   inf] (27), [-0.46111,   inf] (27), [-0.45999,   inf] (27), [-0.45952,   inf] (27), [-0.45905,   inf] (27), [-0.45901,   inf] (27), [-0.45799,   inf] (27), 
length of domains: 5354
Total time: 6.1901	 pickout: 0.5173	 decision: 1.1949	 get_bound: 4.0977	 add_domain: 0.3802
Current lb:-0.48054957389831543
17618 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 22.39265275001526

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 779] [1, 779] [1, 779] [1, 779] [1, 779] [1, 779] [1, 779] [1, 779] [1, 779] [1, 779] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 1395.440673828125 with beta sum per layer: [0.0, 2327.759765625, 1287.748779296875]
alpha/beta optimization time: 3.7198941707611084
This batch time : update_bounds func: 5.6773	 prepare: 0.7639	 bound: 3.7203	 transfer: 0.3105	 finalize: 0.7401
Accumulated time: update_bounds func: 18.7068	 prepare: 2.4315	 bound: 12.9377	 transfer: 0.3105	 finalize: 2.2071
batch bounding time:  5.685281038284302
Current worst splitting domains [lb, ub] (depth):
[-0.47349,   inf] (29), [-0.47254,   inf] (29), [-0.47251,   inf] (29), [-0.47151,   inf] (29), [-0.46822,   inf] (29), [-0.46815,   inf] (29), [-0.46744,   inf] (29), [-0.46722,   inf] (29), [-0.46367,   inf] (29), [-0.46269,   inf] (29), [-0.46239,   inf] (29), [-0.46149,   inf] (29), [-0.45656,   inf] (29), [-0.45568,   inf] (29), [-0.45520,   inf] (29), [-0.45507,   inf] (29), [-0.45394,   inf] (29), [-0.45384,   inf] (29), [-0.45334,   inf] (29), [-0.45288,   inf] (29), 
length of domains: 9415
Total time: 8.8226	 pickout: 0.7265	 decision: 1.7741	 get_bound: 5.7011	 add_domain: 0.6209
Current lb:-0.47349265217781067
25810 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 31.281294107437134

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 445] [1, 445] [1, 445] [1, 445] [1, 445] [1, 445] [1, 445] [1, 445] [1, 445] [1, 445] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 1877.434326171875 with beta sum per layer: [0.0, 3123.51953125, 993.679931640625]
alpha/beta optimization time: 3.720607042312622
This batch time : update_bounds func: 5.6919	 prepare: 0.7580	 bound: 3.7210	 transfer: 0.3127	 finalize: 0.6945
Accumulated time: update_bounds func: 24.3987	 prepare: 3.1895	 bound: 16.6587	 transfer: 0.3127	 finalize: 2.9016
batch bounding time:  5.700768947601318
Current worst splitting domains [lb, ub] (depth):
[-0.46915,   inf] (31), [-0.46820,   inf] (31), [-0.46817,   inf] (31), [-0.46716,   inf] (31), [-0.46391,   inf] (31), [-0.46378,   inf] (31), [-0.46310,   inf] (31), [-0.46289,   inf] (31), [-0.45906,   inf] (31), [-0.45808,   inf] (31), [-0.45778,   inf] (31), [-0.45690,   inf] (31), [-0.45087,   inf] (31), [-0.44997,   inf] (31), [-0.44958,   inf] (31), [-0.44948,   inf] (31), [-0.44831,   inf] (31), [-0.44821,   inf] (31), [-0.44782,   inf] (31), [-0.44764,   inf] (31), 
length of domains: 13511
Total time: 9.1305	 pickout: 0.6997	 decision: 1.8181	 get_bound: 5.7181	 add_domain: 0.8946
Current lb:-0.46914559602737427
34002 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 40.480342388153076

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1588] [1, 1588] [1, 1588] [1, 1588] [1, 1588] [1, 1588] [1, 1588] [1, 1588] [1, 1588] [1, 1588] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2194.175048828125 with beta sum per layer: [0.0, 3995.930419921875, 790.1771850585938]
alpha/beta optimization time: 3.7260966300964355
This batch time : update_bounds func: 5.8349	 prepare: 0.7620	 bound: 3.7265	 transfer: 0.3109	 finalize: 0.7754
Accumulated time: update_bounds func: 30.2336	 prepare: 3.9515	 bound: 20.3852	 transfer: 0.3109	 finalize: 3.6770
batch bounding time:  5.84271240234375
Current worst splitting domains [lb, ub] (depth):
[-0.46167,   inf] (33), [-0.46073,   inf] (33), [-0.46069,   inf] (33), [-0.45970,   inf] (33), [-0.45644,   inf] (33), [-0.45629,   inf] (33), [-0.45629,   inf] (33), [-0.45563,   inf] (33), [-0.45540,   inf] (33), [-0.45535,   inf] (33), [-0.45526,   inf] (33), [-0.45430,   inf] (33), [-0.45148,   inf] (33), [-0.45100,   inf] (33), [-0.45088,   inf] (33), [-0.45046,   inf] (33), [-0.45023,   inf] (33), [-0.45018,   inf] (33), [-0.44999,   inf] (33), [-0.44933,   inf] (33), 
length of domains: 17607
Total time: 8.8494	 pickout: 0.7048	 decision: 1.6162	 get_bound: 5.8567	 add_domain: 0.6716
Current lb:-0.4616670608520508
42194 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 49.40049624443054

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1500] [1, 1500] [1, 1500] [1, 1500] [1, 1500] [1, 1500] [1, 1500] [1, 1500] [1, 1500] [1, 1500] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2412.91650390625 with beta sum per layer: [0.0, 4646.01416015625, 631.2862548828125]
alpha/beta optimization time: 3.727367877960205
This batch time : update_bounds func: 5.6393	 prepare: 0.7649	 bound: 3.7278	 transfer: 0.3120	 finalize: 0.8137
Accumulated time: update_bounds func: 35.8729	 prepare: 4.7164	 bound: 24.1130	 transfer: 0.3120	 finalize: 4.4907
batch bounding time:  5.647570371627808
Current worst splitting domains [lb, ub] (depth):
[-0.45213,   inf] (35), [-0.45206,   inf] (35), [-0.45138,   inf] (35), [-0.45116,   inf] (35), [-0.45105,   inf] (35), [-0.45045,   inf] (35), [-0.45045,   inf] (35), [-0.44938,   inf] (35), [-0.44692,   inf] (35), [-0.44679,   inf] (35), [-0.44678,   inf] (35), [-0.44661,   inf] (35), [-0.44649,   inf] (35), [-0.44637,   inf] (35), [-0.44636,   inf] (35), [-0.44607,   inf] (35), [-0.44577,   inf] (35), [-0.44550,   inf] (35), [-0.44533,   inf] (35), [-0.44521,   inf] (35), 
length of domains: 21703
Total time: 9.0023	 pickout: 0.7090	 decision: 1.9131	 get_bound: 5.6616	 add_domain: 0.7186
Current lb:-0.4521312713623047
50386 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 58.474011182785034

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 292] [1, 292] [1, 292] [1, 292] [1, 292] [1, 292] [1, 292] [1, 292] [1, 292] [1, 292] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2591.80810546875 with beta sum per layer: [0.0, 5173.67333984375, 513.771484375]
alpha/beta optimization time: 3.7450335025787354
This batch time : update_bounds func: 5.7107	 prepare: 0.7834	 bound: 3.7455	 transfer: 0.3128	 finalize: 0.8481
Accumulated time: update_bounds func: 41.5835	 prepare: 5.4998	 bound: 27.8585	 transfer: 0.3128	 finalize: 5.3388
batch bounding time:  5.719247579574585
Current worst splitting domains [lb, ub] (depth):
[-0.44452,   inf] (37), [-0.44420,   inf] (37), [-0.44385,   inf] (37), [-0.44353,   inf] (37), [-0.44319,   inf] (37), [-0.44291,   inf] (37), [-0.44268,   inf] (37), [-0.44178,   inf] (37), [-0.44176,   inf] (37), [-0.44159,   inf] (37), [-0.44100,   inf] (37), [-0.44076,   inf] (37), [-0.44075,   inf] (37), [-0.44010,   inf] (37), [-0.44006,   inf] (37), [-0.43931,   inf] (37), [-0.43917,   inf] (37), [-0.43899,   inf] (37), [-0.43890,   inf] (37), [-0.43886,   inf] (37), 
length of domains: 25799
Total time: 8.8769	 pickout: 0.7342	 decision: 1.6816	 get_bound: 5.7368	 add_domain: 0.7242
Current lb:-0.444521427154541
58578 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 67.42288374900818

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 883] [1, 883] [1, 883] [1, 883] [1, 883] [1, 883] [1, 883] [1, 883] [1, 883] [1, 883] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2776.17041015625 with beta sum per layer: [0.0, 5747.3603515625, 368.23406982421875]
alpha/beta optimization time: 3.7487618923187256
This batch time : update_bounds func: 5.8107	 prepare: 0.8091	 bound: 3.7493	 transfer: 0.3192	 finalize: 0.4858
Accumulated time: update_bounds func: 47.3942	 prepare: 6.3089	 bound: 31.6078	 transfer: 0.3192	 finalize: 5.8246
batch bounding time:  5.818753004074097
Current worst splitting domains [lb, ub] (depth):
[-0.43717,   inf] (39), [-0.43686,   inf] (39), [-0.43650,   inf] (39), [-0.43616,   inf] (39), [-0.43585,   inf] (39), [-0.43557,   inf] (39), [-0.43537,   inf] (39), [-0.43465,   inf] (39), [-0.43461,   inf] (39), [-0.43428,   inf] (39), [-0.43387,   inf] (39), [-0.43382,   inf] (39), [-0.43363,   inf] (39), [-0.43361,   inf] (39), [-0.43351,   inf] (39), [-0.43314,   inf] (39), [-0.43296,   inf] (39), [-0.43293,   inf] (39), [-0.43281,   inf] (39), [-0.43248,   inf] (39), 
length of domains: 29895
Total time: 9.5398	 pickout: 0.7008	 decision: 1.7483	 get_bound: 5.8349	 add_domain: 1.2558
Current lb:-0.4371655285358429
66770 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 77.03479027748108

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 93] [1, 93] [1, 93] [1, 93] [1, 93] [1, 93] [1, 93] [1, 93] [1, 93] [1, 93] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2948.8134765625 with beta sum per layer: [0.0, 5782.22021484375, 268.4516296386719]
alpha/beta optimization time: 3.7520482540130615
This batch time : update_bounds func: 5.3473	 prepare: 0.7778	 bound: 3.7525	 transfer: 0.3143	 finalize: 0.4811
Accumulated time: update_bounds func: 52.7415	 prepare: 7.0867	 bound: 35.3603	 transfer: 0.3143	 finalize: 6.3057
batch bounding time:  5.356462240219116
Current worst splitting domains [lb, ub] (depth):
[-0.43013,   inf] (41), [-0.42971,   inf] (41), [-0.42936,   inf] (41), [-0.42936,   inf] (41), [-0.42893,   inf] (41), [-0.42871,   inf] (41), [-0.42791,   inf] (41), [-0.42748,   inf] (41), [-0.42748,   inf] (41), [-0.42706,   inf] (41), [-0.42671,   inf] (41), [-0.42669,   inf] (41), [-0.42667,   inf] (41), [-0.42667,   inf] (41), [-0.42625,   inf] (41), [-0.42602,   inf] (41), [-0.42592,   inf] (41), [-0.42589,   inf] (41), [-0.42562,   inf] (41), [-0.42548,   inf] (41), 
length of domains: 33991
Total time: 9.2743	 pickout: 0.7111	 decision: 1.8648	 get_bound: 5.3728	 add_domain: 1.3257
Current lb:-0.43012967705726624
74962 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 86.38209915161133

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1253] [1, 1253] [1, 1253] [1, 1253] [1, 1253] [1, 1253] [1, 539] [1, 1253] [1, 1253] [1, 539] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2940.7421875 with beta sum per layer: [0.0, 6700.294921875, 181.63404846191406]
alpha/beta optimization time: 3.815685272216797
This batch time : update_bounds func: 6.0647	 prepare: 0.7720	 bound: 3.8161	 transfer: 0.3199	 finalize: 1.1360
Accumulated time: update_bounds func: 58.8062	 prepare: 7.8587	 bound: 39.1764	 transfer: 0.3199	 finalize: 7.4417
batch bounding time:  6.073351860046387
Current worst splitting domains [lb, ub] (depth):
[-0.42631,   inf] (43), [-0.42597,   inf] (43), [-0.42557,   inf] (43), [-0.42554,   inf] (43), [-0.42517,   inf] (43), [-0.42490,   inf] (43), [-0.42419,   inf] (43), [-0.42333,   inf] (43), [-0.42295,   inf] (43), [-0.42295,   inf] (43), [-0.42288,   inf] (43), [-0.42254,   inf] (43), [-0.42218,   inf] (43), [-0.42217,   inf] (43), [-0.42215,   inf] (43), [-0.42214,   inf] (43), [-0.42211,   inf] (43), [-0.42174,   inf] (43), [-0.42152,   inf] (43), [-0.42152,   inf] (43), 
length of domains: 38087
Total time: 9.0738	 pickout: 0.7046	 decision: 1.3517	 get_bound: 6.0910	 add_domain: 0.9266
Current lb:-0.4263128936290741
83154 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 95.53122997283936

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 539] [1, 539] [1, 539] [1, 539] [1, 539] [1, 539] [1, 1253] [1, 1253] [1, 539] [1, 539] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2929.771728515625 with beta sum per layer: [0.0, 6411.25830078125, 126.33853149414062]
alpha/beta optimization time: 3.840306282043457
This batch time : update_bounds func: 6.0898	 prepare: 0.7813	 bound: 3.8407	 transfer: 0.3137	 finalize: 1.1327
Accumulated time: update_bounds func: 64.8960	 prepare: 8.6400	 bound: 43.0171	 transfer: 0.3137	 finalize: 8.5744
batch bounding time:  6.0991737842559814
Current worst splitting domains [lb, ub] (depth):
[-0.42257,   inf] (45), [-0.42229,   inf] (45), [-0.42187,   inf] (45), [-0.42179,   inf] (45), [-0.42146,   inf] (45), [-0.42121,   inf] (45), [-0.42055,   inf] (45), [-0.41969,   inf] (45), [-0.41928,   inf] (45), [-0.41923,   inf] (45), [-0.41915,   inf] (45), [-0.41885,   inf] (45), [-0.41849,   inf] (45), [-0.41845,   inf] (45), [-0.41845,   inf] (45), [-0.41843,   inf] (45), [-0.41837,   inf] (45), [-0.41802,   inf] (45), [-0.41784,   inf] (45), [-0.41783,   inf] (45), 
length of domains: 42183
Total time: 9.6105	 pickout: 0.7090	 decision: 1.9634	 get_bound: 6.1153	 add_domain: 0.8228
Current lb:-0.4225734770298004
91346 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 105.21949338912964

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1492] [1, 1492] [1, 1492] [1, 1492] [1, 1492] [1, 1492] [1, 1492] [1, 1492] [1, 1492] [1, 1492] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2914.831298828125 with beta sum per layer: [1.182523488998413, 6906.6953125, 135.0235595703125]
alpha/beta optimization time: 3.8356997966766357
This batch time : update_bounds func: 6.1537	 prepare: 0.8069	 bound: 3.8362	 transfer: 0.2995	 finalize: 0.5027
Accumulated time: update_bounds func: 71.0496	 prepare: 9.4469	 bound: 46.8533	 transfer: 0.2995	 finalize: 9.0771
batch bounding time:  6.162473201751709
Current worst splitting domains [lb, ub] (depth):
[-0.41847,   inf] (47), [-0.41847,   inf] (47), [-0.41766,   inf] (47), [-0.41750,   inf] (47), [-0.41746,   inf] (47), [-0.41669,   inf] (47), [-0.41656,   inf] (47), [-0.41558,   inf] (47), [-0.41547,   inf] (47), [-0.41514,   inf] (47), [-0.41505,   inf] (47), [-0.41503,   inf] (47), [-0.41445,   inf] (47), [-0.41428,   inf] (47), [-0.41422,   inf] (47), [-0.41417,   inf] (47), [-0.41409,   inf] (47), [-0.41409,   inf] (47), [-0.41400,   inf] (47), [-0.41349,   inf] (47), 
length of domains: 46279
Total time: 9.1214	 pickout: 0.7245	 decision: 1.3652	 get_bound: 6.1793	 add_domain: 0.8524
Current lb:-0.41846764087677
99538 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 114.42471289634705

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 490] [1, 490] [1, 490] [1, 490] [1, 490] [1, 490] [1, 490] [1, 490] [1, 490] [1, 490] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2956.2861328125 with beta sum per layer: [0.0, 5529.13720703125, 132.6892547607422]
alpha/beta optimization time: 3.798440456390381
This batch time : update_bounds func: 5.4753	 prepare: 0.8303	 bound: 3.7989	 transfer: 0.3191	 finalize: 0.5056
Accumulated time: update_bounds func: 76.5250	 prepare: 10.2772	 bound: 50.6522	 transfer: 0.3191	 finalize: 9.5827
batch bounding time:  5.4842588901519775
Current worst splitting domains [lb, ub] (depth):
[-0.41492,   inf] (49), [-0.41490,   inf] (49), [-0.41412,   inf] (49), [-0.41393,   inf] (49), [-0.41390,   inf] (49), [-0.41316,   inf] (49), [-0.41302,   inf] (49), [-0.41201,   inf] (49), [-0.41187,   inf] (49), [-0.41152,   inf] (49), [-0.41146,   inf] (49), [-0.41145,   inf] (49), [-0.41083,   inf] (49), [-0.41066,   inf] (49), [-0.41065,   inf] (49), [-0.41055,   inf] (49), [-0.41054,   inf] (49), [-0.41048,   inf] (49), [-0.41045,   inf] (49), [-0.40989,   inf] (49), 
length of domains: 50375
Total time: 10.2104	 pickout: 0.7357	 decision: 2.2127	 get_bound: 5.5019	 add_domain: 1.7600
Current lb:-0.4149243235588074
107730 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 124.72317886352539

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 549] [1, 549] [1, 549] [1, 549] [1, 549] [1, 549] [1, 549] [1, 549] [1, 549] [1, 549] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2968.28125 with beta sum per layer: [0.0, 5090.61279296875, 112.4593734741211]
alpha/beta optimization time: 3.7854902744293213
This batch time : update_bounds func: 6.4868	 prepare: 0.8405	 bound: 3.7860	 transfer: 0.3169	 finalize: 1.5188
Accumulated time: update_bounds func: 83.0118	 prepare: 11.1177	 bound: 54.4382	 transfer: 0.3169	 finalize: 11.1015
batch bounding time:  6.495975494384766
Current worst splitting domains [lb, ub] (depth):
[-0.40894,   inf] (51), [-0.40871,   inf] (51), [-0.40835,   inf] (51), [-0.40791,   inf] (51), [-0.40774,   inf] (51), [-0.40741,   inf] (51), [-0.40738,   inf] (51), [-0.40640,   inf] (51), [-0.40579,   inf] (51), [-0.40551,   inf] (51), [-0.40550,   inf] (51), [-0.40527,   inf] (51), [-0.40496,   inf] (51), [-0.40486,   inf] (51), [-0.40480,   inf] (51), [-0.40476,   inf] (51), [-0.40454,   inf] (51), [-0.40447,   inf] (51), [-0.40432,   inf] (51), [-0.40418,   inf] (51), 
length of domains: 54471
Total time: 9.5230	 pickout: 0.7262	 decision: 1.3983	 get_bound: 6.5108	 add_domain: 0.8876
Current lb:-0.4089425206184387
115922 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 134.3366289138794

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 540] [1, 540] [1, 874] [1, 540] [1, 540] [1, 874] [1, 874] [1, 874] [1, 540] [1, 540] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2812.041015625 with beta sum per layer: [0.0, 5819.93603515625, 82.6817398071289]
alpha/beta optimization time: 3.846302032470703
This batch time : update_bounds func: 6.5412	 prepare: 0.8739	 bound: 3.8467	 transfer: 0.3167	 finalize: 1.4821
Accumulated time: update_bounds func: 89.5530	 prepare: 11.9916	 bound: 58.2849	 transfer: 0.3167	 finalize: 12.5836
batch bounding time:  6.550008296966553
Current worst splitting domains [lb, ub] (depth):
[-0.40560,   inf] (53), [-0.40533,   inf] (53), [-0.40508,   inf] (53), [-0.40455,   inf] (53), [-0.40436,   inf] (53), [-0.40415,   inf] (53), [-0.40411,   inf] (53), [-0.40315,   inf] (53), [-0.40242,   inf] (53), [-0.40219,   inf] (53), [-0.40216,   inf] (53), [-0.40189,   inf] (53), [-0.40168,   inf] (53), [-0.40167,   inf] (53), [-0.40139,   inf] (53), [-0.40136,   inf] (53), [-0.40121,   inf] (53), [-0.40110,   inf] (53), [-0.40090,   inf] (53), [-0.40090,   inf] (53), 
length of domains: 58567
Total time: 9.5982	 pickout: 0.7228	 decision: 1.3800	 get_bound: 6.5675	 add_domain: 0.9280
Current lb:-0.40560442209243774
124114 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 144.02535033226013

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1523] [1, 1523] [1, 1523] [1, 1523] [1, 1523] [1, 1523] [1, 1523] [1, 1523] [1, 1523] [1, 1523] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2902.7822265625 with beta sum per layer: [0.0, 4704.2958984375, 88.62992858886719]
alpha/beta optimization time: 3.8261163234710693
This batch time : update_bounds func: 6.5712	 prepare: 0.8421	 bound: 3.8265	 transfer: 0.3156	 finalize: 1.5640
Accumulated time: update_bounds func: 96.1242	 prepare: 12.8338	 bound: 62.1114	 transfer: 0.3156	 finalize: 14.1477
batch bounding time:  6.580039739608765
Current worst splitting domains [lb, ub] (depth):
[-0.40192,   inf] (55), [-0.40165,   inf] (55), [-0.40139,   inf] (55), [-0.40086,   inf] (55), [-0.40068,   inf] (55), [-0.40046,   inf] (55), [-0.40043,   inf] (55), [-0.39947,   inf] (55), [-0.39876,   inf] (55), [-0.39853,   inf] (55), [-0.39800,   inf] (55), [-0.39773,   inf] (55), [-0.39770,   inf] (55), [-0.39754,   inf] (55), [-0.39740,   inf] (55), [-0.39722,   inf] (55), [-0.39717,   inf] (55), [-0.39713,   inf] (55), [-0.39704,   inf] (55), [-0.39703,   inf] (55), 
length of domains: 62663
Total time: 9.6830	 pickout: 0.7354	 decision: 1.4124	 get_bound: 6.5964	 add_domain: 0.9388
Current lb:-0.4019249379634857
132306 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 153.80535221099854

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 874] [2, 58] [2, 58] [1, 874] [2, 58] [2, 58] [2, 58] [2, 58] [2, 58] [2, 58] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2287.62744140625 with beta sum per layer: [0.0, 4376.9130859375, 584.3667602539062]
alpha/beta optimization time: 3.7792787551879883
This batch time : update_bounds func: 6.7213	 prepare: 0.8775	 bound: 3.7797	 transfer: 0.3183	 finalize: 0.5277
Accumulated time: update_bounds func: 102.8454	 prepare: 13.7113	 bound: 65.8911	 transfer: 0.3183	 finalize: 14.6754
batch bounding time:  6.730917453765869
Current worst splitting domains [lb, ub] (depth):
[-0.39865,   inf] (57), [-0.39759,   inf] (57), [-0.39683,   inf] (57), [-0.39655,   inf] (57), [-0.39583,   inf] (57), [-0.39561,   inf] (57), [-0.39556,   inf] (57), [-0.39461,   inf] (57), [-0.39445,   inf] (57), [-0.39420,   inf] (57), [-0.39394,   inf] (57), [-0.39390,   inf] (57), [-0.39372,   inf] (57), [-0.39333,   inf] (57), [-0.39331,   inf] (57), [-0.39319,   inf] (57), [-0.39317,   inf] (57), [-0.39296,   inf] (57), [-0.39287,   inf] (57), [-0.39272,   inf] (57), 
length of domains: 66758
Total time: 9.9123	 pickout: 0.7757	 decision: 1.3956	 get_bound: 6.7476	 add_domain: 0.9933
Current lb:-0.3986528813838959
140498 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 163.82401990890503

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 58] [2, 58] [1, 1466] [1, 1466] [1, 1466] [1, 1466] [1, 1466] [1, 1466] [2, 58] [2, 58] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2565.56103515625 with beta sum per layer: [0.5945098400115967, 4422.72265625, 339.197998046875]
alpha/beta optimization time: 3.7948174476623535
This batch time : update_bounds func: 5.5418	 prepare: 0.8863	 bound: 3.7953	 transfer: 0.3183	 finalize: 0.5189
Accumulated time: update_bounds func: 108.3872	 prepare: 14.5976	 bound: 69.6864	 transfer: 0.3183	 finalize: 15.1943
batch bounding time:  5.550583124160767
Current worst splitting domains [lb, ub] (depth):
[-0.39383,   inf] (59), [-0.39276,   inf] (59), [-0.39066,   inf] (59), [-0.39056,   inf] (59), [-0.39023,   inf] (59), [-0.39016,   inf] (59), [-0.38995,   inf] (59), [-0.38965,   inf] (59), [-0.38958,   inf] (59), [-0.38957,   inf] (59), [-0.38932,   inf] (59), [-0.38929,   inf] (59), [-0.38924,   inf] (59), [-0.38917,   inf] (59), [-0.38908,   inf] (59), [-0.38902,   inf] (59), [-0.38897,   inf] (59), [-0.38850,   inf] (59), [-0.38845,   inf] (59), [-0.38829,   inf] (59), 
length of domains: 70852
Total time: 10.0387	 pickout: 0.7980	 decision: 2.6908	 get_bound: 5.5686	 add_domain: 0.9813
Current lb:-0.39383256435394287
148690 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 173.98372793197632

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1466] [1, 1466] [1, 1466] [1, 874] [0, 2168] [0, 2168] [0, 2168] [1, 1466] [1, 1466] [1, 874] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2720.110107421875 with beta sum per layer: [60.64226531982422, 5192.71875, 129.58407592773438]
alpha/beta optimization time: 3.8336784839630127
This batch time : update_bounds func: 7.0464	 prepare: 0.8657	 bound: 3.8341	 transfer: 0.3184	 finalize: 0.5284
Accumulated time: update_bounds func: 115.4336	 prepare: 15.4633	 bound: 73.5206	 transfer: 0.3184	 finalize: 15.7227
batch bounding time:  7.05504846572876
Current worst splitting domains [lb, ub] (depth):
[-0.38753,   inf] (61), [-0.38725,   inf] (61), [-0.38723,   inf] (61), [-0.38710,   inf] (61), [-0.38694,   inf] (61), [-0.38685,   inf] (61), [-0.38646,   inf] (61), [-0.38623,   inf] (61), [-0.38618,   inf] (61), [-0.38610,   inf] (61), [-0.38610,   inf] (61), [-0.38594,   inf] (61), [-0.38585,   inf] (61), [-0.38585,   inf] (61), [-0.38503,   inf] (61), [-0.38478,   inf] (61), [-0.38434,   inf] (61), [-0.38408,   inf] (61), [-0.38406,   inf] (61), [-0.38381,   inf] (61), 
length of domains: 74947
Total time: 10.3795	 pickout: 0.8016	 decision: 1.4305	 get_bound: 7.0707	 add_domain: 1.0766
Current lb:-0.3875301778316498
156882 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 184.4656810760498

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1047] [1, 1047] [1, 1047] [1, 1047] [1, 1047] [1, 1047] [1, 1047] [1, 1047] [1, 1047] [1, 1047] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 2804.68798828125 with beta sum per layer: [71.51710510253906, 4208.65673828125, 117.17304992675781]
alpha/beta optimization time: 3.797356605529785
This batch time : update_bounds func: 7.0660	 prepare: 0.8818	 bound: 3.7978	 transfer: 0.3155	 finalize: 2.0454
Accumulated time: update_bounds func: 122.4996	 prepare: 16.3451	 bound: 77.3184	 transfer: 0.3155	 finalize: 17.7681
batch bounding time:  7.075580596923828
Current worst splitting domains [lb, ub] (depth):
[-0.38313,   inf] (63), [-0.38295,   inf] (63), [-0.38286,   inf] (63), [-0.38286,   inf] (63), [-0.38269,   inf] (63), [-0.38257,   inf] (63), [-0.38207,   inf] (63), [-0.38194,   inf] (63), [-0.38187,   inf] (63), [-0.38181,   inf] (63), [-0.38168,   inf] (63), [-0.38158,   inf] (63), [-0.38133,   inf] (63)/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/utils.py:462: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  eps_temp = torch.tensor(preprocess_cifar(eps_temp, perturbation=True)).reshape(1,-1,1,1)
, [-0.38109,   inf] (63), [-0.38028,   inf] (63), [-0.38002,   inf] (63), [-0.37970,   inf] (63), [-0.37953,   inf] (63), [-0.37950,   inf] (63), [-0.37944,   inf] (63), 
length of domains: 79042
Total time: 10.2874	 pickout: 0.7642	 decision: 1.4096	 get_bound: 7.0943	 add_domain: 1.0193
Current lb:-0.3831254243850708
165074 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Time out!!!!!!!!
Image 48 label 7 verification end, final lower bound -0.3831254243850708, upper bound inf, time: 196.43179440498352
48 -0.3831254243850708
Result: image 48 verification failure (with branch and bound).
Wall time: 206.16316151618958

number of correctly classified examples: 1
incorrectly classified idx (total 0): []
attack success idx (total 0): []
verification success idx (total 0): []
verification failure idx (total 1): [48]
final verified acc: 0.0%[1]
verifier is called on 1 examples.
total verified: 0
mean time [cnt:1] (excluding attack success): 206.08177781105042
