Building native CUDA modules...
/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
model:
  path: cifar_resnet_8px.pth
  name: model_resnet
data:
  start: 3529
  end: 3530
  num_outputs: 10
  mean: [0.485, 0.456, 0.406]
  std: [0.225, 0.225, 0.225]
  pkl_path: null
  dataset: CIFAR
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: 0.03137254901
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 8
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 20
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 180
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: before
  use_auto_attack: false
  use_diversed_pgd: false
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 50
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 21:39:25 2022 on diablo.cs.ucla.edu
DenseSequential(
  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (3): ReLU()
  (4): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): None
      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (5): ReLU()
  (6): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (7): ReLU()
  (8): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): None
      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (9): ReLU()
  (10): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (11): ReLU()
  (12): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2))
      (1): None
      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (13): ReLU()
  (14): Dense(
    (Ws): ModuleList(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (15): ReLU()
  (16): Dense(
    (Ws): ModuleList(
      (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): None
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (17): ReLU()
  (18): Flatten()
  (19): Linear(in_features=4096, out_features=1000, bias=True)
  (20): ReLU()
  (21): Linear(in_features=1000, out_features=10, bias=True)
)
Trying generic MNIST/CIFAR data loader.
Files already downloaded and verified
epsilon after preprocessing: tensor([[[[0.1394]],

         [[0.1394]],

         [[0.1394]]]]), data_max = tensor([[[[2.2889]],

         [[2.4178]],

         [[2.6400]]]]), data_min = tensor([[[[-2.1556]],

         [[-2.0267]],

         [[-1.8044]]]])
Task length: 1
saving results to Verified_ret_[model_resnet]_start=3529_end=3530_iter=20_b=8_timeout=180_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=before.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 3529 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label 6, correct label 6, image norm 2256.571044921875, logits tensor([-0.7514, -0.0515,  0.3028,  0.3769,  0.2760,  0.4206,  0.5832,  0.1171,
        -0.6951, -0.5786], device='cuda:0', grad_fn=<SelectBackward>)
##### PGD attack: True label: 6, Tested against: ['all'] ######
pgd prediction: tensor([-7.0929e-01,  3.4218e-04,  2.0256e-01,  3.8842e-01,  1.3783e-01,
         4.3501e-01,  4.3620e-01,  1.2178e-01, -5.8402e-01, -4.2888e-01],
       device='cuda:0', grad_fn=<SqueezeBackward1>)
attack margin tensor([1.1455, 0.4359, 0.2336, 0.0478, 0.2984, 0.0012,    inf, 0.3144, 1.0202,
        0.8651], device='cuda:0', grad_fn=<RsubBackward1>)
untargeted pgd failed
Model prediction is: tensor([[-0.7514, -0.0515,  0.3028,  0.3769,  0.2760,  0.4206,  0.5832,  0.1171,
         -0.6951, -0.5786]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[ 1.0885,  0.3509,  0.2245,  0.0169,  0.2717, -0.0367,  0.2588,  0.9327,
          0.7781]], device='cuda:0') None
best_l after optimization: -4.138127326965332 with beta sum per layer: []
alpha/beta optimization time: 38.58559203147888
initial alpha-CROWN bounds: tensor([[ 1.1174,  0.3920,  0.2312,  0.0395,  0.2799, -0.0097,  0.2905,  0.9787,
          0.8187]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-0.0097, device='cuda:0', grad_fn=<MinBackward1>)
Sorted order for labels to verify: [5, 3, 2, 4, 7, 1, 9, 8, 0, 6]
##### [0:3529] Tested against 5 ######
Model prediction is: tensor([[-0.7514, -0.0515,  0.3028,  0.3769,  0.2760,  0.4206,  0.5832,  0.1171,
         -0.6951, -0.5786]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /32 start_node /41
setting alpha for layer /32 start_node /45
setting alpha for layer /32 start_node /51
setting alpha for layer /32 start_node /55
setting alpha for layer /32 start_node /71
setting alpha for layer /32 start_node /81
not setting layer /32 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 16, 32, 32]) != torch.Size([2, 9, 1, 16, 32, 32]))
setting alpha for layer /36 start_node /41
setting alpha for layer /36 start_node /45
setting alpha for layer /36 start_node /51
setting alpha for layer /36 start_node /55
setting alpha for layer /36 start_node /71
setting alpha for layer /36 start_node /81
not setting layer /36 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 16, 32, 32]) != torch.Size([2, 9, 1, 16, 32, 32]))
setting alpha for layer /42 start_node /45
setting alpha for layer /42 start_node /51
setting alpha for layer /42 start_node /55
setting alpha for layer /42 start_node /71
setting alpha for layer /42 start_node /81
not setting layer /42 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 16, 32, 32]) != torch.Size([2, 9, 1, 16, 32, 32]))
setting alpha for layer /46 start_node /51
setting alpha for layer /46 start_node /55
setting alpha for layer /46 start_node /71
setting alpha for layer /46 start_node /81
not setting layer /46 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 16, 32, 32]) != torch.Size([2, 9, 1, 16, 32, 32]))
setting alpha for layer /52 start_node /55
setting alpha for layer /52 start_node /71
setting alpha for layer /52 start_node /81
not setting layer /52 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 16, 32, 32]) != torch.Size([2, 9, 1, 16, 32, 32]))
setting alpha for layer /56 start_node /71
setting alpha for layer /56 start_node /81
not setting layer /56 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 32, 16, 16]) != torch.Size([2, 9, 1, 32, 16, 16]))
setting alpha for layer /62 start_node /71
setting alpha for layer /62 start_node /81
not setting layer /62 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 32, 16, 16]) != torch.Size([2, 9, 1, 32, 16, 16]))
setting alpha for layer /66 start_node /71
setting alpha for layer /66 start_node /81
not setting layer /66 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 64, 8, 8]) != torch.Size([2, 9, 1, 64, 8, 8]))
setting alpha for layer /72 start_node /81
not setting layer /72 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 64, 8, 8]) != torch.Size([2, 9, 1, 64, 8, 8]))
not setting layer /82 start_node /83 because shape mismatch (torch.Size([2, 1, 1, 1000]) != torch.Size([2, 9, 1, 1000]))
0 /31 torch.Size([1, 16, 32, 32])
1 /35 torch.Size([1, 16, 32, 32])
2 /41 torch.Size([1, 16, 32, 32])
3 /45 torch.Size([1, 16, 32, 32])
4 /51 torch.Size([1, 16, 32, 32])
5 /55 torch.Size([1, 32, 16, 16])
6 /61 torch.Size([1, 32, 16, 16])
7 /65 torch.Size([1, 64, 8, 8])
8 /71 torch.Size([1, 64, 8, 8])
9 /81 torch.Size([1, 1000])
best_l after optimization: 0.00966787338256836 with beta sum per layer: []
alpha/beta optimization time: 5.049656629562378
alpha-CROWN with fixed intermediate bounds: tensor([[-0.0097]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-0.00966787338256836
layer 0 size torch.Size([16384]) unstable 1188
layer 1 size torch.Size([16384]) unstable 0
layer 2 size torch.Size([16384]) unstable 416
layer 3 size torch.Size([16384]) unstable 2
layer 4 size torch.Size([16384]) unstable 64
layer 5 size torch.Size([8192]) unstable 2
layer 6 size torch.Size([8192]) unstable 0
layer 7 size torch.Size([4096]) unstable 0
layer 8 size torch.Size([4096]) unstable 12
layer 9 size torch.Size([1000]) unstable 41
-----------------
# of unstable neurons: 1725
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [9, 972] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: 0.0035142330452799797 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7948918342590332
This batch time : update_bounds func: 0.8034	 prepare: 0.0049	 bound: 0.7957	 transfer: 0.0022	 finalize: 0.0006
Accumulated time: update_bounds func: 0.8034	 prepare: 0.0049	 bound: 0.7957	 transfer: 0.0022	 finalize: 0.0006
batch bounding time:  0.8038029670715332
Current worst splitting domains [lb, ub] (depth):
[-0.00539,   inf] (2), 
length of domains: 1
Total time: 0.8910	 pickout: 0.0033	 decision: 0.0837	 get_bound: 0.8038	 add_domain: 0.0002
Current lb:-0.005389988422393799
2 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.9463348388671875

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3300] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: 0.0014761649072170258 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7489478588104248
This batch time : update_bounds func: 0.7558	 prepare: 0.0035	 bound: 0.7496	 transfer: 0.0019	 finalize: 0.0008
Accumulated time: update_bounds func: 1.5592	 prepare: 0.0085	 bound: 1.5452	 transfer: 0.0019	 finalize: 0.0014
batch bounding time:  0.7561359405517578
Current worst splitting domains [lb, ub] (depth):
[-0.00529,   inf] (4), 
length of domains: 1
Total time: 0.8278	 pickout: 0.0026	 decision: 0.0689	 get_bound: 0.7562	 add_domain: 0.0002
Current lb:-0.005285501480102539
4 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.774328947067261

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3302] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.003544926643371582 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7134132385253906
This batch time : update_bounds func: 0.7212	 prepare: 0.0036	 bound: 0.7140	 transfer: 0.0027	 finalize: 0.0008
Accumulated time: update_bounds func: 2.2804	 prepare: 0.0120	 bound: 2.2592	 transfer: 0.0027	 finalize: 0.0021
batch bounding time:  0.7215282917022705
Current worst splitting domains [lb, ub] (depth):
[-0.00525,   inf] (6), 
length of domains: 1
Total time: 0.7914	 pickout: 0.0026	 decision: 0.0670	 get_bound: 0.7215	 add_domain: 0.0003
Current lb:-0.005251407623291016
6 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 8.565968990325928

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [9, 514] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.0033487090840935707 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7113237380981445
This batch time : update_bounds func: 0.7186	 prepare: 0.0036	 bound: 0.7119	 transfer: 0.0024	 finalize: 0.0007
Accumulated time: update_bounds func: 2.9990	 prepare: 0.0156	 bound: 2.9712	 transfer: 0.0024	 finalize: 0.0029
batch bounding time:  0.7189052104949951
Current worst splitting domains [lb, ub] (depth):
[-0.00397,   inf] (8), 
length of domains: 1
Total time: 0.7892	 pickout: 0.0028	 decision: 0.0672	 get_bound: 0.7189	 add_domain: 0.0003
Current lb:-0.0039713382720947266
8 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.355355739593506

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3292] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.011639416217803955 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7375140190124512
This batch time : update_bounds func: 0.7469	 prepare: 0.0036	 bound: 0.7381	 transfer: 0.0019	 finalize: 0.0033
Accumulated time: update_bounds func: 3.7459	 prepare: 0.0192	 bound: 3.7092	 transfer: 0.0019	 finalize: 0.0061
batch bounding time:  0.7479689121246338
Current worst splitting domains [lb, ub] (depth):
[-0.00395,   inf] (10), 
length of domains: 1
Total time: 0.8185	 pickout: 0.0027	 decision: 0.0673	 get_bound: 0.7480	 add_domain: 0.0005
Current lb:-0.003946125507354736
10 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.174243450164795

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3273] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: 0.0012764334678649902 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7406570911407471
This batch time : update_bounds func: 0.7487	 prepare: 0.0037	 bound: 0.7413	 transfer: 0.0025	 finalize: 0.0012
Accumulated time: update_bounds func: 4.4946	 prepare: 0.0229	 bound: 4.4505	 transfer: 0.0025	 finalize: 0.0073
batch bounding time:  0.7490513324737549
Current worst splitting domains [lb, ub] (depth):
[-0.00394,   inf] (12), 
length of domains: 1
Total time: 0.8222	 pickout: 0.0034	 decision: 0.0694	 get_bound: 0.7491	 add_domain: 0.0002
Current lb:-0.003939449787139893
12 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.996620178222656

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3272] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.006807575933635235 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7324843406677246
This batch time : update_bounds func: 0.7397	 prepare: 0.0037	 bound: 0.7331	 transfer: 0.0021	 finalize: 0.0007
Accumulated time: update_bounds func: 5.2343	 prepare: 0.0266	 bound: 5.1836	 transfer: 0.0021	 finalize: 0.0081
batch bounding time:  0.7400031089782715
Current worst splitting domains [lb, ub] (depth):
[-0.00393,   inf] (14), 
length of domains: 1
Total time: 0.8119	 pickout: 0.0026	 decision: 0.0690	 get_bound: 0.7400	 add_domain: 0.0002
Current lb:-0.003932356834411621
14 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 11.808725833892822

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [9, 564] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.002243043389171362 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23732632398605347]
alpha/beta optimization time: 0.8837118148803711
This batch time : update_bounds func: 0.8943	 prepare: 0.0038	 bound: 0.8865	 transfer: 0.0030	 finalize: 0.0009
Accumulated time: update_bounds func: 6.1286	 prepare: 0.0304	 bound: 6.0700	 transfer: 0.0030	 finalize: 0.0090
batch bounding time:  0.89475417137146
Current worst splitting domains [lb, ub] (depth):
[-0.00293,   inf] (16), 
length of domains: 1
Total time: 0.9677	 pickout: 0.0025	 decision: 0.0700	 get_bound: 0.8948	 add_domain: 0.0003
Current lb:-0.0029251575469970703
16 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 12.776696920394897

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3317] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: 0.0037097930908203125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7394945621490479
This batch time : update_bounds func: 0.7468	 prepare: 0.0037	 bound: 0.7401	 transfer: 0.0024	 finalize: 0.0007
Accumulated time: update_bounds func: 6.8754	 prepare: 0.0340	 bound: 6.8101	 transfer: 0.0024	 finalize: 0.0097
batch bounding time:  0.7471063137054443
Current worst splitting domains [lb, ub] (depth):
[-0.00292,   inf] (18), [-0.00079,   inf] (18), 
length of domains: 2
Total time: 0.8198	 pickout: 0.0029	 decision: 0.0695	 get_bound: 0.7471	 add_domain: 0.0003
Current lb:-0.0029210448265075684
18 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 13.596646547317505

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([2, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [9, 349] [9, 349] 
regular batch size: 2*2, diving batch size 1*0
best_l after optimization: -0.0007459262851625681 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.728419303894043
This batch time : update_bounds func: 0.7379	 prepare: 0.0041	 bound: 0.7290	 transfer: 0.0035	 finalize: 0.0012
Accumulated time: update_bounds func: 7.6133	 prepare: 0.0381	 bound: 7.5391	 transfer: 0.0035	 finalize: 0.0109
batch bounding time:  0.7382571697235107
Current worst splitting domains [lb, ub] (depth):
[-0.00180,   inf] (20), 
length of domains: 1
Total time: 0.8227	 pickout: 0.0035	 decision: 0.0806	 get_bound: 0.7383	 add_domain: 0.0003
Current lb:-0.0017969608306884766
22 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 14.4196617603302

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3294] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.008163411170244217 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7300975322723389
This batch time : update_bounds func: 0.7375	 prepare: 0.0036	 bound: 0.7307	 transfer: 0.0024	 finalize: 0.0007
Accumulated time: update_bounds func: 8.3508	 prepare: 0.0418	 bound: 8.2698	 transfer: 0.0024	 finalize: 0.0116
batch bounding time:  0.7377486228942871
Current worst splitting domains [lb, ub] (depth):
[-0.00180,   inf] (22), 
length of domains: 1
Total time: 0.8083	 pickout: 0.0028	 decision: 0.0676	 get_bound: 0.7378	 add_domain: 0.0002
Current lb:-0.001795720192603767
24 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 15.228163242340088

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3327] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: 0.0010963082313537598 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7591667175292969
This batch time : update_bounds func: 0.7677	 prepare: 0.0036	 bound: 0.7598	 transfer: 0.0033	 finalize: 0.0010
Accumulated time: update_bounds func: 9.1185	 prepare: 0.0454	 bound: 9.0296	 transfer: 0.0033	 finalize: 0.0126
batch bounding time:  0.7680635452270508
Current worst splitting domains [lb, ub] (depth):
[-0.00180,   inf] (24), 
length of domains: 1
Total time: 0.8385	 pickout: 0.0026	 decision: 0.0675	 get_bound: 0.7681	 add_domain: 0.0003
Current lb:-0.0017954111099243164
26 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 16.066864728927612

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [9, 654] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: 0.00035309791564941406 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7595574855804443
This batch time : update_bounds func: 0.7701	 prepare: 0.0067	 bound: 0.7603	 transfer: 0.0024	 finalize: 0.0007
Accumulated time: update_bounds func: 9.8886	 prepare: 0.0521	 bound: 9.7899	 transfer: 0.0024	 finalize: 0.0133
batch bounding time:  0.7704150676727295
Current worst splitting domains [lb, ub] (depth):
[-0.00074,   inf] (26), 
length of domains: 1
Total time: 0.8554	 pickout: 0.0036	 decision: 0.0811	 get_bound: 0.7704	 add_domain: 0.0002
Current lb:-0.0007435083389282227
28 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 16.92241907119751

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [9, 62] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.0011888742446899414 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.715977668762207
This batch time : update_bounds func: 0.7228	 prepare: 0.0036	 bound: 0.7166	 transfer: 0.0019	 finalize: 0.0007
Accumulated time: update_bounds func: 10.6114	 prepare: 0.0557	 bound: 10.5065	 transfer: 0.0019	 finalize: 0.0140
batch bounding time:  0.7230932712554932
Current worst splitting domains [lb, ub] (depth):
[-0.00072,   inf] (28), 
length of domains: 1
Total time: 0.7953	 pickout: 0.0026	 decision: 0.0694	 get_bound: 0.7231	 add_domain: 0.0002
Current lb:-0.0007165670394897461
30 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 17.717923641204834

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [8, 3308] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: 0.00029751803958788514 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7312843799591064
This batch time : update_bounds func: 0.7381	 prepare: 0.0036	 bound: 0.7319	 transfer: 0.0019	 finalize: 0.0007
Accumulated time: update_bounds func: 11.3495	 prepare: 0.0592	 bound: 11.2383	 transfer: 0.0019	 finalize: 0.0147
batch bounding time:  0.7383425235748291
Current worst splitting domains [lb, ub] (depth):
[-0.00068,   inf] (30), 
length of domains: 1
Total time: 0.8085	 pickout: 0.0025	 decision: 0.0674	 get_bound: 0.7384	 add_domain: 0.0002
Current lb:-0.0006845828611403704
32 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 18.52660822868347

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [0, 7826] 
regular batch size: 2*1, diving batch size 1*0
best_l after optimization: -0.0023880410008132458 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.7272095680236816
This batch time : update_bounds func: 0.7351	 prepare: 0.0037	 bound: 0.7278	 transfer: 0.0028	 finalize: 0.0008
Accumulated time: update_bounds func: 12.0845	 prepare: 0.0629	 bound: 11.9662	 transfer: 0.0028	 finalize: 0.0155
batch bounding time:  0.7354764938354492
Current worst splitting domains [lb, ub] (depth):
[-0.00064,   inf] (32), 
length of domains: 1
Total time: 0.8057	 pickout: 0.0025	 decision: 0.0674	 get_bound: 0.7355	 add_domain: 0.0003
Current lb:-0.0006428956985473633
34 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 19.332550048828125

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 32, 32]) pre split depth:  1
batch:  torch.Size([1, 16, 32, 32]) post split depth:  1
splitting decisions: 
split level 0: [9, 777] 
regular batch size: 2*1, diving batch size 1*0

all verified at 0th iter
best_l after optimization: -0.000735163688659668 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.031158924102783203
This batch time : update_bounds func: 0.0383	 prepare: 0.0036	 bound: 0.0317	 transfer: 0.0024	 finalize: 0.0006
Accumulated time: update_bounds func: 12.1229	 prepare: 0.0665	 bound: 11.9979	 transfer: 0.0024	 finalize: 0.0161
batch bounding time:  0.038364410400390625
Current worst splitting domains [lb, ub] (depth):

length of domains: 0
Total time: 0.1086	 pickout: 0.0028	 decision: 0.0674	 get_bound: 0.0384	 add_domain: 0.0000
No domains left, verification finished!
Global ub: inf, batch ub: inf
Cumulative time: 19.441407203674316

Image 3529 label 5 verification end, final lower bound 1.0000000116860974e-07, upper bound inf, time: 19.78110432624817
3529 1.0000000116860974e-07
##### [0:3529] Tested against 3 ######
Initial alpha-CROWN verified for label 3 with bound 0.039489805698394775
Image 3529 label 3 verification end, final lower bound 0.039489805698394775, upper bound inf, time: 0.000400543212890625
3529 0.039489805698394775
##### [0:3529] Tested against 2 ######
Initial alpha-CROWN verified for label 2 with bound 0.23118092119693756
Image 3529 label 2 verification end, final lower bound 0.23118092119693756, upper bound inf, time: 0.0003864765167236328
3529 0.23118092119693756
##### [0:3529] Tested against 4 ######
Initial alpha-CROWN verified for label 4 with bound 0.2799108326435089
Image 3529 label 4 verification end, final lower bound 0.2799108326435089, upper bound inf, time: 0.0003764629364013672
3529 0.2799108326435089
##### [0:3529] Tested against 7 ######
Initial alpha-CROWN verified for label 7 with bound 0.2904583811759949
Image 3529 label 7 verification end, final lower bound 0.2904583811759949, upper bound inf, time: 0.0003833770751953125
3529 0.2904583811759949
##### [0:3529] Tested against 1 ######
Initial alpha-CROWN verified for label 1 with bound 0.3919576406478882
Image 3529 label 1 verification end, final lower bound 0.3919576406478882, upper bound inf, time: 0.00037932395935058594
3529 0.3919576406478882
##### [0:3529] Tested against 9 ######
Initial alpha-CROWN verified for label 9 with bound 0.8187490701675415
Image 3529 label 9 verification end, final lower bound 0.8187490701675415, upper bound inf, time: 0.00037860870361328125
3529 0.8187490701675415
##### [0:3529] Tested against 8 ######
Initial alpha-CROWN verified for label 8 with bound 0.9786539077758789
Image 3529 label 8 verification end, final lower bound 0.9786539077758789, upper bound inf, time: 0.0005273818969726562
3529 0.9786539077758789
##### [0:3529] Tested against 0 ######
Initial alpha-CROWN verified for label 0 with bound 1.1173951625823975
Image 3529 label 0 verification end, final lower bound 1.1173951625823975, upper bound inf, time: 0.00038123130798339844
3529 1.1173951625823975
##### [0:3529] Tested against 6 ######
groundtruth label, skip!
Result: image 3529 verification success (with branch and bound)!
Wall time: 68.48422574996948

number of correctly classified examples: 1
incorrectly classified idx (total 0): []
attack success idx (total 0): []
verification success idx (total 1): [3529]
verification failure idx (total 0): []
final verified acc: 100.0%[1]
verifier is called on 1 examples.
total verified: 1
mean time [cnt:1] (excluding attack success): 60.411470890045166
