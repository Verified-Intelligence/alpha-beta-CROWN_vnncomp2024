Experiments at Wed Apr 13 14:01:03 2022 on diablo.cs.ucla.edu
DenseSequential(
  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (3): ReLU()
  (4): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): None
      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (5): ReLU()
  (6): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (7): ReLU()
  (8): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): None
      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (9): ReLU()
  (10): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (11): ReLU()
  (12): Dense(
    (Ws): ModuleList(
      (0): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2))
      (1): None
      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (13): ReLU()
  (14): Dense(
    (Ws): ModuleList(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (15): ReLU()
  (16): Dense(
    (Ws): ModuleList(
      (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): None
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (17): ReLU()
  (18): Flatten()
  (19): Linear(in_features=4096, out_features=1000, bias=True)
  (20): ReLU()
  (21): Linear(in_features=1000, out_features=10, bias=True)
)
dataset not supported! Trying generic data loader.
Files already downloaded and verified
epsilon after preprocession: tensor([[[[0.1394]],

         [[0.1394]],

         [[0.1394]]]]), data_max = tensor([[[[2.2889]],

         [[2.4178]],

         [[2.6400]]]]), data_min = tensor([[[[-2.1556]],

         [[-2.0267]],

         [[-1.8044]]]])
Task length: 1
saving results to Verified_ret_[model_resnet]_start=9134_end=9135_iter=20_b=8_int-beta=False_timeout=180_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=before.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 9134 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label  5  correct label  5 logits tensor([-0.5983,  0.1263,  0.1062,  0.2496, -0.0483,  0.3812,  0.1806,  0.1379,
        -0.4408, -0.0944], device='cuda:0', grad_fn=<SelectBackward>)
False
##### PGD attack: True label: 5, Tested against: ['all'] ######
pgd prediction: tensor([-0.6772,  0.0470,  0.2072,  0.2912,  0.1006,  0.3893,  0.3659,  0.1342,
        -0.5866, -0.2716], device='cuda:0', grad_fn=<SqueezeBackward1>)
attack margin tensor([1.0664, 0.3423, 0.1821, 0.0981, 0.2887,    inf, 0.0234, 0.2550, 0.9759,
        0.6609], device='cuda:0', grad_fn=<RsubBackward1>)
untargeted pgd failed
Model prediction is: tensor([[-0.5983,  0.1263,  0.1062,  0.2496, -0.0483,  0.3812,  0.1806,  0.1379,
         -0.4408, -0.0944]], device='cuda:0', grad_fn=<AddBackward0>)
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[ 0.7919,  0.0584,  0.1641,  0.0404,  0.2624, -0.0023,  0.1081,  0.5588,
          0.1367]], device='cuda:0') None

all verified at 9th iter
best_l after optimization: -2.1849331855773926 with beta sum per layer: []
optimal alpha/beta time: 7.9973695278167725
initial alpha-CROWN bounds: tensor([[8.0104e-01, 6.8429e-02, 1.6539e-01, 4.0781e-02, 2.6475e-01, 3.7694e-04,
         1.1251e-01, 5.7183e-01, 1.5982e-01]], device='cuda:0',
       grad_fn=<AsStridedBackward>) None
verified with init bound!
incomplete verified success!
[[9134.            0.            0.            7.28478765    0.
    -3.                   inf           inf]
 [9134.            0.            0.            9.79740024    0.
    -1.                   inf           inf]]
final verified acc: 100.0%[1]
Total verification count: 1 total verified: 1
mean time [cnt:1] (excluding attack success): 9.79740023612976
