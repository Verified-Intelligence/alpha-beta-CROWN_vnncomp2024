Building native CUDA modules...
/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
model:
  path: cifar_conv_small_pgd.pth
  name: cifar_conv_small
data:
  start: 761
  end: 762
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR_ERAN
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: 0.00784313725
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 2048
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 20
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 120
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  use_auto_attack: false
  use_diversed_pgd: false
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 100
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 21:33:28 2022 on diablo.cs.ucla.edu
Sequential(
  (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2))
  (1): ReLU()
  (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))
  (3): ReLU()
  (4): Flatten()
  (5): Linear(in_features=1152, out_features=100, bias=True)
  (6): ReLU()
  (7): Linear(in_features=100, out_features=10, bias=True)
)
############################
Sampled data loaded. Data already preprocessed!
Shape: torch.Size([1000, 3, 32, 32]) torch.Size([1000]) torch.Size([1000])
X range: tensor(2.7537) tensor(-2.4291) tensor(0.0238)
Note runnerup label is empty here!
############################
epsilon after preprocessing: tensor([[[[0.0388]],

         [[0.0393]],

         [[0.0390]]]]), data_max = tensor([[[[2.5141]],

         [[2.5968]],

         [[2.7537]]]]), data_min = tensor([[[[-2.4291]],

         [[-2.4183]],

         [[-2.2214]]]])
Task length: 1
saving results to Verified_ret_[cifar_conv_small]_start=761_end=762_iter=20_b=2048_timeout=120_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=skip.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 761 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label 6, correct label 6, image norm 2884.7705078125, logits tensor([-2.1852, -7.0448,  1.5912,  3.3089,  2.6246,  3.4353,  4.4936,  1.8950,
        -1.2877, -4.6354], device='cuda:0', grad_fn=<SelectBackward>)
Model prediction is: tensor([[-2.1852, -7.0448,  1.5912,  3.3089,  2.6246,  3.4353,  4.4936,  1.8950,
         -1.2877, -4.6354]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[ 3.5560,  8.6112, -0.3353, -1.0985, -1.2133, -1.2052, -0.9956,  2.9935,
          5.9507]], device='cuda:0') None
best_l after optimization: -17.17147445678711 with beta sum per layer: []
alpha/beta optimization time: 7.307314157485962
initial alpha-CROWN bounds: tensor([[ 3.6746,  8.7124, -0.2414, -0.9980, -1.1065, -1.1253, -0.8861,  3.0848,
          6.0571]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-1.1253, device='cuda:0', grad_fn=<MinBackward1>)
##### [0:761] Tested against 5 ######
Model prediction is: tensor([[-2.1852, -7.0448,  1.5912,  3.3089,  2.6246,  3.4353,  4.4936,  1.8950,
         -1.2877, -4.6354]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /10 start_node /11
setting alpha for layer /10 start_node /21
not setting layer /10 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 16, 15, 15]) != torch.Size([2, 9, 1, 16, 15, 15]))
setting alpha for layer /12 start_node /21
not setting layer /12 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 32, 6, 6]) != torch.Size([2, 9, 1, 32, 6, 6]))
not setting layer /22 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /9 torch.Size([1, 16, 15, 15])
1 /11 torch.Size([1, 32, 6, 6])
2 /21 torch.Size([1, 100])
best_l after optimization: 1.1251424551010132 with beta sum per layer: []
alpha/beta optimization time: 1.8020250797271729
alpha-CROWN with fixed intermediate bounds: tensor([[-1.1251]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-1.1251424551010132
layer 0 size torch.Size([3600]) unstable 455
layer 1 size torch.Size([1152]) unstable 135
layer 2 size torch.Size([100]) unstable 38
-----------------
# of unstable neurons: 628
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 15, 15]) pre split depth:  7
batch:  torch.Size([1, 16, 15, 15]) post split depth:  7
splitting decisions: 
split level 0: [2, 91] 
split level 1: [2, 3] 
split level 2: [2, 76] 
split level 3: [2, 49] 
split level 4: [2, 64] 
split level 5: [2, 43] 
split level 6: [2, 88] 
regular batch size: 2*64, diving batch size 1*0
best_l after optimization: 16.106409072875977 with beta sum per layer: [0.0, 0.0, 110.29788208007812]
alpha/beta optimization time: 0.2635974884033203
This batch time : update_bounds func: 0.2835	 prepare: 0.0096	 bound: 0.2639	 transfer: 0.0020	 finalize: 0.0075
Accumulated time: update_bounds func: 0.2835	 prepare: 0.0096	 bound: 0.2639	 transfer: 0.0020	 finalize: 0.0075
batch bounding time:  0.2837541103363037
Current worst splitting domains [lb, ub] (depth):
[-0.57000,   inf] (8), [-0.55376,   inf] (8), [-0.51637,   inf] (8), [-0.50046,   inf] (8), [-0.49092,   inf] (8), [-0.48477,   inf] (8), [-0.46525,   inf] (8), [-0.45639,   inf] (8), [-0.45274,   inf] (8), [-0.45132,   inf] (8), [-0.43508,   inf] (8), [-0.42871,   inf] (8), [-0.41444,   inf] (8), [-0.41443,   inf] (8), [-0.41357,   inf] (8), [-0.40482,   inf] (8), [-0.40242,   inf] (8), [-0.39514,   inf] (8), [-0.39140,   inf] (8), [-0.37392,   inf] (8), 
length of domains: 81
Total time: 0.3464	 pickout: 0.0008	 decision: 0.0422	 get_bound: 0.2995	 add_domain: 0.0039
Current lb:-0.5699995756149292
128 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 2.9958834648132324

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([81, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([81, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 99] [2, 99] [2, 99] [2, 99] [2, 99] [2, 99] [2, 99] [2, 99] [2, 99] [2, 99] 
regular batch size: 2*81, diving batch size 1*0
best_l after optimization: 27.566579818725586 with beta sum per layer: [0.0, 0.0, 152.1669158935547]
alpha/beta optimization time: 0.2590761184692383
This batch time : update_bounds func: 0.2896	 prepare: 0.0141	 bound: 0.2594	 transfer: 0.0065	 finalize: 0.0092
Accumulated time: update_bounds func: 0.5731	 prepare: 0.0237	 bound: 0.5233	 transfer: 0.0065	 finalize: 0.0168
batch bounding time:  0.28986692428588867
Current worst splitting domains [lb, ub] (depth):
[-0.51703,   inf] (10), [-0.50967,   inf] (10), [-0.50271,   inf] (10), [-0.48972,   inf] (10), [-0.45826,   inf] (10), [-0.45811,   inf] (10), [-0.44638,   inf] (10), [-0.44051,   inf] (10), [-0.43449,   inf] (10), [-0.43233,   inf] (10), [-0.43072,   inf] (10), [-0.42426,   inf] (10), [-0.40604,   inf] (10), [-0.40471,   inf] (10), [-0.39991,   inf] (10), [-0.39766,   inf] (10), [-0.39688,   inf] (10), [-0.39465,   inf] (10), [-0.38891,   inf] (10), [-0.38598,   inf] (10), 
length of domains: 143
Total time: 0.3551	 pickout: 0.0120	 decision: 0.0457	 get_bound: 0.2902	 add_domain: 0.0073
Current lb:-0.5170302987098694
290 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.3522074222564697

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([143, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([143, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 28] [2, 28] [2, 89] [2, 89] [2, 89] [2, 89] [2, 28] [2, 28] [2, 89] [2, 28] 
regular batch size: 2*143, diving batch size 1*0
best_l after optimization: 39.46792984008789 with beta sum per layer: [0.0, 0.0, 294.89044189453125]
alpha/beta optimization time: 0.26890993118286133
This batch time : update_bounds func: 0.3196	 prepare: 0.0242	 bound: 0.2692	 transfer: 0.0092	 finalize: 0.0163
Accumulated time: update_bounds func: 0.8927	 prepare: 0.0479	 bound: 0.7925	 transfer: 0.0092	 finalize: 0.0330
batch bounding time:  0.31993722915649414
Current worst splitting domains [lb, ub] (depth):
[-0.48447,   inf] (12), [-0.47732,   inf] (12), [-0.46911,   inf] (12), [-0.45614,   inf] (12), [-0.44156,   inf] (12), [-0.43204,   inf] (12), [-0.42409,   inf] (12), [-0.42388,   inf] (12), [-0.41190,   inf] (12), [-0.40645,   inf] (12), [-0.40041,   inf] (12), [-0.39900,   inf] (12), [-0.39745,   inf] (12), [-0.39518,   inf] (12), [-0.38931,   inf] (12), [-0.38809,   inf] (12), [-0.38747,   inf] (12), [-0.37979,   inf] (12), [-0.37524,   inf] (12), [-0.37186,   inf] (12), 
length of domains: 212
Total time: 0.4113	 pickout: 0.0196	 decision: 0.0607	 get_bound: 0.3204	 add_domain: 0.0106
Current lb:-0.4844719171524048
576 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.765746593475342

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([212, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([212, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 89] [2, 89] [2, 28] [2, 28] [2, 89] [2, 89] [2, 28] [2, 28] [2, 89] [2, 89] 
regular batch size: 2*212, diving batch size 1*0
best_l after optimization: 56.55082702636719 with beta sum per layer: [0.0, 0.0, 466.670654296875]
alpha/beta optimization time: 0.3198873996734619
This batch time : update_bounds func: 0.3927	 prepare: 0.0350	 bound: 0.3202	 transfer: 0.0128	 finalize: 0.0237
Accumulated time: update_bounds func: 1.2853	 prepare: 0.0828	 bound: 1.1127	 transfer: 0.0128	 finalize: 0.0568
batch bounding time:  0.3931307792663574
Current worst splitting domains [lb, ub] (depth):
[-0.45212,   inf] (14), [-0.44498,   inf] (14), [-0.43633,   inf] (14), [-0.42383,   inf] (14), [-0.40785,   inf] (14), [-0.39826,   inf] (14), [-0.39656,   inf] (14), [-0.39228,   inf] (14), [-0.39191,   inf] (14), [-0.38153,   inf] (14), [-0.37832,   inf] (14), [-0.37261,   inf] (14), [-0.37005,   inf] (14), [-0.36598,   inf] (14), [-0.36573,   inf] (14), [-0.36325,   inf] (14), [-0.36301,   inf] (14), [-0.36138,   inf] (14), [-0.35546,   inf] (14), [-0.35353,   inf] (14), 
length of domains: 335
Total time: 0.5212	 pickout: 0.0288	 decision: 0.0802	 get_bound: 0.3937	 add_domain: 0.0185
Current lb:-0.45212239027023315
1000 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.290133237838745

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([335, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([335, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 57] [2, 57] [2, 57] [2, 57] [2, 57] [2, 57] [2, 57] [2, 57] [2, 57] [2, 57] 
regular batch size: 2*335, diving batch size 1*0
best_l after optimization: 0.7488846778869629 with beta sum per layer: [0.0, 0.0, 708.5032958984375]
alpha/beta optimization time: 0.38823366165161133
This batch time : update_bounds func: 0.4994	 prepare: 0.0544	 bound: 0.3886	 transfer: 0.0167	 finalize: 0.0381
Accumulated time: update_bounds func: 1.7847	 prepare: 0.1372	 bound: 1.5013	 transfer: 0.0167	 finalize: 0.0949
batch bounding time:  0.5000073909759521
Current worst splitting domains [lb, ub] (depth):
[-0.42296,   inf] (16), [-0.41571,   inf] (16), [-0.40895,   inf] (16), [-0.39608,   inf] (16), [-0.37851,   inf] (16), [-0.36829,   inf] (16), [-0.36821,   inf] (16), [-0.36269,   inf] (16), [-0.36250,   inf] (16), [-0.35219,   inf] (16), [-0.34805,   inf] (16), [-0.34191,   inf] (16), [-0.34018,   inf] (16), [-0.33844,   inf] (16), [-0.33825,   inf] (16), [-0.33500,   inf] (16), [-0.33368,   inf] (16), [-0.33128,   inf] (16), [-0.32582,   inf] (16), [-0.32519,   inf] (16), 
length of domains: 302
Total time: 0.7340	 pickout: 0.0458	 decision: 0.1703	 get_bound: 0.5010	 add_domain: 0.0169
Current lb:-0.42296072840690613
1670 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.030690670013428

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([302, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([302, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 95] [2, 82] [2, 82] [2, 82] [2, 95] [2, 95] [2, 95] [2, 82] [2, 82] [2, 95] 
regular batch size: 2*302, diving batch size 1*0
best_l after optimization: 8.802285194396973 with beta sum per layer: [0.0, 0.4809928238391876, 653.279052734375]
alpha/beta optimization time: 0.3652942180633545
This batch time : update_bounds func: 0.4609	 prepare: 0.0506	 bound: 0.3656	 transfer: 0.0086	 finalize: 0.0345
Accumulated time: update_bounds func: 2.2456	 prepare: 0.1878	 bound: 1.8669	 transfer: 0.0086	 finalize: 0.1294
batch bounding time:  0.4614887237548828
Current worst splitting domains [lb, ub] (depth):
[-0.40524,   inf] (18), [-0.39658,   inf] (18), [-0.38984,   inf] (18), [-0.37727,   inf] (18), [-0.36137,   inf] (18), [-0.35122,   inf] (18), [-0.35108,   inf] (18), [-0.34324,   inf] (18), [-0.34290,   inf] (18), [-0.33549,   inf] (18), [-0.33192,   inf] (18), [-0.32577,   inf] (18), [-0.32094,   inf] (18), [-0.32057,   inf] (18), [-0.31923,   inf] (18), [-0.31783,   inf] (18), [-0.31614,   inf] (18), [-0.31253,   inf] (18), [-0.31028,   inf] (18), [-0.30752,   inf] (18), 
length of domains: 282
Total time: 0.6192	 pickout: 0.0414	 decision: 0.0981	 get_bound: 0.4623	 add_domain: 0.0173
Current lb:-0.4052385985851288
2274 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.655572891235352

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([282, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([282, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 82] [2, 95] [2, 95] [2, 95] [2, 82] [2, 82] [2, 82] [2, 95] [2, 95] [2, 82] 
regular batch size: 2*282, diving batch size 1*0
best_l after optimization: 30.406049728393555 with beta sum per layer: [0.0, 2.858524799346924, 684.326416015625]
alpha/beta optimization time: 0.3554661273956299
This batch time : update_bounds func: 0.4811	 prepare: 0.0482	 bound: 0.3558	 transfer: 0.0048	 finalize: 0.0709
Accumulated time: update_bounds func: 2.7267	 prepare: 0.2361	 bound: 2.2227	 transfer: 0.0048	 finalize: 0.2003
batch bounding time:  0.4816734790802002
Current worst splitting domains [lb, ub] (depth):
[-0.38601,   inf] (20), [-0.37907,   inf] (20), [-0.37244,   inf] (20), [-0.36042,   inf] (20), [-0.34176,   inf] (20), [-0.33150,   inf] (20), [-0.33144,   inf] (20), [-0.32686,   inf] (20), [-0.32638,   inf] (20), [-0.31618,   inf] (20), [-0.31262,   inf] (20), [-0.30666,   inf] (20), [-0.30313,   inf] (20), [-0.30142,   inf] (20), [-0.30120,   inf] (20), [-0.30009,   inf] (20), [-0.29867,   inf] (20), [-0.29309,   inf] (20), [-0.29118,   inf] (20), [-0.28809,   inf] (20), 
length of domains: 334
Total time: 0.6350	 pickout: 0.0394	 decision: 0.0926	 get_bound: 0.4825	 add_domain: 0.0205
Current lb:-0.3860071003437042
2838 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.295493841171265

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([334, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([334, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] 
regular batch size: 2*334, diving batch size 1*0
best_l after optimization: 26.467559814453125 with beta sum per layer: [0.0, 7.0013580322265625, 789.5560302734375]
alpha/beta optimization time: 0.38915061950683594
This batch time : update_bounds func: 0.4968	 prepare: 0.0578	 bound: 0.3895	 transfer: 0.0083	 finalize: 0.0395
Accumulated time: update_bounds func: 3.2235	 prepare: 0.2939	 bound: 2.6121	 transfer: 0.0083	 finalize: 0.2398
batch bounding time:  0.49745750427246094
Current worst splitting domains [lb, ub] (depth):
[-0.37024,   inf] (22), [-0.36320,   inf] (22), [-0.35684,   inf] (22), [-0.34467,   inf] (22), [-0.32555,   inf] (22), [-0.31543,   inf] (22), [-0.31534,   inf] (22), [-0.31080,   inf] (22), [-0.31041,   inf] (22), [-0.29986,   inf] (22), [-0.29670,   inf] (22), [-0.29073,   inf] (22), [-0.28754,   inf] (22), [-0.28573,   inf] (22), [-0.28549,   inf] (22), [-0.28443,   inf] (22), [-0.28296,   inf] (22), [-0.27731,   inf] (22), [-0.27519,   inf] (22), [-0.27241,   inf] (22), 
length of domains: 356
Total time: 0.6768	 pickout: 0.0467	 decision: 0.1081	 get_bound: 0.4984	 add_domain: 0.0236
Current lb:-0.3702423572540283
3506 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.9795427322387695

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([356, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([356, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 733] [1, 733] [1, 733] [1, 733] [1, 733] [1, 733] [2, 24] [1, 733] [1, 483] [2, 79] 
regular batch size: 2*356, diving batch size 1*0
best_l after optimization: 34.89862060546875 with beta sum per layer: [0.0, 16.254470825195312, 804.5950927734375]
alpha/beta optimization time: 0.39618778228759766
This batch time : update_bounds func: 0.5554	 prepare: 0.0629	 bound: 0.3965	 transfer: 0.0094	 finalize: 0.0426
Accumulated time: update_bounds func: 3.7789	 prepare: 0.3568	 bound: 3.0087	 transfer: 0.0094	 finalize: 0.2825
batch bounding time:  0.5561556816101074
Current worst splitting domains [lb, ub] (depth):
[-0.35580,   inf] (24), [-0.34870,   inf] (24), [-0.34412,   inf] (24), [-0.33209,   inf] (24), [-0.32866,   inf] (24), [-0.32194,   inf] (24), [-0.31223,   inf] (24), [-0.30964,   inf] (24), [-0.30281,   inf] (24), [-0.30171,   inf] (24), [-0.29951,   inf] (24), [-0.29320,   inf] (24), [-0.29170,   inf] (24), [-0.28795,   inf] (24), [-0.28498,   inf] (24), [-0.28287,   inf] (24), [-0.27684,   inf] (24), [-0.27413,   inf] (24), [-0.27258,   inf] (24), [-0.27106,   inf] (24), 
length of domains: 449
Total time: 0.7560	 pickout: 0.0519	 decision: 0.1163	 get_bound: 0.5572	 add_domain: 0.0307
Current lb:-0.3558039665222168
4218 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.742970943450928

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([449, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([449, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 79] [2, 79] [2, 79] [2, 79] [2, 79] [2, 79] [1, 483] [2, 79] [2, 79] [2, 79] 
regular batch size: 2*449, diving batch size 1*0
best_l after optimization: 53.60670471191406 with beta sum per layer: [0.0, 27.9573917388916, 998.948486328125]
alpha/beta optimization time: 0.4645724296569824
This batch time : update_bounds func: 0.6225	 prepare: 0.0822	 bound: 0.4649	 transfer: 0.0143	 finalize: 0.0590
Accumulated time: update_bounds func: 4.4014	 prepare: 0.4390	 bound: 3.4736	 transfer: 0.0143	 finalize: 0.3415
batch bounding time:  0.6233785152435303
Current worst splitting domains [lb, ub] (depth):
[-0.34082,   inf] (26), [-0.33347,   inf] (26), [-0.32921,   inf] (26), [-0.31709,   inf] (26), [-0.31375,   inf] (26), [-0.30695,   inf] (26), [-0.29614,   inf] (26), [-0.29474,   inf] (26), [-0.28925,   inf] (26), [-0.28809,   inf] (26), [-0.28709,   inf] (26), [-0.28450,   inf] (26), [-0.27609,   inf] (26), [-0.27530,   inf] (26), [-0.27272,   inf] (26), [-0.27212,   inf] (26), [-0.27094,   inf] (26), [-0.26836,   inf] (26), [-0.26246,   inf] (26), [-0.25770,   inf] (26), 
length of domains: 639
Total time: 0.8901	 pickout: 0.0660	 decision: 0.1525	 get_bound: 0.6247	 add_domain: 0.0469
Current lb:-0.34081971645355225
5116 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 8.641396760940552

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([639, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([639, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 483] [2, 36] [2, 36] [2, 36] [1, 483] [2, 36] [2, 79] [2, 36] [2, 79] [2, 36] 
regular batch size: 2*639, diving batch size 1*0
best_l after optimization: 68.38511657714844 with beta sum per layer: [0.0, 49.243797302246094, 1423.9735107421875]
alpha/beta optimization time: 0.5935928821563721
This batch time : update_bounds func: 0.8315	 prepare: 0.1236	 bound: 0.5939	 transfer: 0.0285	 finalize: 0.0823
Accumulated time: update_bounds func: 5.2329	 prepare: 0.5626	 bound: 4.0675	 transfer: 0.0285	 finalize: 0.4238
batch bounding time:  0.8326511383056641
Current worst splitting domains [lb, ub] (depth):
[-0.32486,   inf] (28), [-0.31947,   inf] (28), [-0.31741,   inf] (28), [-0.31352,   inf] (28), [-0.30144,   inf] (28), [-0.29639,   inf] (28), [-0.29324,   inf] (28), [-0.29168,   inf] (28), [-0.29030,   inf] (28), [-0.28225,   inf] (28), [-0.28109,   inf] (28), [-0.27812,   inf] (28), [-0.27440,   inf] (28), [-0.27188,   inf] (28), [-0.27092,   inf] (28), [-0.26858,   inf] (28), [-0.26817,   inf] (28), [-0.26765,   inf] (28), [-0.25901,   inf] (28), [-0.25816,   inf] (28), 
length of domains: 845
Total time: 1.3247	 pickout: 0.1069	 decision: 0.2522	 get_bound: 0.8345	 add_domain: 0.1311
Current lb:-0.32485994696617126
6394 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.978490591049194

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([845, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([845, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 36] [2, 36] [2, 15] [2, 15] [2, 15] [2, 36] [2, 36] [2, 15] [2, 15] [2, 15] 
regular batch size: 2*845, diving batch size 1*0
best_l after optimization: 104.20072937011719 with beta sum per layer: [0.0, 60.146202087402344, 1815.6070556640625]
alpha/beta optimization time: 0.7278671264648438
This batch time : update_bounds func: 1.0834	 prepare: 0.1602	 bound: 0.7283	 transfer: 0.0335	 finalize: 0.1571
Accumulated time: update_bounds func: 6.3164	 prepare: 0.7228	 bound: 4.7958	 transfer: 0.0335	 finalize: 0.5808
batch bounding time:  1.0849769115447998
Current worst splitting domains [lb, ub] (depth):
[-0.30863,   inf] (30), [-0.30298,   inf] (30), [-0.30135,   inf] (30), [-0.29759,   inf] (30), [-0.28603,   inf] (30), [-0.28561,   inf] (30), [-0.28272,   inf] (30), [-0.27929,   inf] (30), [-0.27618,   inf] (30), [-0.27536,   inf] (30), [-0.27361,   inf] (30), [-0.26609,   inf] (30), [-0.26450,   inf] (30), [-0.26151,   inf] (30), [-0.26113,   inf] (30), [-0.26041,   inf] (30), [-0.25943,   inf] (30), [-0.25765,   inf] (30), [-0.25621,   inf] (30), [-0.25543,   inf] (30), 
length of domains: 1298
Total time: 1.5914	 pickout: 0.1344	 decision: 0.2667	 get_bound: 1.0875	 add_domain: 0.1027
Current lb:-0.30862632393836975
8084 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 11.586135864257812

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1298, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([1298, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 15] [2, 15] [1, 483] [1, 483] [2, 15] [2, 24] [2, 15] [2, 15] [2, 15] [2, 24] 
regular batch size: 2*1298, diving batch size 1*0
best_l after optimization: 100.61235809326172 with beta sum per layer: [0.0, 104.83855438232422, 2854.72265625]
alpha/beta optimization time: 1.0327417850494385
This batch time : update_bounds func: 1.5618	 prepare: 0.2456	 bound: 1.0331	 transfer: 0.0650	 finalize: 0.2114
Accumulated time: update_bounds func: 7.8782	 prepare: 0.9684	 bound: 5.8290	 transfer: 0.0650	 finalize: 0.7922
batch bounding time:  1.563974142074585
Current worst splitting domains [lb, ub] (depth):
[-0.29242,   inf] (32), [-0.28681,   inf] (32), [-0.28648,   inf] (32), [-0.28306,   inf] (32), [-0.28179,   inf] (32), [-0.27647,   inf] (32), [-0.27284,   inf] (32), [-0.26960,   inf] (32), [-0.26626,   inf] (32), [-0.26237,   inf] (32), [-0.26099,   inf] (32), [-0.25941,   inf] (32), [-0.25765,   inf] (32), [-0.25517,   inf] (32), [-0.25337,   inf] (32), [-0.25242,   inf] (32), [-0.24892,   inf] (32), [-0.24870,   inf] (32), [-0.24612,   inf] (32), [-0.24607,   inf] (32), 
length of domains: 1744
Total time: 2.3599	 pickout: 0.1973	 decision: 0.4462	 get_bound: 1.5677	 add_domain: 0.1487
Current lb:-0.2924228012561798
10680 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 13.972805976867676

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1744, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([1744, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 24] [2, 24] [2, 24] [2, 24] [2, 24] [2, 24] [2, 71] [2, 24] [2, 24] [2, 24] 
regular batch size: 2*1744, diving batch size 1*0
best_l after optimization: 76.9888687133789 with beta sum per layer: [0.0, 153.58560180664062, 3641.689453125]
alpha/beta optimization time: 1.314439058303833
This batch time : update_bounds func: 2.0510	 prepare: 0.3534	 bound: 1.3148	 transfer: 0.0895	 finalize: 0.2843
Accumulated time: update_bounds func: 9.9292	 prepare: 1.3218	 bound: 7.1438	 transfer: 0.0895	 finalize: 1.0765
batch bounding time:  2.0541841983795166
Current worst splitting domains [lb, ub] (depth):
[-0.27971,   inf] (34), [-0.27412,   inf] (34), [-0.27346,   inf] (34), [-0.27070,   inf] (34), [-0.26888,   inf] (34), [-0.26424,   inf] (34), [-0.26245,   inf] (34), [-0.25555,   inf] (34), [-0.25231,   inf] (34), [-0.24932,   inf] (34), [-0.24651,   inf] (34), [-0.24564,   inf] (34), [-0.24474,   inf] (34), [-0.24234,   inf] (34), [-0.23990,   inf] (34), [-0.23887,   inf] (34), [-0.23802,   inf] (34), [-0.23616,   inf] (34), [-0.23493,   inf] (34), [-0.23412,   inf] (34), 
length of domains: 2140
Total time: 3.1748	 pickout: 0.2820	 decision: 0.6499	 get_bound: 2.0598	 add_domain: 0.1831
Current lb:-0.27970588207244873
14168 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 17.185192584991455

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 719] [1, 719] [2, 71] [1, 719] [2, 71] [1, 719] [1, 483] [1, 719] [1, 719] [1, 719] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 154.21253967285156 with beta sum per layer: [0.0, 195.7721710205078, 4096.9521484375]
alpha/beta optimization time: 1.5782508850097656
This batch time : update_bounds func: 2.5283	 prepare: 0.3930	 bound: 1.5786	 transfer: 0.1026	 finalize: 0.4434
Accumulated time: update_bounds func: 12.4574	 prepare: 1.7148	 bound: 8.7224	 transfer: 0.1026	 finalize: 1.5199
batch bounding time:  2.5325543880462646
Current worst splitting domains [lb, ub] (depth):
[-0.26888,   inf] (36), [-0.26331,   inf] (36), [-0.26302,   inf] (36), [-0.25988,   inf] (36), [-0.25853,   inf] (36), [-0.25342,   inf] (36), [-0.24996,   inf] (36), [-0.24994,   inf] (36), [-0.24489,   inf] (36), [-0.24437,   inf] (36), [-0.23997,   inf] (36), [-0.23949,   inf] (36), [-0.23830,   inf] (36), [-0.23675,   inf] (36), [-0.23552,   inf] (36), [-0.23434,   inf] (36), [-0.23303,   inf] (36), [-0.23227,   inf] (36), [-0.23196,   inf] (36), [-0.23048,   inf] (36), 
length of domains: 2998
Total time: 3.9446	 pickout: 0.3257	 decision: 0.7948	 get_bound: 2.5398	 add_domain: 0.2842
Current lb:-0.26888224482536316
18264 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 21.17361092567444

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 71] [2, 71] [1, 719] [1, 37] [1, 719] [1, 37] [1, 719] [2, 71] [1, 719] [1, 37] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 196.33892822265625 with beta sum per layer: [0.0, 232.68971252441406, 3711.63037109375]
alpha/beta optimization time: 1.5938529968261719
This batch time : update_bounds func: 2.6909	 prepare: 0.6529	 bound: 1.5943	 transfer: 0.1013	 finalize: 0.3310
Accumulated time: update_bounds func: 15.1483	 prepare: 2.3676	 bound: 10.3168	 transfer: 0.1013	 finalize: 1.8509
batch bounding time:  2.6946287155151367
Current worst splitting domains [lb, ub] (depth):
[-0.25828,   inf] (38), [-0.25287,   inf] (38), [-0.25251,   inf] (38), [-0.24850,   inf] (38), [-0.24804,   inf] (38), [-0.24471,   inf] (38), [-0.24201,   inf] (38), [-0.23974,   inf] (38), [-0.23973,   inf] (38), [-0.23827,   inf] (38), [-0.23466,   inf] (38), [-0.23410,   inf] (38), [-0.23155,   inf] (38), [-0.22955,   inf] (38), [-0.22815,   inf] (38), [-0.22811,   inf] (38), [-0.22722,   inf] (38), [-0.22637,   inf] (38), [-0.22522,   inf] (38), [-0.22465,   inf] (38), 
length of domains: 4043
Total time: 4.4375	 pickout: 0.4302	 decision: 0.9064	 get_bound: 2.7010	 add_domain: 0.3998
Current lb:-0.2582841217517853
22360 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 25.655797243118286

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 134] [1, 134] [1, 134] [2, 71] [1, 134] [2, 71] [2, 71] [1, 134] [1, 37] [2, 71] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 233.10440063476562 with beta sum per layer: [0.011349949054419994, 269.0684814453125, 3285.0947265625]
alpha/beta optimization time: 1.5948550701141357
This batch time : update_bounds func: 2.4621	 prepare: 0.3878	 bound: 1.5952	 transfer: 0.1011	 finalize: 0.3672
Accumulated time: update_bounds func: 17.6104	 prepare: 2.7554	 bound: 11.9120	 transfer: 0.1011	 finalize: 2.2181
batch bounding time:  2.4658825397491455
Current worst splitting domains [lb, ub] (depth):
[-0.24789,   inf] (40), [-0.24561,   inf] (40), [-0.24241,   inf] (40), [-0.24204,   inf] (40), [-0.24024,   inf] (40), [-0.23964,   inf] (40), [-0.23779,   inf] (40), [-0.23758,   inf] (40), [-0.23521,   inf] (40), [-0.23406,   inf] (40), [-0.23140,   inf] (40), [-0.22933,   inf] (40), [-0.22866,   inf] (40), [-0.22770,   inf] (40), [-0.22705,   inf] (40), [-0.22536,   inf] (40), [-0.22358,   inf] (40), [-0.22356,   inf] (40), [-0.22137,   inf] (40), [-0.22126,   inf] (40), 
length of domains: 5175
Total time: 3.9209	 pickout: 0.3235	 decision: 0.8165	 get_bound: 2.4725	 add_domain: 0.3084
Current lb:-0.24789096415042877
26456 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 29.61858034133911

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 37] [1, 37] [1, 37] [1, 37] [1, 37] [1, 37] [1, 134] [1, 37] [1, 37] [1, 134] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 355.95037841796875 with beta sum per layer: [0.0, 312.0931091308594, 2664.01513671875]
alpha/beta optimization time: 1.5827033519744873
This batch time : update_bounds func: 2.4931	 prepare: 0.3883	 bound: 1.5831	 transfer: 0.1041	 finalize: 0.4070
Accumulated time: update_bounds func: 20.1036	 prepare: 3.1437	 bound: 13.4951	 transfer: 0.1041	 finalize: 2.6251
batch bounding time:  2.497138738632202
Current worst splitting domains [lb, ub] (depth):
[-0.23769,   inf] (42), [-0.23540,   inf] (42), [-0.23313,   inf] (42), [-0.23221,   inf] (42), [-0.23195,   inf] (42), [-0.23086,   inf] (42), [-0.23003,   inf] (42), [-0.22958,   inf] (42), [-0.22769,   inf] (42), [-0.22768,   inf] (42), [-0.22750,   inf] (42), [-0.22712,   inf] (42), [-0.22612,   inf] (42), [-0.22551,   inf] (42), [-0.22533,   inf] (42), [-0.22512,   inf] (42), [-0.22347,   inf] (42), [-0.22322,   inf] (42), [-0.22248,   inf] (42), [-0.22084,   inf] (42), 
length of domains: 7038
Total time: 4.0660	 pickout: 0.3254	 decision: 0.8510	 get_bound: 2.5038	 add_domain: 0.3858
Current lb:-0.23768934607505798
30552 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 33.72280025482178

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 725] [1, 725] [1, 725] [1, 725] [1, 1101] [1, 725] [1, 725] [1, 1101] [0, 2541] [1, 725] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 438.22943115234375 with beta sum per layer: [0.0062295133247971535, 347.1464538574219, 2085.218994140625]
alpha/beta optimization time: 1.5733294486999512
This batch time : update_bounds func: 2.5107	 prepare: 0.3932	 bound: 1.5737	 transfer: 0.1038	 finalize: 0.4288
Accumulated time: update_bounds func: 22.6142	 prepare: 3.5369	 bound: 15.0688	 transfer: 0.1038	 finalize: 3.0539
batch bounding time:  2.514587640762329
Current worst splitting domains [lb, ub] (depth):
[-0.23097,   inf] (44), [-0.22868,   inf] (44), [-0.22648,   inf] (44), [-0.22607,   inf] (44), [-0.22553,   inf] (44), [-0.22423,   inf] (44), [-0.22334,   inf] (44), [-0.22310,   inf] (44), [-0.22167,   inf] (44), [-0.22163,   inf] (44), [-0.22106,   inf] (44), [-0.21981,   inf] (44), [-0.21981,   inf] (44), [-0.21970,   inf] (44), [-0.21893,   inf] (44), [-0.21888,   inf] (44), [-0.21866,   inf] (44), [-0.21822,   inf] (44), [-0.21753,   inf] (44), [-0.21744,   inf] (44), 
length of domains: 9086
Total time: 4.1881	 pickout: 0.3243	 decision: 0.9240	 get_bound: 2.5213	 add_domain: 0.4185
Current lb:-0.23096998035907745
34648 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 37.94927167892456

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 1101] [1, 1101] [0, 2541] [0, 2541] [1, 1101] [1, 1101] [1, 1101] [0, 2541] [1, 1101] [1, 137] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 481.73291015625 with beta sum per layer: [0.06898331642150879, 406.6439208984375, 1663.1693115234375]
alpha/beta optimization time: 1.5739071369171143
This batch time : update_bounds func: 2.5279	 prepare: 0.3894	 bound: 1.5743	 transfer: 0.1047	 finalize: 0.4477
Accumulated time: update_bounds func: 25.1421	 prepare: 3.9262	 bound: 16.6431	 transfer: 0.1047	 finalize: 3.5016
batch bounding time:  2.531712293624878
Current worst splitting domains [lb, ub] (depth):
[-0.22497,   inf] (46), [-0.22234,   inf] (46), [-0.22040,   inf] (46), [-0.22033,   inf] (46), [-0.21959,   inf] (46), [-0.21866,   inf] (46), [-0.21815,   inf] (46), [-0.21794,   inf] (46), [-0.21743,   inf] (46), [-0.21701,   inf] (46), [-0.21599,   inf] (46), [-0.21595,   inf] (46), [-0.21519,   inf] (46), [-0.21518,   inf] (46), [-0.21402,   inf] (46), [-0.21395,   inf] (46), [-0.21318,   inf] (46), [-0.21303,   inf] (46), [-0.21303,   inf] (46), [-0.21268,   inf] (46), 
length of domains: 11134
Total time: 4.0906	 pickout: 0.3260	 decision: 0.7941	 get_bound: 2.5384	 add_domain: 0.4321
Current lb:-0.22496873140335083
38744 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 42.07782530784607

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [0, 2541] [0, 2541] [1, 110] [1, 1101] [1, 110] [1, 1101] [1, 110] [0, 2541] [1, 110] [1, 110] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 504.16253662109375 with beta sum per layer: [0.09803935885429382, 487.86334228515625, 1261.0123291015625]
alpha/beta optimization time: 1.5718660354614258
This batch time : update_bounds func: 2.5448	 prepare: 0.3895	 bound: 1.5723	 transfer: 0.1026	 finalize: 0.4695
Accumulated time: update_bounds func: 27.6869	 prepare: 4.3157	 bound: 18.2154	 transfer: 0.1026	 finalize: 3.9711
batch bounding time:  2.54879093170166
Current worst splitting domains [lb, ub] (depth):
[-0.21908,   inf] (48), [-0.21689,   inf] (48), [-0.21643,   inf] (48), [-0.21469,   inf] (48), [-0.21444,   inf] (48), [-0.21424,   inf] (48), [-0.21394,   inf] (48), [-0.21269,   inf] (48), [-0.21254,   inf] (48), [-0.21181,   inf] (48), [-0.21174,   inf] (48), [-0.21137,   inf] (48), [-0.21038,   inf] (48), [-0.21029,   inf] (48), [-0.21007,   inf] (48), [-0.20960,   inf] (48), [-0.20960,   inf] (48), [-0.20846,   inf] (48), [-0.20838,   inf] (48), [-0.20744,   inf] (48), 
length of domains: 13182
Total time: 4.1530	 pickout: 0.3273	 decision: 0.8247	 get_bound: 2.5558	 add_domain: 0.4452
Current lb:-0.21908318996429443
42840 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 46.270589113235474

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 110] [1, 110] [1, 110] [1, 137] [1, 137] [1, 110] [1, 579] [1, 137] [1, 137] [1, 137] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 523.3760986328125 with beta sum per layer: [0.23738911747932434, 500.53118896484375, 1025.025146484375]
alpha/beta optimization time: 1.5723607540130615
This batch time : update_bounds func: 2.6485	 prepare: 0.4028	 bound: 1.5728	 transfer: 0.1055	 finalize: 0.5550
Accumulated time: update_bounds func: 30.3353	 prepare: 4.7185	 bound: 19.7881	 transfer: 0.1055	 finalize: 4.5260
batch bounding time:  2.6521854400634766
Current worst splitting domains [lb, ub] (depth):
[-0.21330,   inf] (50), [-0.21129,   inf] (50), [-0.21066,   inf] (50), [-0.20906,   inf] (50), [-0.20885,   inf] (50), [-0.20865,   inf] (50), [-0.20708,   inf] (50), [-0.20688,   inf] (50), [-0.20620,   inf] (50), [-0.20608,   inf] (50), [-0.20596,   inf] (50), [-0.20468,   inf] (50), [-0.20442,   inf] (50), [-0.20394,   inf] (50), [-0.20340,   inf] (50), [-0.20313,   inf] (50), [-0.20284,   inf] (50), [-0.20279,   inf] (50), [-0.20172,   inf] (50), [-0.20157,   inf] (50), 
length of domains: 15230
Total time: 4.3237	 pickout: 0.3311	 decision: 0.8700	 get_bound: 2.6586	 add_domain: 0.4639
Current lb:-0.21330463886260986
46936 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 50.633594036102295

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 579] [1, 579] [1, 579] [1, 579] [1, 579] [1, 579] [1, 579] [1, 579] [1, 579] [1, 579] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 533.7225341796875 with beta sum per layer: [0.8709709048271179, 520.483642578125, 752.4923706054688]
alpha/beta optimization time: 1.567121982574463
This batch time : update_bounds func: 2.6698	 prepare: 0.4073	 bound: 1.5675	 transfer: 0.1032	 finalize: 0.5807
Accumulated time: update_bounds func: 33.0052	 prepare: 5.1258	 bound: 21.3556	 transfer: 0.1032	 finalize: 5.1068
batch bounding time:  2.67364501953125
Current worst splitting domains [lb, ub] (depth):
[-0.20544,   inf] (52), [-0.20335,   inf] (52), [-0.20280,   inf] (52), [-0.20209,   inf] (52), [-0.20155,   inf] (52), [-0.20090,   inf] (52), [-0.20071,   inf] (52), [-0.20039,   inf] (52), [-0.20035,   inf] (52), [-0.19950,   inf] (52), [-0.19933,   inf] (52), [-0.19914,   inf] (52), [-0.19904,   inf] (52), [-0.19860,   inf] (52), [-0.19829,   inf] (52), [-0.19784,   inf] (52), [-0.19777,   inf] (52), [-0.19753,   inf] (52), [-0.19737,   inf] (52), [-0.19722,   inf] (52), 
length of domains: 17278
Total time: 4.3965	 pickout: 0.3338	 decision: 0.9099	 get_bound: 2.6803	 add_domain: 0.4726
Current lb:-0.20544083416461945
51032 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 55.07134127616882

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 137] [1, 137] [1, 137] [1, 137] [1, 1041] [1, 110] [1, 137] [1, 137] [1, 1041] [1, 137] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 543.7216186523438 with beta sum per layer: [1.076545238494873, 527.3572998046875, 552.3084716796875]
alpha/beta optimization time: 1.571326732635498
This batch time : update_bounds func: 2.3598	 prepare: 0.4162	 bound: 1.5717	 transfer: 0.1035	 finalize: 0.2570
Accumulated time: update_bounds func: 35.3650	 prepare: 5.5421	 bound: 22.9273	 transfer: 0.1035	 finalize: 5.3638
batch bounding time:  2.3639328479766846
Current worst splitting domains [lb, ub] (depth):
[-0.19982,   inf] (54), [-0.19772,   inf] (54), [-0.19720,   inf] (54), [-0.19646,   inf] (54), [-0.19562,   inf] (54), [-0.19509,   inf] (54), [-0.19477,   inf] (54), [-0.19435,   inf] (54), [-0.19387,   inf] (54), [-0.19381,   inf] (54), [-0.19326,   inf] (54), [-0.19298,   inf] (54), [-0.19296,   inf] (54), [-0.19216,   inf] (54), [-0.19213,   inf] (54), [-0.19212,   inf] (54), [-0.19195,   inf] (54), [-0.19126,   inf] (54), [-0.19124,   inf] (54), [-0.19108,   inf] (54), 
length of domains: 19326
Total time: 4.1337	 pickout: 0.3373	 decision: 0.9409	 get_bound: 2.3708	 add_domain: 0.4847
Current lb:-0.19981949031352997
55128 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 59.2469801902771

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 1041] [1, 1041] [1, 1041] [1, 1041] [1, 1041] [1, 1041] [1, 1041] [1, 725] [1, 1041] [1, 1041] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 548.870849609375 with beta sum per layer: [5.1783294677734375, 547.7009887695312, 381.0032043457031]
alpha/beta optimization time: 1.567523717880249
This batch time : update_bounds func: 2.7545	 prepare: 0.4178	 bound: 1.5679	 transfer: 0.1035	 finalize: 0.6541
Accumulated time: update_bounds func: 38.1195	 prepare: 5.9598	 bound: 24.4953	 transfer: 0.1035	 finalize: 6.0179
batch bounding time:  2.7585222721099854
Current worst splitting domains [lb, ub] (depth):
[-0.19274,   inf] (56), [-0.19064,   inf] (56), [-0.19055,   inf] (56), [-0.19007,   inf] (56), [-0.18938,   inf] (56), [-0.18868,   inf] (56), [-0.18865,   inf] (56), [-0.18845,   inf] (56), [-0.18838,   inf] (56), [-0.18817,   inf] (56), [-0.18795,   inf] (56), [-0.18768,   inf] (56), [-0.18727,   inf] (56), [-0.18719,   inf] (56), [-0.18689,   inf] (56), [-0.18672,   inf] (56), [-0.18646,   inf] (56), [-0.18646,   inf] (56), [-0.18644,   inf] (56), [-0.18607,   inf] (56), 
length of domains: 21374
Total time: 4.5829	 pickout: 0.3380	 decision: 0.9811	 get_bound: 2.7654	 add_domain: 0.4983
Current lb:-0.19274045526981354
59224 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 63.87257719039917

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 1053] [1, 1053] [1, 1053] [1, 1053] [1, 1053] [1, 1042] [1, 1053] [1, 1053] [1, 1053] [1, 1053] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 545.963623046875 with beta sum per layer: [5.410693645477295, 644.7327880859375, 211.93898010253906]
alpha/beta optimization time: 1.5735809803009033
This batch time : update_bounds func: 2.8055	 prepare: 0.4456	 bound: 1.5740	 transfer: 0.1039	 finalize: 0.6705
Accumulated time: update_bounds func: 40.9250	 prepare: 6.4054	 bound: 26.0692	 transfer: 0.1039	 finalize: 6.6884
batch bounding time:  2.8097455501556396
Current worst splitting domains [lb, ub] (depth):
[-0.18772,   inf] (58), [-0.18563,   inf] (58), [-0.18541,   inf] (58), [-0.18502,   inf] (58), [-0.18439,   inf] (58), [-0.18365,   inf] (58), [-0.18339,   inf] (58), [-0.18332,   inf] (58), [-0.18314,   inf] (58), [-0.18309,   inf] (58), [-0.18296,   inf] (58), [-0.18258,   inf] (58), [-0.18209,   inf] (58), [-0.18207,   inf] (58), [-0.18193,   inf] (58), [-0.18168,   inf] (58), [-0.18134,   inf] (58), [-0.18107,   inf] (58), [-0.18099,   inf] (58), [-0.18098,   inf] (58), 
length of domains: 23422
Total time: 4.2861	 pickout: 0.3416	 decision: 0.6166	 get_bound: 2.8171	 add_domain: 0.5109
Current lb:-0.18771828711032867
63320 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 68.2028546333313

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 478] [1, 478] [1, 478] [1, 478] [1, 478] [1, 478] [1, 478] [1, 478] [1, 478] [1, 478] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 504.06585693359375 with beta sum per layer: [6.032473087310791, 706.8143310546875, 125.97480773925781]
alpha/beta optimization time: 1.5787692070007324
This batch time : update_bounds func: 2.3885	 prepare: 0.4332	 bound: 1.5792	 transfer: 0.1028	 finalize: 0.2624
Accumulated time: update_bounds func: 43.3135	 prepare: 6.8386	 bound: 27.6484	 transfer: 0.1028	 finalize: 6.9508
batch bounding time:  2.392906427383423
Current worst splitting domains [lb, ub] (depth):
[-0.18303,   inf] (60), [-0.18092,   inf] (60), [-0.18069,   inf] (60), [-0.18033,   inf] (60), [-0.17959,   inf] (60), [-0.17894,   inf] (60), [-0.17871,   inf] (60), [-0.17866,   inf] (60), [-0.17858,   inf] (60), [-0.17835,   inf] (60), [-0.17824,   inf] (60), [-0.17779,   inf] (60), [-0.17763,   inf] (60), [-0.17725,   inf] (60), [-0.17722,   inf] (60), [-0.17689,   inf] (60), [-0.17661,   inf] (60), [-0.17647,   inf] (60), [-0.17632,   inf] (60), [-0.17627,   inf] (60), 
length of domains: 25470
Total time: 4.3444	 pickout: 0.3446	 decision: 1.0672	 get_bound: 2.3999	 add_domain: 0.5327
Current lb:-0.18302862346172333
67416 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 72.59906339645386

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 1042] [1, 1042] [1, 1042] [1, 1042] [1, 1042] [1, 1042] [1, 709] [1, 1042] [1, 1042] [1, 1042] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 534.58837890625 with beta sum per layer: [5.3874993324279785, 687.9745483398438, 157.87088012695312]
alpha/beta optimization time: 1.5779800415039062
This batch time : update_bounds func: 2.4003	 prepare: 0.4421	 bound: 1.5784	 transfer: 0.1046	 finalize: 0.2636
Accumulated time: update_bounds func: 45.7138	 prepare: 7.2807	 bound: 29.2268	 transfer: 0.1046	 finalize: 7.2144
batch bounding time:  2.4041945934295654
Current worst splitting domains [lb, ub] (depth):
[-0.17765,   inf] (62), [-0.17599,   inf] (62), [-0.17554,   inf] (62), [-0.17490,   inf] (62), [-0.17461,   inf] (62), [-0.17422,   inf] (62), [-0.17389,   inf] (62), [-0.17360,   inf] (62), [-0.17358,   inf] (62), [-0.17353,   inf] (62), [-0.17326,   inf] (62), [-0.17278,   inf] (62), [-0.17257,   inf] (62), [-0.17243,   inf] (62), [-0.17236,   inf] (62), [-0.17214,   inf] (62), [-0.17193,   inf] (62), [-0.17184,   inf] (62), [-0.17162,   inf] (62), [-0.17151,   inf] (62), 
length of domains: 27518
Total time: 4.4203	 pickout: 0.3462	 decision: 1.1154	 get_bound: 2.4110	 add_domain: 0.5477
Current lb:-0.17764663696289062
71512 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 77.07070899009705

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 751] [1, 751] [1, 751] [1, 751] [1, 932] [1, 751] [1, 751] [1, 751] [1, 751] [1, 932] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 542.48486328125 with beta sum per layer: [6.167645454406738, 539.3419189453125, 112.87492370605469]
alpha/beta optimization time: 1.5694124698638916
This batch time : update_bounds func: 2.3982	 prepare: 0.4449	 bound: 1.5699	 transfer: 0.1028	 finalize: 0.2695
Accumulated time: update_bounds func: 48.1121	 prepare: 7.7256	 bound: 30.7967	 transfer: 0.1028	 finalize: 7.4838
batch bounding time:  2.4024195671081543
Current worst splitting domains [lb, ub] (depth):
[-0.17145,   inf] (64), [-0.16981,   inf] (64), [-0.16935,   inf] (64), [-0.16881,   inf] (64), [-0.16841,   inf] (64), [-0.16802,   inf] (64), [-0.16772,   inf] (64), [-0.16766,   inf] (64), [-0.16755,   inf] (64), [-0.16738,   inf] (64), [-0.16710,   inf] (64), [-0.16670,   inf] (64), [-0.16664,   inf] (64), [-0.16638,   inf] (64), [-0.16623,   inf] (64), [-0.16615,   inf] (64), [-0.16594,   inf] (64), [-0.16574,   inf] (64), [-0.16567,   inf] (64), [-0.16545,   inf] (64), 
length of domains: 29566
Total time: 4.4771	 pickout: 0.3491	 decision: 1.1681	 get_bound: 2.4094	 add_domain: 0.5505
Current lb:-0.1714489758014679
75608 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 81.59867739677429

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 472] [1, 472] [1, 472] [1, 472] [1, 751] [1, 472] [1, 472] [1, 751] [1, 472] [1, 472] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 547.0858764648438 with beta sum per layer: [6.272119522094727, 449.6522216796875, 58.25394058227539]
alpha/beta optimization time: 1.5771334171295166
This batch time : update_bounds func: 2.4164	 prepare: 0.4559	 bound: 1.5775	 transfer: 0.1051	 finalize: 0.2664
Accumulated time: update_bounds func: 50.5285	 prepare: 8.1815	 bound: 32.3742	 transfer: 0.1051	 finalize: 7.7503
batch bounding time:  2.4215142726898193
Current worst splitting domains [lb, ub] (depth):
[-0.16435,   inf] (66), [-0.16415,   inf] (66), [-0.16272,   inf] (66), [-0.16254,   inf] (66), [-0.16251,   inf] (66), [-0.16221,   inf] (66), [-0.16216,   inf] (66), [-0.16182,   inf] (66), [-0.16171,   inf] (66), [-0.16152,   inf] (66), [-0.16088,   inf] (66), [-0.16083,   inf] (66), [-0.16058,   inf] (66), [-0.16053,   inf] (66), [-0.16045,   inf] (66), [-0.16032,   inf] (66), [-0.16030,   inf] (66), [-0.16025,   inf] (66), [-0.16010,   inf] (66), [-0.16007,   inf] (66), 
length of domains: 31614
Total time: 4.5767	 pickout: 0.3459	 decision: 1.2023	 get_bound: 2.4290	 add_domain: 0.5994
Current lb:-0.1643490344285965
79704 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 86.22582054138184

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [0, 2542] [0, 2542] [0, 2542] [1, 472] [0, 2542] [1, 709] [1, 709] [1, 472] [0, 2542] [0, 2542] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 537.9439086914062 with beta sum per layer: [17.959552764892578, 427.29180908203125, 28.197494506835938]
alpha/beta optimization time: 1.573418378829956
This batch time : update_bounds func: 2.3950	 prepare: 0.4438	 bound: 1.5738	 transfer: 0.1034	 finalize: 0.2617
Accumulated time: update_bounds func: 52.9235	 prepare: 8.6253	 bound: 33.9480	 transfer: 0.1034	 finalize: 8.0119
batch bounding time:  2.3988001346588135
Current worst splitting domains [lb, ub] (depth):
[-0.16003,   inf] (68), [-0.15987,   inf] (68), [-0.15844,   inf] (68), [-0.15840,   inf] (68), [-0.15836,   inf] (68), [-0.15824,   inf] (68), [-0.15740,   inf] (68), [-0.15722,   inf] (68), [-0.15680,   inf] (68), [-0.15672,   inf] (68), [-0.15665,   inf] (68), [-0.15661,   inf] (68), [-0.15613,   inf] (68), [-0.15603,   inf] (68), [-0.15603,   inf] (68), [-0.15597,   inf] (68), [-0.15581,   inf] (68), [-0.15581,   inf] (68), [-0.15580,   inf] (68), [-0.15572,   inf] (68), 
length of domains: 33662
Total time: 4.5704	 pickout: 0.3559	 decision: 1.2294	 get_bound: 2.4056	 add_domain: 0.5795
Current lb:-0.16003362834453583
83800 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 90.84462118148804

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 709] [1, 709] [1, 932] [1, 709] [1, 932] [1, 709] [1, 709] [1, 709] [1, 932] [1, 932] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 518.4243774414062 with beta sum per layer: [16.766826629638672, 478.22906494140625, 29.701309204101562]
alpha/beta optimization time: 1.5778374671936035
This batch time : update_bounds func: 2.3887	 prepare: 0.4397	 bound: 1.5783	 transfer: 0.1025	 finalize: 0.2564
Accumulated time: update_bounds func: 55.3122	 prepare: 9.0650	 bound: 35.5263	 transfer: 0.1025	 finalize: 8.2683
batch bounding time:  2.3925864696502686
Current worst splitting domains [lb, ub] (depth):
[-0.15628,   inf] (70), [-0.15612,   inf] (70), [-0.15464,   inf] (70), [-0.15448,   inf] (70), [-0.15365,   inf] (70), [-0.15348,   inf] (70), [-0.15289,   inf] (70), [-0.15286,   inf] (70), [-0.15261,   inf] (70), [-0.15252,   inf] (70), [-0.15239,   inf] (70), [-0.15232,   inf] (70), [-0.15225,   inf] (70), [-0.15222,   inf] (70), [-0.15210,   inf] (70), [-0.15206,   inf] (70), [-0.15144,   inf] (70), [-0.15135,   inf] (70), [-0.15131,   inf] (70), [-0.15126,   inf] (70), 
length of domains: 35710
Total time: 4.6333	 pickout: 0.3550	 decision: 1.2896	 get_bound: 2.3995	 add_domain: 0.5892
Current lb:-0.1562807559967041
87896 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 95.52877759933472

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 932] [1, 932] [1, 932] [1, 932] [1, 932] [1, 932] [1, 932] [1, 932] [1, 702] [1, 702] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 513.0070190429688 with beta sum per layer: [16.79415512084961, 440.883544921875, 69.97544860839844]
alpha/beta optimization time: 1.566789150238037
This batch time : update_bounds func: 2.3851	 prepare: 0.4421	 bound: 1.5672	 transfer: 0.1026	 finalize: 0.2612
Accumulated time: update_bounds func: 57.6972	 prepare: 9.5071	 bound: 37.0935	 transfer: 0.1026	 finalize: 8.5295
batch bounding time:  2.3891823291778564
Current worst splitting domains [lb, ub] (depth):
[-0.15046,   inf] (72), [-0.15030,   inf] (72), [-0.14928,   inf] (72), [-0.14913,   inf] (72), [-0.14762,   inf] (72), [-0.14746,   inf] (72), [-0.14741,   inf] (72), [-0.14732,   inf] (72), [-0.14707,   inf] (72), [-0.14703,   inf] (72), [-0.14677,   inf] (72), [-0.14660,   inf] (72), [-0.14650,   inf] (72), [-0.14644,   inf] (72), [-0.14628,   inf] (72), [-0.14624,   inf] (72), [-0.14624,   inf] (72), [-0.14615,   inf] (72), [-0.14590,   inf] (72), [-0.14585,   inf] (72), 
length of domains: 37758
Total time: 4.6824	 pickout: 0.3559	 decision: 1.3380	 get_bound: 2.3962	 add_domain: 0.5922
Current lb:-0.15045881271362305/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/utils.py:530: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  eps_temp = torch.tensor(eps_temp / std).reshape(1, -1, 1, 1)

91992 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 100.26388311386108

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 702] [1, 702] [1, 702] [1, 702] [1, 702] [1, 702] [1, 1046] [1, 1046] [1, 702] [1, 702] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 500.24346923828125 with beta sum per layer: [14.610899925231934, 483.49908447265625, 103.57659912109375]
alpha/beta optimization time: 1.5762364864349365
This batch time : update_bounds func: 3.1502	 prepare: 0.4390	 bound: 1.5766	 transfer: 0.1046	 finalize: 0.2680
Accumulated time: update_bounds func: 60.8474	 prepare: 9.9462	 bound: 38.6701	 transfer: 0.1046	 finalize: 8.7975
batch bounding time:  3.1542186737060547
Current worst splitting domains [lb, ub] (depth):
[-0.14525,   inf] (74), [-0.14509,   inf] (74), [-0.14408,   inf] (74), [-0.14392,   inf] (74), [-0.14240,   inf] (74), [-0.14223,   inf] (74), [-0.14213,   inf] (74), [-0.14206,   inf] (74), [-0.14187,   inf] (74), [-0.14182,   inf] (74), [-0.14155,   inf] (74), [-0.14139,   inf] (74), [-0.14128,   inf] (74), [-0.14124,   inf] (74), [-0.14106,   inf] (74), [-0.14103,   inf] (74), [-0.14096,   inf] (74), [-0.14088,   inf] (74), [-0.14070,   inf] (74), [-0.14065,   inf] (74), 
length of domains: 39806
Total time: 4.7432	 pickout: 0.3576	 decision: 0.6280	 get_bound: 3.1612	 add_domain: 0.5964
Current lb:-0.14525294303894043
96088 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 105.06166982650757

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 1046] [1, 1046] [1, 1046] [1, 1046] [1, 1046] [1, 1046] [1, 246] [1, 246] [1, 1046] [1, 1046] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 489.7123718261719 with beta sum per layer: [15.586360931396484, 521.978759765625, 139.81637573242188]
alpha/beta optimization time: 1.5766944885253906
This batch time : update_bounds func: 3.2869	 prepare: 0.4626	 bound: 1.5771	 transfer: 0.1037	 finalize: 1.1300
Accumulated time: update_bounds func: 64.1343	 prepare: 10.4088	 bound: 40.2472	 transfer: 0.1037	 finalize: 9.9275
batch bounding time:  3.2914774417877197
Current worst splitting domains [lb, ub] (depth):
[-0.13987,   inf] (76), [-0.13972,   inf] (76), [-0.13869,   inf] (76), [-0.13855,   inf] (76), [-0.13848,   inf] (76), [-0.13840,   inf] (76), [-0.13731,   inf] (76), [-0.13723,   inf] (76), [-0.13701,   inf] (76), [-0.13685,   inf] (76), [-0.13671,   inf] (76), [-0.13661,   inf] (76), [-0.13659,   inf] (76), [-0.13617,   inf] (76), [-0.13617,   inf] (76), [-0.13602,   inf] (76), [-0.13591,   inf] (76), [-0.13591,   inf] (76), [-0.13590,   inf] (76), [-0.13572,   inf] (76), 
length of domains: 41854
Total time: 4.9094	 pickout: 0.3640	 decision: 0.6344	 get_bound: 3.2992	 add_domain: 0.6118
Current lb:-0.13986730575561523
100184 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 110.02564454078674

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2048, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([2048, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [1, 246] [1, 246] [1, 246] [1, 246] [1, 425] [1, 425] [1, 425] [1, 425] [1, 503] [1, 503] 
regular batch size: 2*2048, diving batch size 1*0
best_l after optimization: 482.0295715332031 with beta sum per layer: [12.985851287841797, 567.310546875, 153.9300537109375]
alpha/beta optimization time: 1.5706403255462646
This batch time : update_bounds func: 2.4063	 prepare: 0.4451	 bound: 1.5710	 transfer: 0.1034	 finalize: 0.2735
Accumulated time: update_bounds func: 66.5405	 prepare: 10.8539	 bound: 41.8183	 transfer: 0.1034	 finalize: 10.2010
batch bounding time:  2.410674810409546
Current worst splitting domains [lb, ub] (depth):
[-0.13621,   inf] (78), [-0.13607,   inf] (78), [-0.13504,   inf] (78), [-0.13490,   inf] (78), [-0.13370,   inf] (78), [-0.13362,   inf] (78), [-0.13332,   inf] (78), [-0.13323,   inf] (78), [-0.13318,   inf] (78), [-0.13305,   inf] (78), [-0.13293,   inf] (78), [-0.13279,   inf] (78), [-0.13255,   inf] (78), [-0.13254,   inf] (78), [-0.13253,   inf] (78), [-0.13247,   inf] (78), [-0.13239,   inf] (78), [-0.13225,   inf] (78), [-0.13222,   inf] (78), [-0.13208,   inf] (78), 
length of domains: 43902
Total time: 4.0319	 pickout: 0.3645	 decision: 0.6413	 get_bound: 2.4183	 add_domain: 0.6077
Current lb:-0.1362133026123047
104280 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Time out!!!!!!!!
Image 761 label 5 verification end, final lower bound -0.1362133026123047, upper bound inf, time: 115.03173995018005
761 -0.1362133026123047
Result: image 761 verification failure (with branch and bound).
Wall time: 124.05909514427185

number of correctly classified examples: 1
incorrectly classified idx (total 0): []
attack success idx (total 0): []
verification success idx (total 0): []
verification failure idx (total 1): [761]
final verified acc: 0.0%[1]
verifier is called on 1 examples.
total verified: 0
mean time [cnt:1] (excluding attack success): 124.00261092185974
