Building native CUDA modules...
/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
model:
  path: cifar_cnn_a_adv.model
  name: cnn_4layer_adv
data:
  start: 98
  end: 99
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR_SDP
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: 0.00784313725
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 4096
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 20
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 30
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: max
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  use_auto_attack: false
  use_diversed_pgd: false
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 20:51:36 2022 on diablo.cs.ucla.edu
Sequential(
  (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (3): ReLU()
  (4): Flatten()
  (5): Linear(in_features=2048, out_features=100, bias=True)
  (6): ReLU()
  (7): Linear(in_features=100, out_features=10, bias=True)
)
############################
Sampled data loaded. Data already preprocessed!
Shape: torch.Size([200, 3, 32, 32]) torch.Size([200]) torch.Size([200])
X range: tensor(2.1256) tensor(-1.9889) tensor(-0.0131)
############################
epsilon after preprocessing: tensor([[[[0.0317]],

         [[0.0322]],

         [[0.0300]]]]), data_max = tensor([[[[2.0587]],

         [[2.1256]],

         [[2.1154]]]]), data_min = tensor([[[[-1.9889]],

         [[-1.9807]],

         [[-1.7076]]]])
Task length: 1
saving results to Verified_ret_[cnn_4layer_adv]_start=98_end=99_iter=20_b=4096_timeout=30_branching=kfsb-max-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=skip.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 98 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label 5, correct label 5, image norm 2470.015625, logits tensor([-6.9728, -9.5904, -6.4481, -4.7895, -4.9730, -4.4264, -6.6802, -6.6811,
        -7.7096, -9.4774], device='cuda:0', grad_fn=<SelectBackward>)
Model prediction is: tensor([[-6.9728, -9.5904, -6.4481, -4.7895, -4.9730, -4.4264, -6.6802, -6.6811,
         -7.7096, -9.4774]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[ 1.0408,  2.8690,  0.7544,  0.0683, -0.6199,  1.0873,  0.8534,  1.2566,
          3.1061]], device='cuda:0') None
best_l after optimization: -11.078217506408691 with beta sum per layer: []
alpha/beta optimization time: 8.028266906738281
initial alpha-CROWN bounds: tensor([[ 1.1267,  2.9728,  0.8138,  0.0883, -0.5676,  1.1447,  0.9210,  1.3798,
          3.1986]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-0.5676, device='cuda:0', grad_fn=<MinBackward1>)
##### [0:98] Tested against 4 ######
Model prediction is: tensor([[-6.9728, -9.5904, -6.4481, -4.7895, -4.9730, -4.4264, -6.6802, -6.6811,
         -7.7096, -9.4774]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /10 start_node /11
setting alpha for layer /10 start_node /21
not setting layer /10 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 16, 16, 16]) != torch.Size([2, 9, 1, 16, 16, 16]))
setting alpha for layer /12 start_node /21
not setting layer /12 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
not setting layer /22 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /9 torch.Size([1, 16, 16, 16])
1 /11 torch.Size([1, 32, 8, 8])
2 /21 torch.Size([1, 100])
best_l after optimization: 0.567524254322052 with beta sum per layer: []
alpha/beta optimization time: 2.106238603591919
alpha-CROWN with fixed intermediate bounds: tensor([[-0.5675]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-0.567524254322052
layer 0 size torch.Size([4096]) unstable 664
layer 1 size torch.Size([2048]) unstable 231
layer 2 size torch.Size([100]) unstable 22
-----------------
# of unstable neurons: 917
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  8
batch:  torch.Size([1, 16, 16, 16]) post split depth:  8
splitting decisions: 
split level 0: [2, 65] 
split level 1: [2, 51] 
split level 2: [2, 41] 
split level 3: [2, 92] 
split level 4: [2, 17] 
split level 5: [2, 53] 
split level 6: [2, 9] 
split level 7: [2, 68] 
regular batch size: 2*128, diving batch size 1*0
best_l after optimization: -8.066550254821777 with beta sum per layer: [0.0, 0.0, 65.2263412475586]
alpha/beta optimization time: 0.3140387535095215
This batch time : update_bounds func: 0.3601	 prepare: 0.0194	 bound: 0.3144	 transfer: 0.0096	 finalize: 0.0161
Accumulated time: update_bounds func: 0.3601	 prepare: 0.0194	 bound: 0.3144	 transfer: 0.0096	 finalize: 0.0161
batch bounding time:  0.3605034351348877
Current worst splitting domains [lb, ub] (depth):
[-0.33510,   inf] (9), [-0.32928,   inf] (9), [-0.31069,   inf] (9), [-0.29997,   inf] (9), [-0.28958,   inf] (9), [-0.28316,   inf] (9), [-0.27104,   inf] (9), [-0.25792,   inf] (9), [-0.24830,   inf] (9), [-0.24680,   inf] (9), [-0.24300,   inf] (9), [-0.24158,   inf] (9), [-0.23968,   inf] (9), [-0.21615,   inf] (9), [-0.21513,   inf] (9), [-0.21488,   inf] (9), [-0.21371,   inf] (9), [-0.21119,   inf] (9), [-0.20854,   inf] (9), [-0.20693,   inf] (9), 
length of domains: 77
Total time: 0.6653	 pickout: 0.0009	 decision: 0.2680	 get_bound: 0.3930	 add_domain: 0.0034
Current lb:-0.33510246872901917
256 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.636519432067871

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([77, 16, 16, 16]) pre split depth:  2
batch:  torch.Size([77, 16, 16, 16]) post split depth:  2
splitting decisions: 
split level 0: [2, 10] [2, 10] [2, 10] [2, 10] [2, 10] [2, 10] [2, 10] [2, 10] [2, 10] [2, 10] 
split level 1: [2, 32] [2, 32] [2, 54] [2, 54] [2, 32] [2, 32] [2, 32] [2, 32] [2, 32] [2, 32] 
regular batch size: 2*154, diving batch size 1*0
best_l after optimization: 1.1533985137939453 with beta sum per layer: [0.0, 0.0, 112.52995300292969]
alpha/beta optimization time: 0.30466413497924805
This batch time : update_bounds func: 0.3680	 prepare: 0.0272	 bound: 0.3050	 transfer: 0.0163	 finalize: 0.0184
Accumulated time: update_bounds func: 0.7281	 prepare: 0.0466	 bound: 0.6193	 transfer: 0.0163	 finalize: 0.0346
batch bounding time:  0.36842942237854004
Current worst splitting domains [lb, ub] (depth):
[-0.31701,   inf] (12), [-0.31127,   inf] (12), [-0.29292,   inf] (12), [-0.28225,   inf] (12), [-0.27098,   inf] (12), [-0.26712,   inf] (12), [-0.26449,   inf] (12), [-0.25252,   inf] (12), [-0.25070,   inf] (12), [-0.23869,   inf] (12), [-0.22912,   inf] (12), [-0.22738,   inf] (12), [-0.22236,   inf] (12), [-0.22159,   inf] (12), [-0.22020,   inf] (12), [-0.21807,   inf] (12), [-0.21036,   inf] (12), [-0.19907,   inf] (12), [-0.19717,   inf] (12), [-0.19565,   inf] (12), 
length of domains: 130
Total time: 0.4566	 pickout: 0.0120	 decision: 0.0482	 get_bound: 0.3894	 add_domain: 0.0071
Current lb:-0.31700798869132996
564 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.094897747039795

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([130, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([130, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 4] [2, 4] [2, 4] [2, 4] [2, 4] [2, 4] [2, 4] [2, 4] [2, 4] [2, 4] 
regular batch size: 2*130, diving batch size 1*0
best_l after optimization: 10.911845207214355 with beta sum per layer: [0.0, 0.0, 86.34620666503906]
alpha/beta optimization time: 0.30025601387023926
This batch time : update_bounds func: 0.3462	 prepare: 0.0239	 bound: 0.3006	 transfer: 0.0056	 finalize: 0.0156
Accumulated time: update_bounds func: 1.0744	 prepare: 0.0705	 bound: 0.9199	 transfer: 0.0056	 finalize: 0.0502
batch bounding time:  0.34662389755249023
Current worst splitting domains [lb, ub] (depth):
[-0.30781,   inf] (14), [-0.30192,   inf] (14), [-0.28341,   inf] (14), [-0.27185,   inf] (14), [-0.26161,   inf] (14), [-0.25690,   inf] (14), [-0.25510,   inf] (14), [-0.24267,   inf] (14), [-0.24188,   inf] (14), [-0.22754,   inf] (14), [-0.21897,   inf] (14), [-0.21660,   inf] (14), [-0.21229,   inf] (14), [-0.21110,   inf] (14), [-0.21017,   inf] (14), [-0.20708,   inf] (14), [-0.19919,   inf] (14), [-0.19285,   inf] (14), [-0.18975,   inf] (14), [-0.18768,   inf] (14), 
length of domains: 155
Total time: 0.4304	 pickout: 0.0194	 decision: 0.0553	 get_bound: 0.3470	 add_domain: 0.0087
Current lb:-0.30780643224716187
824 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.527413845062256

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([155, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([155, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 11] [2, 45] [2, 11] [2, 11] [2, 45] [2, 11] [2, 45] [2, 11] [2, 11] [2, 45] 
regular batch size: 2*155, diving batch size 1*0
best_l after optimization: 5.4252777099609375 with beta sum per layer: [0.0, 2.3417482376098633, 111.91600036621094]
alpha/beta optimization time: 0.33225131034851074
This batch time : update_bounds func: 0.3920	 prepare: 0.0307	 bound: 0.3327	 transfer: 0.0083	 finalize: 0.0197
Accumulated time: update_bounds func: 1.4664	 prepare: 0.1012	 bound: 1.2526	 transfer: 0.0083	 finalize: 0.0698
batch bounding time:  0.3924226760864258
Current worst splitting domains [lb, ub] (depth):
[-0.29958,   inf] (16), [-0.29604,   inf] (16), [-0.27470,   inf] (16), [-0.26326,   inf] (16), [-0.25557,   inf] (16), [-0.24883,   inf] (16), [-0.24774,   inf] (16), [-0.23455,   inf] (16), [-0.23370,   inf] (16), [-0.22176,   inf] (16), [-0.21279,   inf] (16), [-0.21048,   inf] (16), [-0.20745,   inf] (16), [-0.20322,   inf] (16), [-0.20249,   inf] (16), [-0.20170,   inf] (16), [-0.19648,   inf] (16), [-0.19277,   inf] (16), [-0.18240,   inf] (16), [-0.18187,   inf] (16), 
length of domains: 172
Total time: 0.4928	 pickout: 0.0271	 decision: 0.0630	 get_bound: 0.3929	 add_domain: 0.0098
Current lb:-0.2995789051055908
1134 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.023212432861328

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([172, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([172, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 45] [2, 11] [2, 32] [2, 32] [2, 11] [2, 11] [2, 54] [2, 54] [2, 54] [2, 11] 
regular batch size: 2*172, diving batch size 1*0
best_l after optimization: 11.120512008666992 with beta sum per layer: [0.0, 9.70412540435791, 117.17810821533203]
alpha/beta optimization time: 0.34709930419921875
This batch time : update_bounds func: 0.4111	 prepare: 0.0319	 bound: 0.3474	 transfer: 0.0083	 finalize: 0.0227
Accumulated time: update_bounds func: 1.8775	 prepare: 0.1331	 bound: 1.6000	 transfer: 0.0083	 finalize: 0.0925
batch bounding time:  0.41156554222106934
Current worst splitting domains [lb, ub] (depth):
[-0.29388,   inf] (18), [-0.28791,   inf] (18), [-0.26366,   inf] (18), [-0.25235,   inf] (18), [-0.24750,   inf] (18), [-0.24080,   inf] (18), [-0.23839,   inf] (18), [-0.22541,   inf] (18), [-0.22367,   inf] (18), [-0.21405,   inf] (18), [-0.20420,   inf] (18), [-0.20323,   inf] (18), [-0.20211,   inf] (18), [-0.19893,   inf] (18), [-0.19546,   inf] (18), [-0.19198,   inf] (18), [-0.19040,   inf] (18), [-0.18978,   inf] (18), [-0.18787,   inf] (18), [-0.18355,   inf] (18), 
length of domains: 221
Total time: 0.5263	 pickout: 0.0259	 decision: 0.0749	 get_bound: 0.4121	 add_domain: 0.0133
Current lb:-0.29387587308883667
1478 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.553081512451172

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([221, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([221, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 54] [2, 54] [2, 45] [2, 45] [2, 31] [2, 54] [2, 45] [2, 45] [2, 45] [2, 31] 
regular batch size: 2*221, diving batch size 1*0
best_l after optimization: 12.575068473815918 with beta sum per layer: [0.0, 39.50148391723633, 144.5754852294922]
alpha/beta optimization time: 0.4055769443511963
This batch time : update_bounds func: 0.4967	 prepare: 0.0426	 bound: 0.4059	 transfer: 0.0095	 finalize: 0.0376
Accumulated time: update_bounds func: 2.3742	 prepare: 0.1757	 bound: 2.0059	 transfer: 0.0095	 finalize: 0.1301
batch bounding time:  0.4972712993621826
Current worst splitting domains [lb, ub] (depth):
[-0.28451,   inf] (20), [-0.27871,   inf] (20), [-0.25785,   inf] (20), [-0.24650,   inf] (20), [-0.24138,   inf] (20), [-0.23221,   inf] (20), [-0.23139,   inf] (20), [-0.21968,   inf] (20), [-0.21769,   inf] (20), [-0.21639,   inf] (20), [-0.20814,   inf] (20), [-0.19762,   inf] (20), [-0.19558,   inf] (20), [-0.19549,   inf] (20), [-0.19237,   inf] (20), [-0.18775,   inf] (20), [-0.18644,   inf] (20), [-0.18560,   inf] (20), [-0.18397,   inf] (20), [-0.18148,   inf] (20), 
length of domains: 292
Total time: 0.6965	 pickout: 0.0329	 decision: 0.1481	 get_bound: 0.4980	 add_domain: 0.0176
Current lb:-0.2845138609409332
1920 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.253633499145508

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([292, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([292, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 31] [2, 31] [1, 922] [2, 31] [2, 54] [2, 31] [2, 31] [2, 31] [2, 31] [2, 31] 
regular batch size: 2*292, diving batch size 1*0
best_l after optimization: 11.613472938537598 with beta sum per layer: [0.0, 94.16244506835938, 199.94024658203125]
alpha/beta optimization time: 0.4504659175872803
This batch time : update_bounds func: 0.5909	 prepare: 0.0563	 bound: 0.4509	 transfer: 0.0273	 finalize: 0.0550
Accumulated time: update_bounds func: 2.9651	 prepare: 0.2320	 bound: 2.4568	 transfer: 0.0273	 finalize: 0.1851
batch bounding time:  0.5915884971618652
Current worst splitting domains [lb, ub] (depth):
[-0.27862,   inf] (22), [-0.27287,   inf] (22), [-0.25185,   inf] (22), [-0.24064,   inf] (22), [-0.23860,   inf] (22), [-0.23108,   inf] (22), [-0.22589,   inf] (22), [-0.22530,   inf] (22), [-0.21371,   inf] (22), [-0.21159,   inf] (22), [-0.20942,   inf] (22), [-0.19753,   inf] (22), [-0.18917,   inf] (22), [-0.18787,   inf] (22), [-0.18697,   inf] (22), [-0.18641,   inf] (22), [-0.18173,   inf] (22), [-0.18033,   inf] (22), [-0.17961,   inf] (22), [-0.17769,   inf] (22), 
length of domains: 366
Total time: 0.7654	 pickout: 0.0447	 decision: 0.1031	 get_bound: 0.5925	 add_domain: 0.0252
Current lb:-0.2786181569099426
2504 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.025153636932373

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([366, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([366, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 922] [1, 922] [2, 31] [1, 922] [2, 31] [1, 922] [1, 1131] [1, 922] [1, 780] [1, 922] 
regular batch size: 2*366, diving batch size 1*0
best_l after optimization: 27.00579261779785 with beta sum per layer: [0.0, 202.04405212402344, 218.73855590820312]
alpha/beta optimization time: 0.4920625686645508
This batch time : update_bounds func: 0.6387	 prepare: 0.0803	 bound: 0.4925	 transfer: 0.0168	 finalize: 0.0473
Accumulated time: update_bounds func: 3.6038	 prepare: 0.3123	 bound: 2.9493	 transfer: 0.0168	 finalize: 0.2324
batch bounding time:  0.6394975185394287
Current worst splitting domains [lb, ub] (depth):
[-0.27154,   inf] (24), [-0.26687,   inf] (24), [-0.26248,   inf] (24), [-0.25708,   inf] (24), [-0.24601,   inf] (24), [-0.23561,   inf] (24), [-0.23255,   inf] (24), [-0.22592,   inf] (24), [-0.22129,   inf] (24), [-0.22101,   inf] (24), [-0.21930,   inf] (24), [-0.21917,   inf] (24), [-0.21389,   inf] (24), [-0.21255,   inf] (24), [-0.20854,   inf] (24), [-0.20512,   inf] (24), [-0.20420,   inf] (24), [-0.19365,   inf] (24), [-0.19225,   inf] (24), [-0.18509,   inf] (24), 
length of domains: 533
Total time: 0.9147	 pickout: 0.0559	 decision: 0.1818	 get_bound: 0.6407	 add_domain: 0.0363
Current lb:-0.2715400159358978
3236 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.946977376937866

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([533, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([533, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 780] [1, 780] [1, 780] [1, 780] [1, 915] [1, 915] [1, 988] [1, 464] [1, 988] [1, 464] 
regular batch size: 2*533, diving batch size 1*0
best_l after optimization: 40.82261657714844 with beta sum per layer: [0.0, 434.7118835449219, 302.8642883300781]
alpha/beta optimization time: 0.6158924102783203
This batch time : update_bounds func: 0.8663	 prepare: 0.1066	 bound: 0.6163	 transfer: 0.0254	 finalize: 0.1154
Accumulated time: update_bounds func: 4.4701	 prepare: 0.4189	 bound: 3.5656	 transfer: 0.0254	 finalize: 0.3478
batch bounding time:  0.8672940731048584
Current worst splitting domains [lb, ub] (depth):
[-0.27049,   inf] (26), [-0.26572,   inf] (26), [-0.26131,   inf] (26), [-0.25587,   inf] (26), [-0.24167,   inf] (26), [-0.24071,   inf] (26), [-0.23760,   inf] (26), [-0.23289,   inf] (26), [-0.23037,   inf] (26), [-0.22961,   inf] (26), [-0.22838,   inf] (26), [-0.22783,   inf] (26), [-0.22314,   inf] (26), [-0.21978,   inf] (26), [-0.21822,   inf] (26), [-0.21797,   inf] (26), [-0.21651,   inf] (26), [-0.21637,   inf] (26), [-0.21615,   inf] (26), [-0.21159,   inf] (26), 
length of domains: 794
Total time: 1.1986	 pickout: 0.0856	 decision: 0.1876	 get_bound: 0.8689	 add_domain: 0.0566
Current lb:-0.2704911231994629
4302 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.15539836883545

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([794, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([794, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 64] [2, 15] [2, 64] [2, 15] [2, 64] [1, 988] [2, 15] [2, 64] [1, 988] [1, 988] 
regular batch size: 2*794, diving batch size 1*0
best_l after optimization: 60.77783203125 with beta sum per layer: [0.0, 851.8406372070312, 423.909912109375]
alpha/beta optimization time: 0.844135046005249
This batch time : update_bounds func: 1.2255	 prepare: 0.1606	 bound: 0.8446	 transfer: 0.0477	 finalize: 0.1209
Accumulated time: update_bounds func: 5.6956	 prepare: 0.5796	 bound: 4.4102	 transfer: 0.0477	 finalize: 0.4688
batch bounding time:  1.2271664142608643
Current worst splitting domains [lb, ub] (depth):
[-0.26906,   inf] (28), [-0.26110,   inf] (28), [-0.25984,   inf] (28), [-0.25129,   inf] (28), [-0.23890,   inf] (28), [-0.23585,   inf] (28), [-0.23167,   inf] (28), [-0.23011,   inf] (28), [-0.22568,   inf] (28), [-0.22543,   inf] (28), [-0.22405,   inf] (28), [-0.22246,   inf] (28), [-0.22007,   inf] (28), [-0.21845,   inf] (28), [-0.21596,   inf] (28), [-0.21486,   inf] (28), [-0.21465,   inf] (28), [-0.21440,   inf] (28), [-0.21440,   inf] (28), [-0.21343,   inf] (28), 
length of domains: 1234
Total time: 1.7154	 pickout: 0.1357	 decision: 0.2597	 get_bound: 1.2297	 add_domain: 0.0903
Current lb:-0.26905834674835205
5890 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.886133670806885

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1234, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([1234, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 15] [1, 915] [2, 15] [1, 988] [2, 15] [1, 464] [1, 988] [2, 15] [1, 780] [1, 464] 
regular batch size: 2*1234, diving batch size 1*0
best_l after optimization: 93.98275756835938 with beta sum per layer: [0.1498490869998932, 1632.296142578125, 632.5716552734375]
alpha/beta optimization time: 1.201392650604248
This batch time : update_bounds func: 1.7933	 prepare: 0.2627	 bound: 1.2019	 transfer: 0.0859	 finalize: 0.2356
Accumulated time: update_bounds func: 7.4889	 prepare: 0.8423	 bound: 5.6120	 transfer: 0.0859	 finalize: 0.7044
batch bounding time:  1.7953920364379883
Current worst splitting domains [lb, ub] (depth):
[-0.26445,   inf] (30), [-0.25772,   inf] (30), [-0.25523,   inf] (30), [-0.24776,   inf] (30), [-0.24704,   inf] (30), [-0.24153,   inf] (30), [-0.23345,   inf] (30), [-0.23260,   inf] (30), [-0.22655,   inf] (30), [-0.22463,   inf] (30), [-0.22450,   inf] (30), [-0.22170,   inf] (30), [-0.22136,   inf] (30), [-0.22057,   inf] (30), [-0.21749,   inf] (30), [-0.21737,   inf] (30), [-0.21703,   inf] (30), [-0.21669,   inf] (30), [-0.21314,   inf] (30), [-0.21291,   inf] (30), 
length of domains: 1898
Total time: 2.6542	 pickout: 0.2066	 decision: 0.4684	 get_bound: 1.7991	 add_domain: 0.1800
Current lb:-0.2644520699977875
8358 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 13.563534498214722

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1898, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([1898, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1131] [1, 464] [1, 1131] [1, 464] [1, 464] [1, 464] [1, 1131] [2, 15] [1, 464] [1, 1131] 
regular batch size: 2*1898, diving batch size 1*0
best_l after optimization: 142.24951171875 with beta sum per layer: [0.7637062072753906, 2975.19287109375, 953.5458984375]
alpha/beta optimization time: 1.8200669288635254
This batch time : update_bounds func: 2.6642	 prepare: 0.3867	 bound: 1.8205	 transfer: 0.1254	 finalize: 0.3218
Accumulated time: update_bounds func: 10.1531	 prepare: 1.2290	 bound: 7.4325	 transfer: 0.1254	 finalize: 1.0262
batch bounding time:  2.667863368988037
Current worst splitting domains [lb, ub] (depth):
[-0.26193,   inf] (32), [-0.25461,   inf] (32), [-0.25277,   inf] (32), [-0.24767,   inf] (32), [-0.24428,   inf] (32), [-0.24387,   inf] (32), [-0.24142,   inf] (32), [-0.23822,   inf] (32), [-0.23798,   inf] (32), [-0.23106,   inf] (32), [-0.23058,   inf] (32), [-0.23028,   inf] (32), [-0.22822,   inf] (32), [-0.22459,   inf] (32), [-0.22269,   inf] (32), [-0.22155,   inf] (32), [-0.22051,   inf] (32), [-0.22000,   inf] (32), [-0.21705,   inf] (32), [-0.21624,   inf] (32), 
length of domains: 2973
Total time: 4.0626	 pickout: 0.3341	 decision: 0.7274	 get_bound: 2.6743	 add_domain: 0.3269
Current lb:-0.26193469762802124
12154 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 17.662580728530884

remaining dive domains: 0/-1, dive_rate:0.0/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/utils.py:462: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  eps_temp = torch.tensor(preprocess_cifar(eps_temp, perturbation=True)).reshape(1,-1,1,1)

batch:  torch.Size([2973, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([2973, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 464] [1, 921] [1, 464] [1, 464] [1, 921] [1, 921] [1, 921] [1, 464] [1, 921] [1, 921] 
regular batch size: 2*2973, diving batch size 1*0
best_l after optimization: 252.03782653808594 with beta sum per layer: [1.675727367401123, 5376.3916015625, 1419.2529296875]
alpha/beta optimization time: 2.7844009399414062
This batch time : update_bounds func: 4.2171	 prepare: 0.6111	 bound: 2.7848	 transfer: 0.1909	 finalize: 0.5311
Accumulated time: update_bounds func: 14.3702	 prepare: 1.8401	 bound: 10.2174	 transfer: 0.1909	 finalize: 1.5573
batch bounding time:  4.223120450973511
Current worst splitting domains [lb, ub] (depth):
[-0.25881,   inf] (34), [-0.25159,   inf] (34), [-0.24964,   inf] (34), [-0.24444,   inf] (34), [-0.24444,   inf] (34), [-0.24178,   inf] (34), [-0.24130,   inf] (34), [-0.24032,   inf] (34), [-0.23779,   inf] (34), [-0.23538,   inf] (34), [-0.23497,   inf] (34), [-0.23479,   inf] (34), [-0.23035,   inf] (34), [-0.23010,   inf] (34), [-0.22898,   inf] (34), [-0.22807,   inf] (34), [-0.22750,   inf] (34), [-0.22691,   inf] (34), [-0.22645,   inf] (34), [-0.22158,   inf] (34), 
length of domains: 4819
Total time: 6.3745	 pickout: 0.4990	 decision: 1.1214	 get_bound: 4.2334	 add_domain: 0.5207
Current lb:-0.25881344079971313
18100 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Time out!!!!!!!!
Image 98 label 4 verification end, final lower bound -0.25881344079971313, upper bound inf, time: 24.22652244567871
98 -0.25881344079971313
Result: image 98 verification failure (with branch and bound).
Wall time: 34.25493502616882

number of correctly classified examples: 1
incorrectly classified idx (total 0): []
attack success idx (total 0): []
verification success idx (total 0): []
verification failure idx (total 1): [98]
final verified acc: 0.0%[1]
verifier is called on 1 examples.
total verified: 0
mean time [cnt:1] (excluding attack success): 34.19921541213989
