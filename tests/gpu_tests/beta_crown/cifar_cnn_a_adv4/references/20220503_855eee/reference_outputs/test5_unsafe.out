Building native CUDA modules...
/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
model:
  path: cifar_cnn_a_adv4.model
  name: cnn_4layer_adv4
data:
  start: 199
  end: 200
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR_SDP
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: 0.00784313725
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 4096
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 20
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 30
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  use_auto_attack: false
  use_diversed_pgd: false
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 20:54:37 2022 on diablo.cs.ucla.edu
Sequential(
  (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (3): ReLU()
  (4): Flatten()
  (5): Linear(in_features=2048, out_features=100, bias=True)
  (6): ReLU()
  (7): Linear(in_features=100, out_features=10, bias=True)
)
############################
Sampled data loaded. Data already preprocessed!
Shape: torch.Size([200, 3, 32, 32]) torch.Size([200]) torch.Size([200])
X range: tensor(2.1256) tensor(-1.9889) tensor(-0.0131)
############################
epsilon after preprocessing: tensor([[[[0.0317]],

         [[0.0322]],

         [[0.0300]]]]), data_max = tensor([[[[2.0587]],

         [[2.1256]],

         [[2.1154]]]]), data_min = tensor([[[[-1.9889]],

         [[-1.9807]],

         [[-1.7076]]]])
Task length: 1
saving results to Verified_ret_[cnn_4layer_adv4]_start=199_end=200_iter=20_b=4096_timeout=30_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=skip.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 199 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label 6, correct label 6, image norm 1735.158935546875, logits tensor([-6.1802, -5.9800, -5.0660, -4.9451, -5.0898, -5.0150, -4.4718, -5.6071,
        -6.8761, -5.1820], device='cuda:0', grad_fn=<SelectBackward>)
Model prediction is: tensor([[-6.1802, -5.9800, -5.0660, -4.9451, -5.0898, -5.0150, -4.4718, -5.6071,
         -6.8761, -5.1820]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[ 0.7373,  0.5094,  0.1153,  0.0642,  0.1335,  0.1235,  0.5090,  1.3784,
         -0.2645]], device='cuda:0') None
best_l after optimization: -3.542954206466675 with beta sum per layer: []
alpha/beta optimization time: 7.789313077926636
initial alpha-CROWN bounds: tensor([[ 0.7692,  0.5529,  0.1313,  0.0770,  0.1492,  0.1383,  0.5310,  1.4146,
         -0.2204]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-0.2204, device='cuda:0', grad_fn=<MinBackward1>)
##### [0:199] Tested against 9 ######
Model prediction is: tensor([[-6.1802, -5.9800, -5.0660, -4.9451, -5.0898, -5.0150, -4.4718, -5.6071,
         -6.8761, -5.1820]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /10 start_node /11
setting alpha for layer /10 start_node /21
not setting layer /10 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 16, 16, 16]) != torch.Size([2, 9, 1, 16, 16, 16]))
setting alpha for layer /12 start_node /21
not setting layer /12 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
not setting layer /22 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /9 torch.Size([1, 16, 16, 16])
1 /11 torch.Size([1, 32, 8, 8])
2 /21 torch.Size([1, 100])
best_l after optimization: 0.2203516960144043 with beta sum per layer: []
alpha/beta optimization time: 2.082568883895874
alpha-CROWN with fixed intermediate bounds: tensor([[-0.2204]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-0.2203516960144043
layer 0 size torch.Size([4096]) unstable 543
layer 1 size torch.Size([2048]) unstable 202
layer 2 size torch.Size([100]) unstable 6
-----------------
# of unstable neurons: 751
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  8
batch:  torch.Size([1, 16, 16, 16]) post split depth:  8
splitting decisions: 
split level 0: [1, 924] 
split level 1: [2, 27] 
split level 2: [1, 555] 
split level 3: [2, 62] 
split level 4: [1, 996] 
split level 5: [1, 1243] 
split level 6: [1, 370] 
split level 7: [1, 947] 
regular batch size: 2*128, diving batch size 1*0
best_l after optimization: 1.8956456184387207 with beta sum per layer: [0.0, 278.2142639160156, 51.342384338378906]
alpha/beta optimization time: 0.3115684986114502
This batch time : update_bounds func: 0.3574	 prepare: 0.0196	 bound: 0.3119	 transfer: 0.0098	 finalize: 0.0154
Accumulated time: update_bounds func: 0.3574	 prepare: 0.0196	 bound: 0.3119	 transfer: 0.0098	 finalize: 0.0154
batch bounding time:  0.3577570915222168
Current worst splitting domains [lb, ub] (depth):
[-0.17858,   inf] (9), [-0.17590,   inf] (9), [-0.17437,   inf] (9), [-0.17172,   inf] (9), [-0.16946,   inf] (9), [-0.16734,   inf] (9), [-0.16560,   inf] (9), [-0.16552,   inf] (9), [-0.16427,   inf] (9), [-0.16230,   inf] (9), [-0.16222,   inf] (9), [-0.16060,   inf] (9), [-0.15967,   inf] (9), [-0.15963,   inf] (9), [-0.15703,   inf] (9), [-0.15649,   inf] (9), [-0.15631,   inf] (9), [-0.15319,   inf] (9), [-0.15312,   inf] (9), [-0.15124,   inf] (9), 
length of domains: 131
Total time: 0.6642	 pickout: 0.0009	 decision: 0.2679	 get_bound: 0.3895	 add_domain: 0.0059
Current lb:-0.17857545614242554
256 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.5959482192993164

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([131, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([131, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 373] [1, 373] [1, 373] [1, 373] [1, 373] [1, 373] [1, 373] [1, 373] [1, 373] [1, 373] 
regular batch size: 2*131, diving batch size 1*0
best_l after optimization: 18.118186950683594 with beta sum per layer: [0.0, 324.0947570800781, 24.902990341186523]
alpha/beta optimization time: 0.3001093864440918
This batch time : update_bounds func: 0.3557	 prepare: 0.0266	 bound: 0.3004	 transfer: 0.0127	 finalize: 0.0153
Accumulated time: update_bounds func: 0.7131	 prepare: 0.0462	 bound: 0.6123	 transfer: 0.0127	 finalize: 0.0307
batch bounding time:  0.3561089038848877
Current worst splitting domains [lb, ub] (depth):
[-0.17424,   inf] (11), [-0.17154,   inf] (11), [-0.16973,   inf] (11), [-0.16710,   inf] (11), [-0.16494,   inf] (11), [-0.16288,   inf] (11), [-0.16098,   inf] (11), [-0.16082,   inf] (11), [-0.15964,   inf] (11), [-0.15754,   inf] (11), [-0.15745,   inf] (11), [-0.15715,   inf] (11), [-0.15601,   inf] (11), [-0.15475,   inf] (11), [-0.15463,   inf] (11), [-0.15446,   inf] (11), [-0.15445,   inf] (11), [-0.15197,   inf] (11), [-0.15178,   inf] (11), [-0.15174,   inf] (11), 
length of domains: 223
Total time: 0.4432	 pickout: 0.0203	 decision: 0.0560	 get_bound: 0.3565	 add_domain: 0.0104
Current lb:-0.1742423176765442
518 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.041276216506958

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([223, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([223, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 916] [1, 349] [1, 916] [1, 349] [1, 916] [1, 916] [1, 348] [1, 916] [1, 916] [1, 348] 
regular batch size: 2*223, diving batch size 1*0
best_l after optimization: 32.06077575683594 with beta sum per layer: [0.0, 627.593017578125, 34.64573287963867]
alpha/beta optimization time: 0.37537598609924316
This batch time : update_bounds func: 0.4648	 prepare: 0.0483	 bound: 0.3757	 transfer: 0.0137	 finalize: 0.0260
Accumulated time: update_bounds func: 1.1779	 prepare: 0.0945	 bound: 0.9880	 transfer: 0.0137	 finalize: 0.0566
batch bounding time:  0.4653465747833252
Current worst splitting domains [lb, ub] (depth):
[-0.17057,   inf] (13), [-0.16860,   inf] (13), [-0.16592,   inf] (13), [-0.16396,   inf] (13), [-0.16089,   inf] (13), [-0.15916,   inf] (13), [-0.15799,   inf] (13), [-0.15685,   inf] (13), [-0.15663,   inf] (13), [-0.15601,   inf] (13), [-0.15587,   inf] (13), [-0.15461,   inf] (13), [-0.15330,   inf] (13), [-0.15328,   inf] (13), [-0.15252,   inf] (13), [-0.15223,   inf] (13), [-0.15135,   inf] (13), [-0.15134,   inf] (13), [-0.15131,   inf] (13), [-0.15046,   inf] (13), 
length of domains: 412
Total time: 0.6057	 pickout: 0.0348	 decision: 0.0842	 get_bound: 0.4661	 add_domain: 0.0206
Current lb:-0.17057110369205475
964 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.650317192077637

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([412, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([412, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1633] [1, 1633] [1, 1633] [1, 1633] [1, 1633] [1, 1633] [1, 1633] [1, 1633] [1, 1633] [1, 1633] 
regular batch size: 2*412, diving batch size 1*0
best_l after optimization: 57.793479919433594 with beta sum per layer: [0.0, 1286.4195556640625, 57.03633117675781]
alpha/beta optimization time: 0.5221843719482422
This batch time : update_bounds func: 0.6776	 prepare: 0.0790	 bound: 0.5225	 transfer: 0.0260	 finalize: 0.0481
Accumulated time: update_bounds func: 1.8554	 prepare: 0.1735	 bound: 1.5106	 transfer: 0.0260	 finalize: 0.1047
batch bounding time:  0.6784067153930664
Current worst splitting domains [lb, ub] (depth):
[-0.16573,   inf] (15), [-0.16493,   inf] (15), [-0.16375,   inf] (15), [-0.16294,   inf] (15), [-0.16097,   inf] (15), [-0.16019,   inf] (15), [-0.15898,   inf] (15), [-0.15819,   inf] (15), [-0.15600,   inf] (15), [-0.15521,   inf] (15), [-0.15427,   inf] (15), [-0.15346,   inf] (15), [-0.15314,   inf] (15), [-0.15233,   inf] (15), [-0.15171,   inf] (15), [-0.15161,   inf] (15), [-0.15094,   inf] (15), [-0.15093,   inf] (15), [-0.15081,   inf] (15), [-0.15080,   inf] (15), 
length of domains: 748
Total time: 0.9670	 pickout: 0.0641	 decision: 0.1825	 get_bound: 0.6796	 add_domain: 0.0408
Current lb:-0.16573429107666016
1788 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.6238112449646

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([748, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([748, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 349] [1, 349] [1, 916] [1, 916] [1, 349] [1, 349] [1, 916] [1, 916] [1, 349] [1, 349] 
regular batch size: 2*748, diving batch size 1*0
best_l after optimization: 100.99653625488281 with beta sum per layer: [0.8150204420089722, 2678.83203125, 86.53284454345703]
alpha/beta optimization time: 0.8101396560668945
This batch time : update_bounds func: 1.0925	 prepare: 0.1471	 bound: 0.8105	 transfer: 0.0424	 finalize: 0.0890
Accumulated time: update_bounds func: 2.9480	 prepare: 0.3206	 bound: 2.3211	 transfer: 0.0424	 finalize: 0.1937
batch bounding time:  1.0938386917114258
Current worst splitting domains [lb, ub] (depth):
[-0.16253,   inf] (17), [-0.16170,   inf] (17), [-0.16090,   inf] (17), [-0.16008,   inf] (17), [-0.15774,   inf] (17), [-0.15694,   inf] (17), [-0.15609,   inf] (17), [-0.15529,   inf] (17), [-0.15288,   inf] (17), [-0.15208,   inf] (17), [-0.15120,   inf] (17), [-0.15038,   inf] (17), [-0.15017,   inf] (17), [-0.15001,   inf] (17), [-0.14935,   inf] (17), [-0.14920,   inf] (17), [-0.14854,   inf] (17), [-0.14786,   inf] (17), [-0.14774,   inf] (17), [-0.14729,   inf] (17), 
length of domains: 1311
Total time: 1.5873	 pickout: 0.1211	 decision: 0.2950	 get_bound: 1.0961	 add_domain: 0.0750
Current lb:-0.16252708435058594
3284 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.22292423248291

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1311, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([1311, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1746] [1, 1746] [1, 348] [1, 348] [1, 1746] [1, 1746] [1, 348] [1, 348] [1, 1746] [1, 1746] 
regular batch size: 2*1311, diving batch size 1*0
best_l after optimization: 177.19268798828125 with beta sum per layer: [1.4842584133148193, 5491.68310546875, 104.42667388916016]
alpha/beta optimization time: 1.3180296421051025
This batch time : update_bounds func: 1.8813	 prepare: 0.2579	 bound: 1.3185	 transfer: 0.0858	 finalize: 0.1628
Accumulated time: update_bounds func: 4.8292	 prepare: 0.5785	 bound: 3.6395	 transfer: 0.0858	 finalize: 0.3566
batch bounding time:  1.8840677738189697
Current worst splitting domains [lb, ub] (depth):
[-0.15966,   inf] (19), [-0.15884,   inf] (19), [-0.15797,   inf] (19), [-0.15716,   inf] (19), [-0.15484,   inf] (19), [-0.15473,   inf] (19), [-0.15404,   inf] (19), [-0.15395,   inf] (19), [-0.15313,   inf] (19), [-0.15233,   inf] (19), [-0.15004,   inf] (19), [-0.14991,   inf] (19), [-0.14925,   inf] (19), [-0.14914,   inf] (19), [-0.14835,   inf] (19), [-0.14753,   inf] (19), [-0.14715,   inf] (19), [-0.14705,   inf] (19), [-0.14633,   inf] (19), [-0.14624,   inf] (19), 
length of domains: 2401
Total time: 2.7195	 pickout: 0.2148	 decision: 0.4674	 get_bound: 1.8885	 add_domain: 0.1488
Current lb:-0.1596648395061493
5906 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.967437744140625

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2401, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([2401, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 180] [1, 180] [1, 180] [1, 180] [1, 180] [1, 180] [1, 180] [1, 180] [1, 180] [1, 180] 
regular batch size: 2*2401, diving batch size 1*0
best_l after optimization: 323.50909423828125 with beta sum per layer: [1.4903051853179932, 11557.900390625, 134.01913452148438]
alpha/beta optimization time: 2.359886646270752
This batch time : update_bounds func: 3.3769	 prepare: 0.4728	 bound: 2.3603	 transfer: 0.1652	 finalize: 0.3672
Accumulated time: update_bounds func: 8.2062	 prepare: 1.0513	 bound: 5.9998	 transfer: 0.1652	 finalize: 0.7237
batch bounding time:  3.381516218185425
Current worst splitting domains [lb, ub] (depth):
[-0.15619,   inf] (21), [-0.15536,   inf] (21), [-0.15457,   inf] (21), [-0.15375,   inf] (21), [-0.15211,   inf] (21), [-0.15141,   inf] (21), [-0.15129,   inf] (21), [-0.15121,   inf] (21), [-0.15061,   inf] (21), [-0.15049,   inf] (21), [-0.15043,   inf] (21)/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/utils.py:462: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  eps_temp = torch.tensor(preprocess_cifar(eps_temp, perturbation=True)).reshape(1,-1,1,1)
, [-0.14978,   inf] (21), [-0.14966,   inf] (21), [-0.14897,   inf] (21), [-0.14715,   inf] (21), [-0.14657,   inf] (21), [-0.14646,   inf] (21), [-0.14637,   inf] (21), [-0.14619,   inf] (21), [-0.14577,   inf] (21), 
length of domains: 4502
Total time: 5.1913	 pickout: 0.4339	 decision: 0.9878	 get_bound: 3.3892	 add_domain: 0.3805
Current lb:-0.1561858206987381
10708 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 15.199815034866333

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4096, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4096, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 926] [1, 926] [1, 926] [1, 926] [1, 926] [1, 926] [1, 926] [1, 926] [1, 926] [1, 926] 
regular batch size: 2*4096, diving batch size 1*0
best_l after optimization: 603.0489501953125 with beta sum per layer: [0.0, 20969.173828125, 61.69557189941406]
alpha/beta optimization time: 3.903041362762451
This batch time : update_bounds func: 5.7577	 prepare: 0.8093	 bound: 3.9035	 transfer: 0.2779	 finalize: 0.7482
Accumulated time: update_bounds func: 13.9639	 prepare: 1.8606	 bound: 9.9033	 transfer: 0.2779	 finalize: 1.4720
batch bounding time:  5.76512885093689
Current worst splitting domains [lb, ub] (depth):
[-0.15232,   inf] (23), [-0.15171,   inf] (23), [-0.15150,   inf] (23), [-0.15089,   inf] (23), [-0.15069,   inf] (23), [-0.15002,   inf] (23), [-0.14987,   inf] (23), [-0.14920,   inf] (23), [-0.14818,   inf] (23), [-0.14758,   inf] (23), [-0.14755,   inf] (23), [-0.14735,   inf] (23), [-0.14734,   inf] (23), [-0.14694,   inf] (23), [-0.14674,   inf] (23), [-0.14674,   inf] (23), [-0.14674,   inf] (23), [-0.14656,   inf] (23), [-0.14654,   inf] (23), [-0.14614,   inf] (23), 
length of domains: 8593
Total time: 8.9053	 pickout: 0.8060	 decision: 1.6196	 get_bound: 5.7784	 add_domain: 0.7012
Current lb:-0.15231536328792572
18900 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Time out!!!!!!!!
Image 199 label 9 verification end, final lower bound -0.15231536328792572, upper bound inf, time: 24.33488130569458
199 -0.15231536328792572
Result: image 199 verification failure (with branch and bound).
Wall time: 34.060545682907104

number of correctly classified examples: 1
incorrectly classified idx (total 0): []
attack success idx (total 0): []
verification success idx (total 0): []
verification failure idx (total 1): [199]
final verified acc: 0.0%[1]
verifier is called on 1 examples.
total verified: 0
mean time [cnt:1] (excluding attack success): 34.00550675392151
