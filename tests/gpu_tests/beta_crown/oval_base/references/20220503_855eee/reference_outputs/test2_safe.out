Building native CUDA modules...
/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: specify-target
  complete_verifier: bab
  enable_incomplete_verification: false
  get_crown_verified_acc: false
model:
  path: cifar_base.pth
  name: cifar_model_base
data:
  start: 17
  end: 18
  num_outputs: 10
  mean: [0.485, 0.456, 0.406]
  std: [0.225, 0.225, 0.225]
  pkl_path: base_100.pkl
  dataset: CIFAR
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: null
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 1024
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 20
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 60
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: fsb
    candidates: 1
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  use_auto_attack: false
  use_diversed_pgd: false
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Tue May  3 01:26:52 2022 on diablo.cs.ucla.edu
Sequential(
  (0): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (3): ReLU()
  (4): Flatten()
  (5): Linear(in_features=1024, out_features=100, bias=True)
  (6): ReLU()
  (7): Linear(in_features=100, out_features=10, bias=True)
)
No epsilon defined!
Files already downloaded and verified
Overwrite epsilon that saved in .pkl file, they should be after normalized!
Task length: 1
saving results to Verified_ret_[cifar_model_base]_start=17_end=18_iter=20_b=1024_timeout=60_branching=fsb-min-1_lra-init=0.1_lra=0.01_lrb=0.05_PGD=skip.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 17 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label 4, correct label 4, image norm 2094.38427734375, logits tensor([ 1.0358, -1.1430,  0.8130, -0.3865,  1.8739, -0.4889, -0.8827,  0.4617,
        -1.5547,  0.2713], device='cuda:0', grad_fn=<SelectBackward>)
##### [0:17] Tested against 1 ######
Model prediction is: tensor([[ 1.0358, -1.1430,  0.8130, -0.3865,  1.8739, -0.4889, -0.8827,  0.4617,
         -1.5547,  0.2713]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[-1.8571]], device='cuda:0') None
best_l after optimization: 1.5165337324142456 with beta sum per layer: []
alpha/beta optimization time: 7.513155937194824
initial alpha-CROWN bounds: tensor([[-1.5165]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-1.5165, device='cuda:0', grad_fn=<MinBackward1>)
-1.5165337324142456
layer 0 size torch.Size([2048]) unstable 375
layer 1 size torch.Size([1024]) unstable 304
layer 2 size torch.Size([100]) unstable 51
-----------------
# of unstable neurons: 730
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 8, 16, 16]) pre split depth:  6
batch:  torch.Size([1, 8, 16, 16]) post split depth:  6
splitting decisions: 
split level 0: [2, 24] 
split level 1: [2, 60] 
split level 2: [2, 39] 
split level 3: [2, 7] 
split level 4: [2, 71] 
split level 5: [2, 6] 
regular batch size: 2*32, diving batch size 1*0
best_l after optimization: -10.591901779174805 with beta sum per layer: [0.0, 0.0, 8.682672500610352]
alpha/beta optimization time: 0.25539159774780273
This batch time : update_bounds func: 0.2682	 prepare: 0.0071	 bound: 0.2557	 transfer: 0.0011	 finalize: 0.0041
Accumulated time: update_bounds func: 0.2682	 prepare: 0.0071	 bound: 0.2557	 transfer: 0.0011	 finalize: 0.0041
batch bounding time:  0.26841306686401367
Current worst splitting domains [lb, ub] (depth):
[-0.50312,   inf] (7), [-0.46394,   inf] (7), [-0.43242,   inf] (7), [-0.43031,   inf] (7), [-0.42997,   inf] (7), [-0.41638,   inf] (7), [-0.41480,   inf] (7), [-0.39583,   inf] (7), [-0.03459,   inf] (7), [-0.02870,   inf] (7), 
length of domains: 10
Total time: 0.3499	 pickout: 0.0010	 decision: 0.0724	 get_bound: 0.2760	 add_domain: 0.0005
Current lb:-0.5031232833862305
64 neurons visited
0 diving domains visited
Global ub: tensor([[inf]], device='cuda:0'), batch ub: inf
Cumulative time: 9.640063285827637

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([10, 8, 16, 16]) pre split depth:  3
batch:  torch.Size([10, 8, 16, 16]) post split depth:  3
splitting decisions: 
split level 0: [2, 56] [2, 56] [2, 56] [2, 56] [2, 56] [2, 56] [2, 56] [2, 56] [2, 56] [2, 56] 
split level 1: [2, 59] [2, 59] [2, 59] [1, 617] [2, 59] [2, 59] [2, 59] [2, 59] [2, 59] [2, 59] 
split level 2: [1, 618] [1, 722] [2, 42] [2, 42] [2, 42] [2, 42] [2, 42] [1, 722] [2, 42] [2, 42] 
regular batch size: 2*40, diving batch size 1*0
best_l after optimization: -3.2157256603240967 with beta sum per layer: [0.0, 1.1582216024398804, 42.00910186767578]
alpha/beta optimization time: 0.25147247314453125
This batch time : update_bounds func: 0.2680	 prepare: 0.0086	 bound: 0.2518	 transfer: 0.0021	 finalize: 0.0053
Accumulated time: update_bounds func: 0.5362	 prepare: 0.0157	 bound: 0.5075	 transfer: 0.0021	 finalize: 0.0094
batch bounding time:  0.26830124855041504
Current worst splitting domains [lb, ub] (depth):
[-0.36843,   inf] (11), [-0.32699,   inf] (11), [-0.30612,   inf] (11), [-0.30449,   inf] (11), [-0.29058,   inf] (11), [-0.28989,   inf] (11), [-0.28530,   inf] (11), [-0.28148,   inf] (11), [-0.26614,   inf] (11), [-0.26492,   inf] (11), [-0.25513,   inf] (11), [-0.24088,   inf] (11), [-0.15694,   inf] (11), [-0.12873,   inf] (11), [-0.12249,   inf] (11), [-0.11692,   inf] (11), [-0.10675,   inf] (11), [-0.10631,   inf] (11), [-0.10374,   inf] (11), [-0.10088,   inf] (11), 
length of domains: 34
Total time: 0.3334	 pickout: 0.0019	 decision: 0.0522	 get_bound: 0.2778	 add_domain: 0.0014
Current lb:-0.368426650762558
144 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.973928928375244

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([34, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([34, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 626] [1, 626] [1, 626] [1, 626] [2, 59] [2, 59] [2, 68] [2, 68] [2, 68] [2, 68] 
regular batch size: 2*34, diving batch size 1*0
best_l after optimization: 5.502034664154053 with beta sum per layer: [0.0, 1.503143310546875, 42.5576171875]
alpha/beta optimization time: 0.2519052028656006
This batch time : update_bounds func: 0.2656	 prepare: 0.0080	 bound: 0.2522	 transfer: 0.0012	 finalize: 0.0041
Accumulated time: update_bounds func: 0.8018	 prepare: 0.0236	 bound: 0.7597	 transfer: 0.0012	 finalize: 0.0135
batch bounding time:  0.26580095291137695
Current worst splitting domains [lb, ub] (depth):
[-0.32884,   inf] (13), [-0.32060,   inf] (13), [-0.29070,   inf] (13), [-0.27723,   inf] (13), [-0.27430,   inf] (13), [-0.26984,   inf] (13), [-0.25366,   inf] (13), [-0.25360,   inf] (13), [-0.25213,   inf] (13), [-0.24556,   inf] (13), [-0.24280,   inf] (13), [-0.23745,   inf] (13), [-0.22619,   inf] (13), [-0.22520,   inf] (13), [-0.22016,   inf] (13), [-0.20745,   inf] (13), [-0.20587,   inf] (13), [-0.19331,   inf] (13), [-0.09863,   inf] (13), [-0.09329,   inf] (13), 
length of domains: 51
Total time: 0.3076	 pickout: 0.0053	 decision: 0.0343	 get_bound: 0.2659	 add_domain: 0.0020
Current lb:-0.32884466648101807
212 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.282036066055298

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([51, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([51, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 617] [2, 42] [1, 617] [1, 617] [2, 42] [1, 617] [2, 42] [2, 68] [2, 68] [2, 96] 
regular batch size: 2*51, diving batch size 1*0
best_l after optimization: 2.1560611724853516 with beta sum per layer: [0.0, 2.6084511280059814, 65.1064682006836]
alpha/beta optimization time: 0.24852490425109863
This batch time : update_bounds func: 0.2681	 prepare: 0.0112	 bound: 0.2488	 transfer: 0.0015	 finalize: 0.0063
Accumulated time: update_bounds func: 1.0699	 prepare: 0.0349	 bound: 1.0085	 transfer: 0.0015	 finalize: 0.0197
batch bounding time:  0.2682685852050781
Current worst splitting domains [lb, ub] (depth):
[-0.29519,   inf] (15), [-0.29082,   inf] (15), [-0.28898,   inf] (15), [-0.25972,   inf] (15), [-0.25563,   inf] (15), [-0.25031,   inf] (15), [-0.24474,   inf] (15), [-0.24087,   inf] (15), [-0.23881,   inf] (15), [-0.23480,   inf] (15), [-0.22634,   inf] (15), [-0.22413,   inf] (15), [-0.22379,   inf] (15), [-0.21255,   inf] (15), [-0.21117,   inf] (15), [-0.21066,   inf] (15), [-0.21023,   inf] (15), [-0.20410,   inf] (15), [-0.20307,   inf] (15), [-0.20174,   inf] (15), 
length of domains: 51
Total time: 0.3230	 pickout: 0.0075	 decision: 0.0451	 get_bound: 0.2684	 add_domain: 0.0020
Current lb:-0.2951904535293579
314 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.60586142539978

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([51, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([51, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 42] [2, 42] [2, 68] [2, 42] [2, 42] [2, 42] [2, 68] [2, 42] [2, 42] [2, 42] 
regular batch size: 2*51, diving batch size 1*0
best_l after optimization: 3.07559871673584 with beta sum per layer: [0.0, 3.0814831256866455, 55.67388916015625]
alpha/beta optimization time: 0.24789071083068848
This batch time : update_bounds func: 0.2679	 prepare: 0.0113	 bound: 0.2482	 transfer: 0.0020	 finalize: 0.0061
Accumulated time: update_bounds func: 1.3378	 prepare: 0.0462	 bound: 1.2567	 transfer: 0.0020	 finalize: 0.0259
batch bounding time:  0.26804542541503906
Current worst splitting domains [lb, ub] (depth):
[-0.26523,   inf] (17), [-0.26072,   inf] (17), [-0.24615,   inf] (17), [-0.23094,   inf] (17), [-0.22668,   inf] (17), [-0.21905,   inf] (17), [-0.21003,   inf] (17), [-0.20993,   inf] (17), [-0.20691,   inf] (17), [-0.20593,   inf] (17), [-0.20513,   inf] (17), [-0.20344,   inf] (17), [-0.19238,   inf] (17), [-0.18628,   inf] (17), [-0.18410,   inf] (17), [-0.18279,   inf] (17), [-0.18269,   inf] (17), [-0.17972,   inf] (17), [-0.17961,   inf] (17), [-0.17926,   inf] (17), 
length of domains: 42
Total time: 0.3221	 pickout: 0.0075	 decision: 0.0448	 get_bound: 0.2682	 add_domain: 0.0016
Current lb:-0.26523107290267944
416 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.92876124382019

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([42, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([42, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 68] [2, 68] [1, 617] [2, 68] [2, 68] [2, 68] [2, 68] [2, 68] [1, 617] [2, 68] 
regular batch size: 2*42, diving batch size 1*0
best_l after optimization: 8.86208724975586 with beta sum per layer: [0.0, 3.371293067932129, 22.676755905151367]
alpha/beta optimization time: 0.25060153007507324
This batch time : update_bounds func: 0.2670	 prepare: 0.0096	 bound: 0.2509	 transfer: 0.0012	 finalize: 0.0051
Accumulated time: update_bounds func: 1.6047	 prepare: 0.0558	 bound: 1.5076	 transfer: 0.0012	 finalize: 0.0310
batch bounding time:  0.26714587211608887
Current worst splitting domains [lb, ub] (depth):
[-0.22489,   inf] (19), [-0.22031,   inf] (19), [-0.21562,   inf] (19), [-0.20667,   inf] (19), [-0.19072,   inf] (19), [-0.18642,   inf] (19), [-0.18124,   inf] (19), [-0.18065,   inf] (19), [-0.17862,   inf] (19), [-0.17417,   inf] (19), [-0.17251,   inf] (19), [-0.17216,   inf] (19), [-0.17184,   inf] (19), [-0.17017,   inf] (19), [-0.16980,   inf] (19), [-0.16769,   inf] (19), [-0.16737,   inf] (19), [-0.16645,   inf] (19), [-0.16567,   inf] (19), [-0.16282,   inf] (19), 
length of domains: 66
Total time: 0.3141	 pickout: 0.0063	 decision: 0.0378	 get_bound: 0.2673	 add_domain: 0.0027
Current lb:-0.22489063441753387
500 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 11.243457078933716

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([66, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([66, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 275] [1, 275] [1, 275] [1, 275] [2, 96] [2, 96] [1, 275] [1, 721] [1, 275] [2, 96] 
regular batch size: 2*66, diving batch size 1*0
best_l after optimization: 15.469696044921875 with beta sum per layer: [0.0, 4.799942970275879, 14.944759368896484]
alpha/beta optimization time: 0.25167346000671387
This batch time : update_bounds func: 0.2774	 prepare: 0.0141	 bound: 0.2520	 transfer: 0.0026	 finalize: 0.0084
Accumulated time: update_bounds func: 1.8821	 prepare: 0.0699	 bound: 1.7595	 transfer: 0.0026	 finalize: 0.0393
batch bounding time:  0.27770423889160156
Current worst splitting domains [lb, ub] (depth):
[-0.20119,   inf] (21), [-0.20028,   inf] (21), [-0.19660,   inf] (21), [-0.19569,   inf] (21), [-0.19126,   inf] (21), [-0.19035,   inf] (21), [-0.18266,   inf] (21), [-0.18175,   inf] (21), [-0.17111,   inf] (21), [-0.16708,   inf] (21), [-0.16064,   inf] (21), [-0.15868,   inf] (21), [-0.15695,   inf] (21), [-0.15602,   inf] (21), [-0.15489,   inf] (21), [-0.15402,   inf] (21), [-0.15387,   inf] (21), [-0.15296,   inf] (21), [-0.15285,   inf] (21), [-0.15159,   inf] (21), 
length of domains: 128
Total time: 0.3451	 pickout: 0.0094	 decision: 0.0524	 get_bound: 0.2779	 add_domain: 0.0054
Current lb:-0.20118893682956696
632 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 11.589560270309448

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([128, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([128, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 96] [2, 96] [2, 96] [2, 96] [2, 96] [2, 96] [2, 96] [2, 96] [1, 618] [1, 618] 
regular batch size: 2*128, diving batch size 1*0
best_l after optimization: 23.8599853515625 with beta sum per layer: [0.0, 18.082172393798828, 25.85015106201172]
alpha/beta optimization time: 0.2585148811340332
This batch time : update_bounds func: 0.3078	 prepare: 0.0257	 bound: 0.2588	 transfer: 0.0071	 finalize: 0.0155
Accumulated time: update_bounds func: 2.1899	 prepare: 0.0957	 bound: 2.0184	 transfer: 0.0071	 finalize: 0.0548
batch bounding time:  0.30811238288879395
Current worst splitting domains [lb, ub] (depth):
[-0.17996,   inf] (23), [-0.17939,   inf] (23), [-0.17769,   inf] (23), [-0.17662,   inf] (23), [-0.17536,   inf] (23), [-0.17478,   inf] (23), [-0.17308,   inf] (23), [-0.17202,   inf] (23), [-0.16965,   inf] (23), [-0.16909,   inf] (23), [-0.16839,   inf] (23), [-0.16742,   inf] (23), [-0.16109,   inf] (23), [-0.16054,   inf] (23), [-0.15954,   inf] (23), [-0.15856,   inf] (23), [-0.15571,   inf] (23), [-0.14913,   inf] (23), [-0.14181,   inf] (23), [-0.14045,   inf] (23), 
length of domains: 256
Total time: 0.4282	 pickout: 0.0176	 decision: 0.0911	 get_bound: 0.3085	 add_domain: 0.0110
Current lb:-0.17996102571487427
888 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 12.019396781921387

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([256, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([256, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 310] [1, 310] [1, 310] [1, 310] [1, 310] [1, 310] [1, 310] [1, 310] [1, 310] [1, 310] 
regular batch size: 2*256, diving batch size 1*0
best_l after optimization: 30.815227508544922 with beta sum per layer: [0.0, 49.48543930053711, 56.78622817993164]
alpha/beta optimization time: 0.2852170467376709
This batch time : update_bounds func: 0.3831	 prepare: 0.0507	 bound: 0.2855	 transfer: 0.0137	 finalize: 0.0321
Accumulated time: update_bounds func: 2.5731	 prepare: 0.1464	 bound: 2.3039	 transfer: 0.0137	 finalize: 0.0870
batch bounding time:  0.3836672306060791
Current worst splitting domains [lb, ub] (depth):
[-0.15950,   inf] (25), [-0.15915,   inf] (25), [-0.15703,   inf] (25), [-0.15615,   inf] (25), [-0.15483,   inf] (25), [-0.15449,   inf] (25), [-0.15319,   inf] (25), [-0.15284,   inf] (25), [-0.15237,   inf] (25), [-0.15168,   inf] (25), [-0.15146,   inf] (25), [-0.15087,   inf] (25), [-0.14913,   inf] (25), [-0.14881,   inf] (25), [-0.14853,   inf] (25), [-0.14817,   inf] (25), [-0.14764,   inf] (25), [-0.14712,   inf] (25), [-0.14688,   inf] (25), [-0.14626,   inf] (25), 
length of domains: 488
Total time: 0.6273	 pickout: 0.0348	 decision: 0.1869	 get_bound: 0.3844	 add_domain: 0.0212
Current lb:-0.15949858725070953
1400 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 12.650062561035156

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([488, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([488, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 180] [1, 180] [1, 721] [1, 721] [1, 180] [1, 180] [1, 180] [1, 180] [1, 721] [1, 721] 
regular batch size: 2*488, diving batch size 1*0
best_l after optimization: 39.411598205566406 with beta sum per layer: [0.0, 124.4640884399414, 122.13250732421875]
alpha/beta optimization time: 0.37806105613708496
This batch time : update_bounds func: 0.5635	 prepare: 0.0971	 bound: 0.3784	 transfer: 0.0226	 finalize: 0.0632
Accumulated time: update_bounds func: 3.1366	 prepare: 0.2435	 bound: 2.6823	 transfer: 0.0226	 finalize: 0.1502
batch bounding time:  0.5644447803497314
Current worst splitting domains [lb, ub] (depth):
[-0.14572,   inf] (27), [-0.14555,   inf] (27), [-0.14106,   inf] (27), [-0.14092,   inf] (27), [-0.13939,   inf] (27), [-0.13925,   inf] (27), [-0.13670,   inf] (27), [-0.13636,   inf] (27), [-0.13496,   inf] (27), [-0.13487,   inf] (27), [-0.13474,   inf] (27), [-0.13460,   inf] (27), [-0.13200,   inf] (27), [-0.13168,   inf] (27), [-0.13145,   inf] (27), [-0.13114,   inf] (27), [-0.12887,   inf] (27), [-0.12863,   inf] (27), [-0.12854,   inf] (27), [-0.12796,   inf] (27), 
length of domains: 811
Total time: 1.0418	 pickout: 0.0662	 decision: 0.3719	 get_bound: 0.5659	 add_domain: 0.0378
Current lb:-0.14571642875671387
2376 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 13.700159072875977

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([811, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([811, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 721] [1, 721] [1, 721] [1, 721] [1, 721] [1, 721] [1, 722] [1, 722] [1, 721] [1, 721] 
regular batch size: 2*811, diving batch size 1*0
best_l after optimization: 33.576271057128906 with beta sum per layer: [0.0, 205.00250244140625, 190.98623657226562]
alpha/beta optimization time: 0.506218433380127
This batch time : update_bounds func: 0.7995	 prepare: 0.1592	 bound: 0.5066	 transfer: 0.0269	 finalize: 0.1030
Accumulated time: update_bounds func: 3.9361	 prepare: 0.4027	 bound: 3.1888	 transfer: 0.0269	 finalize: 0.2532
batch bounding time:  0.8009889125823975
Current worst splitting domains [lb, ub] (depth):
[-0.12607,   inf] (29), [-0.12578,   inf] (29), [-0.12299,   inf] (29), [-0.12142,   inf] (29), [-0.12112,   inf] (29), [-0.11977,   inf] (29), [-0.11947,   inf] (29), [-0.11904,   inf] (29), [-0.11830,   inf] (29), [-0.11804,   inf] (29), [-0.11783,   inf] (29), [-0.11777,   inf] (29), [-0.11534,   inf] (29), [-0.11513,   inf] (29), [-0.11510,   inf] (29), [-0.11504,   inf] (29), [-0.11482,   inf] (29), [-0.11433,   inf] (29), [-0.11382,   inf] (29), [-0.11378,   inf] (29), 
length of domains: 1114
Total time: 1.5392	 pickout: 0.1115	 decision: 0.5711	 get_bound: 0.8035	 add_domain: 0.0532
Current lb:-0.12606912851333618
3998 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 15.25299620628357

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1024, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1024, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 722] [1, 722] [1, 714] [1, 722] [1, 722] [1, 722] [1, 722] [1, 180] [1, 180] [1, 722] 
regular batch size: 2*1024, diving batch size 1*0
best_l after optimization: 25.852893829345703 with beta sum per layer: [0.0, 282.4507751464844, 242.82855224609375]
alpha/beta optimization time: 0.5982024669647217
This batch time : update_bounds func: 0.9760	 prepare: 0.2063	 bound: 0.5986	 transfer: 0.0361	 finalize: 0.1300
Accumulated time: update_bounds func: 4.9121	 prepare: 0.6090	 bound: 3.7874	 transfer: 0.0361	 finalize: 0.3832
batch bounding time:  0.977750301361084
Current worst splitting domains [lb, ub] (depth):
[-0.11286,   inf] (31), [-0.10858,   inf] (31), [-0.10821,   inf] (31), [-0.10655,   inf] (31), [-0.10622,   inf] (31), [-0.10584,   inf] (31), [-0.10522,   inf] (31), [-0.10521,   inf] (31), [-0.10473,   inf] (31), [-0.10395,   inf] (31), [-0.10227,   inf] (31), [-0.10214,   inf] (31), [-0.10208,   inf] (31), [-0.10188,   inf] (31), [-0.10129,   inf] (31), [-0.10060,   inf] (31), [-0.10060,   inf] (31), [-0.10053,   inf] (31), [-0.10031,   inf] (31), [-0.09887,   inf] (31), 
length of domains: 1359
Total time: 1.9541	 pickout: 0.1410	 decision: 0.6979	 get_bound: 0.9808	 add_domain: 0.1344
Current lb:-0.11286401748657227
6046 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 17.224653482437134

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1024, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1024, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 339] [1, 339] [1, 339] [1, 339] [1, 180] [1, 714] [1, 714] [1, 339] [1, 714] [1, 339] 
regular batch size: 2*1024, diving batch size 1*0
best_l after optimization: 29.699697494506836 with beta sum per layer: [0.0, 270.18572998046875, 244.67874145507812]
alpha/beta optimization time: 0.5954275131225586
This batch time : update_bounds func: 1.0214	 prepare: 0.2038	 bound: 0.5958	 transfer: 0.0370	 finalize: 0.1801
Accumulated time: update_bounds func: 5.9336	 prepare: 0.8127	 bound: 4.3832	 transfer: 0.0370	 finalize: 0.5633
batch bounding time:  1.0231602191925049
Current worst splitting domains [lb, ub] (depth):
[-0.09552,   inf] (33), [-0.09328,   inf] (33), [-0.09084,   inf] (33), [-0.09029,   inf] (33), [-0.08927,   inf] (33), [-0.08919,   inf] (33), [-0.08825,   inf] (33), [-0.08812,   inf] (33), [-0.08792,   inf] (33), [-0.08738,   inf] (33), [-0.08585,   inf] (33), [-0.08561,   inf] (33), [-0.08503,   inf] (33), [-0.08481,   inf] (33), [-0.08453,   inf] (33), [-0.08432,   inf] (33), [-0.08410,   inf] (33), [-0.08397,   inf] (33), [-0.08382,   inf] (33), [-0.08351,   inf] (33), 
length of domains: 1705
Total time: 1.8869	 pickout: 0.1461	 decision: 0.6446	 get_bound: 1.0262	 add_domain: 0.0699
Current lb:-0.09551951289176941
8094 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 19.130510568618774

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1024, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1024, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 126] [1, 339] [1, 126] [1, 126] [1, 339] [1, 126] [1, 339] [1, 339] [1, 126] [1, 126] 
regular batch size: 2*1024, diving batch size 1*0
best_l after optimization: 23.978914260864258 with beta sum per layer: [0.0, 219.23497009277344, 151.80995178222656]
alpha/beta optimization time: 0.5955469608306885
This batch time : update_bounds func: 0.9686	 prepare: 0.2029	 bound: 0.5959	 transfer: 0.0390	 finalize: 0.1255
Accumulated time: update_bounds func: 6.9021	 prepare: 1.0156	 bound: 4.9790	 transfer: 0.0390	 finalize: 0.6888
batch bounding time:  0.9702756404876709
Current worst splitting domains [lb, ub] (depth):
[-0.07723,   inf] (35), [-0.07560,   inf] (35), [-0.07256,   inf] (35), [-0.07200,   inf] (35), [-0.07087,   inf] (35), [-0.07073,   inf] (35), [-0.07061,   inf] (35), [-0.06984,   inf] (35), [-0.06964,   inf] (35), [-0.06926,   inf] (35), [-0.06909,   inf] (35), [-0.06797,   inf] (35), [-0.06733,   inf] (35), [-0.06729,   inf] (35), [-0.06707,   inf] (35), [-0.06648,   inf] (35), [-0.06630,   inf] (35), [-0.06616,   inf] (35), [-0.06595,   inf] (35), [-0.06582,   inf] (35), 
length of domains: 2121
Total time: 1.9004	 pickout: 0.1481	 decision: 0.7016	 get_bound: 0.9733	 add_domain: 0.0773
Current lb:-0.07722888141870499
10142 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 21.055155277252197

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1024, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1024, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 181] [1, 705] [1, 181] [1, 181] [1, 181] [1, 705] [1, 705] [1, 705] [1, 181] [1, 181] 
regular batch size: 2*1024, diving batch size 1*0
best_l after optimization: 16.418285369873047 with beta sum per layer: [0.0, 236.82040405273438, 98.11166381835938]
alpha/beta optimization time: 0.5950441360473633
This batch time : update_bounds func: 1.0560	 prepare: 0.2063	 bound: 0.5954	 transfer: 0.0386	 finalize: 0.2101
Accumulated time: update_bounds func: 7.9581	 prepare: 1.2219	 bound: 5.5745	 transfer: 0.0386	 finalize: 0.8989
batch bounding time:  1.0577528476715088
Current worst splitting domains [lb, ub] (depth):
[-0.06463,   inf] (37), [-0.06284,   inf] (37), [-0.05996,   inf] (37), [-0.05939,   inf] (37), [-0.05805,   inf] (37), [-0.05765,   inf] (37), [-0.05715,   inf] (37), [-0.05713,   inf] (37), [-0.05703,   inf] (37), [-0.05688,   inf] (37), [-0.05649,   inf] (37), [-0.05520,   inf] (37), [-0.05473,   inf] (37), [-0.05450,   inf] (37), [-0.05438,   inf] (37), [-0.05390,   inf] (37), [-0.05342,   inf] (37), [-0.05299,   inf] (37), [-0.05291,   inf] (37), [-0.05248,   inf] (37), 
length of domains: 2360
Total time: 1.9900	 pickout: 0.1497	 decision: 0.7097	 get_bound: 1.0609	 add_domain: 0.0697
Current lb:-0.06462574005126953
12190 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 23.065579414367676

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1024, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1024, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 714] [1, 181] [1, 714] [1, 714] [1, 181] [1, 181] [1, 714] [1, 181] [1, 714] [1, 714] 
regular batch size: 2*1024, diving batch size 1*0
best_l after optimization: -2.317786931991577 with beta sum per layer: [0.0, 213.15634155273438, 93.54082489013672]
alpha/beta optimization time: 0.5972244739532471
This batch time : update_bounds func: 0.9722	 prepare: 0.2035	 bound: 0.5976	 transfer: 0.0374	 finalize: 0.1283
Accumulated time: update_bounds func: 8.9304	 prepare: 1.4254	 bound: 6.1720	 transfer: 0.0374	 finalize: 1.0272
batch bounding time:  0.9739658832550049
Current worst splitting domains [lb, ub] (depth):
[-0.05025,   inf] (39), [-0.04903,   inf] (39), [-0.04549,   inf] (39), [-0.04456,   inf] (39), [-0.04438,   inf] (39), [-0.04420,   inf] (39), [-0.04371,   inf] (39), [-0.04260,   inf] (39), [-0.04231,   inf] (39), [-0.04210,   inf] (39), [-0.04181,   inf] (39), [-0.04154,   inf] (39), [-0.04138,   inf] (39), [-0.04132,   inf] (39), [-0.04081,   inf] (39), [-0.04079,   inf] (39), [-0.04019,   inf] (39), [-0.03980,   inf] (39), [-0.03955,   inf] (39), [-0.03943,   inf] (39), 
length of domains: 2148
Total time: 1.8921	 pickout: 0.1515	 decision: 0.7165	 get_bound: 0.9771	 add_domain: 0.0469
Current lb:-0.050254106521606445
14238 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 24.982052087783813

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1024, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1024, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 707] [1, 609] [1, 707] [1, 707] [1, 609] [1, 707] [1, 609] [1, 707] [1, 339] [1, 339] 
regular batch size: 2*1024, diving batch size 1*0
best_l after optimization: -19.442249298095703 with beta sum per layer: [0.0, 200.95550537109375, 108.05134582519531]
alpha/beta optimization time: 0.5926322937011719
This batch time : update_bounds func: 1.0451	 prepare: 0.2033	 bound: 0.5930	 transfer: 0.0393	 finalize: 0.2047
Accumulated time: update_bounds func: 9.9754	 prepare: 1.6287	 bound: 6.7650	 transfer: 0.0393	 finalize: 1.2319
batch bounding time:  1.047286033630371
Current worst splitting domains [lb, ub] (depth):
[-0.03805,   inf] (41), [-0.03328,   inf] (41), [-0.03246,   inf] (41), [-0.03197,   inf] (41), [-0.03182,   inf] (41), [-0.03040,   inf] (41), [-0.02964,   inf] (41), [-0.02857,   inf] (41), [-0.02857,   inf] (41), [-0.02789,   inf] (41), [-0.02767,   inf] (41), [-0.02729,   inf] (41), [-0.02650,   inf] (41), [-0.02638,   inf] (41), [-0.02573,   inf] (41), [-0.02485,   inf] (41), [-0.02467,   inf] (41), [-0.02448,   inf] (41), [-0.02435,   inf] (41), [-0.02433,   inf] (41), 
length of domains: 1494
Total time: 1.9435	 pickout: 0.1541	 decision: 0.7173	 get_bound: 1.0508	 add_domain: 0.0213
Current lb:-0.03805338591337204
16286 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 26.952094554901123

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1024, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1024, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 609] [1, 609] [1, 609] [1, 609] [1, 705] [1, 609] [1, 609] [1, 609] [1, 705] [1, 609] 
regular batch size: 2*1024, diving batch size 1*0
best_l after optimization: -35.073143005371094 with beta sum per layer: [0.0, 218.66055297851562, 141.07302856445312]
alpha/beta optimization time: 0.5935523509979248
This batch time : update_bounds func: 1.0304	 prepare: 0.2023	 bound: 0.5939	 transfer: 0.0394	 finalize: 0.1900
Accumulated time: update_bounds func: 11.0058	 prepare: 1.8310	 bound: 7.3588	 transfer: 0.0394	 finalize: 1.4219
batch bounding time:  1.0321805477142334
Current worst splitting domains [lb, ub] (depth):
[-0.02105,   inf] (43), [-0.01876,   inf] (43), [-0.01773,   inf] (43), [-0.01550,   inf] (43), [-0.01543,   inf] (43), [-0.01492,   inf] (43), [-0.01344,   inf] (43), [-0.01340,   inf] (43), [-0.01261,   inf] (43), [-0.01211,   inf] (43), [-0.01159,   inf] (43), [-0.01152,   inf] (43), [-0.01127,   inf] (43), [-0.01111,   inf] (43), [-0.01105,   inf] (43), [-0.01083,   inf] (43), [-0.01050,   inf] (43), [-0.01017,   inf] (43), [-0.00962,   inf] (43), [-0.00930,   inf] (43), 
length of domains: 557
Total time: 1.8443	 pickout: 0.1568	 decision: 0.6461	 get_bound: 1.0353	 add_domain: 0.0061
Current lb:-0.02105426788330078
18334 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 28.825411081314087

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([557, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([557, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 713] [1, 707] [1, 713] [1, 707] [1, 713] [1, 126] [1, 707] [1, 126] [1, 126] [1, 713] 
regular batch size: 2*557, diving batch size 1*0
best_l after optimization: -22.882312774658203 with beta sum per layer: [0.0, 130.6200408935547, 82.38870239257812]
alpha/beta optimization time: 0.35825276374816895
This batch time : update_bounds func: 0.5495	 prepare: 0.1119	 bound: 0.3587	 transfer: 0.0067	 finalize: 0.0695
Accumulated time: update_bounds func: 11.5553	 prepare: 1.9429	 bound: 7.7175	 transfer: 0.0067	 finalize: 1.4914
batch bounding time:  0.5505015850067139
Current worst splitting domains [lb, ub] (depth):
[-0.00669,   inf] (45), [-0.00342,   inf] (45), [-0.00260,   inf] (45), [-0.00137,   inf] (45), 
length of domains: 4
Total time: 0.9950	 pickout: 0.0870	 decision: 0.3551	 get_bound: 0.5522	 add_domain: 0.0007
Current lb:-0.006694316864013672
19448 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 29.834839820861816

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4, 8, 16, 16]) pre split depth:  4
batch:  torch.Size([4, 8, 16, 16]) post split depth:  4
splitting decisions: 
split level 0: [1, 713] [1, 713] [1, 126] [1, 713] 
split level 1: [1, 382] [1, 382] [1, 531] [1, 382] 
split level 2: [1, 531] [1, 531] [1, 598] [1, 531] 
split level 3: [2, 79] [2, 79] [2, 79] [2, 79] 
regular batch size: 2*32, diving batch size 1*0

all verified at 0th iter
best_l after optimization: -3.9855072498321533 with beta sum per layer: [0.0, 0.0, 1.0357155799865723]
alpha/beta optimization time: 0.008941888809204102
This batch time : update_bounds func: 0.0544	 prepare: 0.0076	 bound: 0.0092	 transfer: 0.0015	 finalize: 0.0358
Accumulated time: update_bounds func: 11.6097	 prepare: 1.9505	 bound: 7.7267	 transfer: 0.0015	 finalize: 1.5272
batch bounding time:  0.054499149322509766
Current worst splitting domains [lb, ub] (depth):

length of domains: 0
Total time: 0.1216	 pickout: 0.0015	 decision: 0.0578	 get_bound: 0.0622	 add_domain: 0.0000
No domains left, verification finished!
Global ub: inf, batch ub: inf
Cumulative time: 29.957815408706665

Image 17 label 1 verification end, final lower bound 1.0000000116860974e-07, upper bound inf, time: 30.022231340408325
17 1.0000000116860974e-07
Result: image 17 verification success (with branch and bound)!
Wall time: 30.0558123588562

number of correctly classified examples: 1
incorrectly classified idx (total 0): []
attack success idx (total 0): []
verification success idx (total 1): [17]
verification failure idx (total 0): []
final verified acc: 100.0%[1]
verifier is called on 1 examples.
total verified: 1
mean time [cnt:1] (excluding attack success): 30.022231340408325
