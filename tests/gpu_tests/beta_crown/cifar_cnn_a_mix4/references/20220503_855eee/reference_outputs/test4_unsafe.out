Building native CUDA modules...
/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
model:
  path: cifar_cnn_a_mix4.model
  name: cnn_4layer_mix4
data:
  start: 194
  end: 195
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR_SDP
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: 0.00784313725
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 4096
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 20
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 30
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  use_auto_attack: false
  use_diversed_pgd: false
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 21:07:25 2022 on diablo.cs.ucla.edu
Sequential(
  (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (3): ReLU()
  (4): Flatten()
  (5): Linear(in_features=2048, out_features=100, bias=True)
  (6): ReLU()
  (7): Linear(in_features=100, out_features=10, bias=True)
)
############################
Sampled data loaded. Data already preprocessed!
Shape: torch.Size([200, 3, 32, 32]) torch.Size([200]) torch.Size([200])
X range: tensor(2.1256) tensor(-1.9889) tensor(-0.0131)
############################
epsilon after preprocessing: tensor([[[[0.0317]],

         [[0.0322]],

         [[0.0300]]]]), data_max = tensor([[[[2.0587]],

         [[2.1256]],

         [[2.1154]]]]), data_min = tensor([[[[-1.9889]],

         [[-1.9807]],

         [[-1.7076]]]])
Task length: 1
saving results to Verified_ret_[cnn_4layer_mix4]_start=194_end=195_iter=20_b=4096_timeout=30_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=skip.npy

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 img ID: 194 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
predicted label 2, correct label 2, image norm 1223.34912109375, logits tensor([-20.5788, -25.8728, -18.4034, -22.5339, -18.7835, -22.3559, -20.8291,
        -22.1396, -23.9025, -25.5513], device='cuda:0',
       grad_fn=<SelectBackward>)
Model prediction is: tensor([[-20.5788, -25.8728, -18.4034, -22.5339, -18.7835, -22.3559, -20.8291,
         -22.1396, -23.9025, -25.5513]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[ 0.7879,  5.4719,  3.0611, -0.5059,  2.8269,  1.1729,  2.2674,  3.6454,
          5.3222]], device='cuda:0') None
best_l after optimization: -24.70962142944336 with beta sum per layer: []
alpha/beta optimization time: 7.943941354751587
initial alpha-CROWN bounds: tensor([[ 0.8623,  5.5830,  3.1049, -0.4539,  2.8764,  1.2274,  2.3426,  3.7514,
          5.4155]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-0.4539, device='cuda:0', grad_fn=<MinBackward1>)
##### [0:194] Tested against 4 ######
Model prediction is: tensor([[-20.5788, -25.8728, -18.4034, -22.5339, -18.7835, -22.3559, -20.8291,
         -22.1396, -23.9025, -25.5513]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /10 start_node /11
setting alpha for layer /10 start_node /21
not setting layer /10 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 16, 16, 16]) != torch.Size([2, 9, 1, 16, 16, 16]))
setting alpha for layer /12 start_node /21
not setting layer /12 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
not setting layer /22 start_node /23 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /9 torch.Size([1, 16, 16, 16])
1 /11 torch.Size([1, 32, 8, 8])
2 /21 torch.Size([1, 100])
best_l after optimization: 0.45387351512908936 with beta sum per layer: []
alpha/beta optimization time: 2.0470266342163086
alpha-CROWN with fixed intermediate bounds: tensor([[-0.4539]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-0.45387354493141174
layer 0 size torch.Size([4096]) unstable 942
layer 1 size torch.Size([2048]) unstable 292
layer 2 size torch.Size([100]) unstable 22
-----------------
# of unstable neurons: 1256
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  8
batch:  torch.Size([1, 16, 16, 16]) post split depth:  8
splitting decisions: 
split level 0: [2, 58] 
split level 1: [2, 97] 
split level 2: [2, 90] 
split level 3: [2, 67] 
split level 4: [2, 2] 
split level 5: [2, 79] 
split level 6: [2, 86] 
split level 7: [2, 56] 
regular batch size: 2*128, diving batch size 1*0
best_l after optimization: -27.0207576751709 with beta sum per layer: [0.0, 0.0, 23.913984298706055]
alpha/beta optimization time: 0.2928805351257324
This batch time : update_bounds func: 0.3374	 prepare: 0.0179	 bound: 0.2932	 transfer: 0.0109	 finalize: 0.0148
Accumulated time: update_bounds func: 0.3374	 prepare: 0.0179	 bound: 0.2932	 transfer: 0.0109	 finalize: 0.0148
batch bounding time:  0.33777713775634766
Current worst splitting domains [lb, ub] (depth):
[-0.22266,   inf] (9), [-0.19842,   inf] (9), [-0.19629,   inf] (9), [-0.17799,   inf] (9), [-0.17314,   inf] (9), [-0.15249,   inf] (9), [-0.14606,   inf] (9), [-0.12724,   inf] (9), [-0.12420,   inf] (9), [-0.12276,   inf] (9), [-0.10523,   inf] (9), [-0.10162,   inf] (9), [-0.10068,   inf] (9), [-0.07623,   inf] (9), [-0.06077,   inf] (9), [-0.05996,   inf] (9), [-0.05342,   inf] (9), [-0.04559,   inf] (9), [-0.04497,   inf] (9), [-0.03012,   inf] (9), 
length of domains: 28
Total time: 0.6385	 pickout: 0.0010	 decision: 0.2671	 get_bound: 0.3691	 add_domain: 0.0013
Current lb:-0.22265774011611938
256 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.532834053039551

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([28, 16, 16, 16]) pre split depth:  3
batch:  torch.Size([28, 16, 16, 16]) post split depth:  3
splitting decisions: 
split level 0: [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] [2, 83] 
split level 1: [2, 14] [1, 170] [2, 14] [1, 170] [1, 170] [1, 170] [1, 170] [2, 14] [1, 170] [2, 14] 
split level 2: [1, 170] [2, 14] [1, 170] [2, 14] [2, 14] [2, 14] [2, 14] [1, 170] [2, 14] [1, 170] 
regular batch size: 2*112, diving batch size 1*0
best_l after optimization: -8.108789443969727 with beta sum per layer: [0.0, 4.92331075668335, 57.92906951904297]
alpha/beta optimization time: 0.2872285842895508
This batch time : update_bounds func: 0.3340	 prepare: 0.0216	 bound: 0.2877	 transfer: 0.0109	 finalize: 0.0131
Accumulated time: update_bounds func: 0.6714	 prepare: 0.0395	 bound: 0.5809	 transfer: 0.0109	 finalize: 0.0279
batch bounding time:  0.3343369960784912
Current worst splitting domains [lb, ub] (depth):
[-0.20678,   inf] (13), [-0.20149,   inf] (13), [-0.18116,   inf] (13), [-0.18085,   inf] (13), [-0.17714,   inf] (13), [-0.17488,   inf] (13), [-0.16145,   inf] (13), [-0.15674,   inf] (13), [-0.15625,   inf] (13), [-0.15196,   inf] (13), [-0.13378,   inf] (13), [-0.13140,   inf] (13), [-0.12827,   inf] (13), [-0.12450,   inf] (13), [-0.11063,   inf] (13), [-0.10613,   inf] (13), [-0.10549,   inf] (13), [-0.10353,   inf] (13), [-0.10235,   inf] (13), [-0.10173,   inf] (13), 
length of domains: 53
Total time: 0.4082	 pickout: 0.0048	 decision: 0.0411	 get_bound: 0.3594	 add_domain: 0.0028
Current lb:-0.20677676796913147
480 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.9422614574432373

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([53, 16, 16, 16]) pre split depth:  2
batch:  torch.Size([53, 16, 16, 16]) post split depth:  2
splitting decisions: 
split level 0: [1, 1355] [1, 1355] [1, 1995] [1, 1355] [1, 1355] [1, 1995] [1, 1355] [1, 1355] [1, 1355] [1, 1355] 
split level 1: [1, 1113] [1, 1113] [1, 1355] [1, 1354] [1, 1354] [1, 1355] [1, 1113] [1, 1113] [1, 1995] [1, 1995] 
regular batch size: 2*106, diving batch size 1*0
best_l after optimization: 13.205428123474121 with beta sum per layer: [0.0, 24.180007934570312, 28.364181518554688]
alpha/beta optimization time: 0.28696250915527344
This batch time : update_bounds func: 0.3265	 prepare: 0.0218	 bound: 0.2873	 transfer: 0.0045	 finalize: 0.0123
Accumulated time: update_bounds func: 0.9979	 prepare: 0.0613	 bound: 0.8682	 transfer: 0.0045	 finalize: 0.0402
batch bounding time:  0.3267979621887207
Current worst splitting domains [lb, ub] (depth):
[-0.19918,   inf] (16), [-0.19652,   inf] (16), [-0.19560,   inf] (16), [-0.19393,   inf] (16), [-0.19293,   inf] (16), [-0.19126,   inf] (16), [-0.19063,   inf] (16), [-0.18796,   inf] (16), [-0.17452,   inf] (16), [-0.17384,   inf] (16), [-0.17152,   inf] (16), [-0.17115,   inf] (16), [-0.17107,   inf] (16), [-0.16917,   inf] (16), [-0.16815,   inf] (16), [-0.16758,   inf] (16), [-0.16746,   inf] (16), [-0.16613,   inf] (16), [-0.16555,   inf] (16), [-0.16517,   inf] (16), 
length of domains: 171
Total time: 0.3994	 pickout: 0.0087	 decision: 0.0391	 get_bound: 0.3417	 add_domain: 0.0098
Current lb:-0.19918392598628998
692 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.342686176300049

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([171, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([171, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1364] [1, 1354] [1, 1354] [1, 1364] [1, 1354] [1, 1354] [1, 1354] [1, 1354] [1, 1113] [1, 1113] 
regular batch size: 2*171, diving batch size 1*0
best_l after optimization: 25.060928344726562 with beta sum per layer: [0.0, 50.25897216796875, 37.943519592285156]
alpha/beta optimization time: 0.3082613945007324
This batch time : update_bounds func: 0.3813	 prepare: 0.0364	 bound: 0.3086	 transfer: 0.0144	 finalize: 0.0211
Accumulated time: update_bounds func: 1.3792	 prepare: 0.0977	 bound: 1.1769	 transfer: 0.0144	 finalize: 0.0613
batch bounding time:  0.3817296028137207
Current worst splitting domains [lb, ub] (depth):
[-0.19630,   inf] (18), [-0.19466,   inf] (18), [-0.19275,   inf] (18), [-0.19274,   inf] (18), [-0.19111,   inf] (18), [-0.18991,   inf] (18), [-0.18946,   inf] (18), [-0.18918,   inf] (18), [-0.18779,   inf] (18), [-0.18755,   inf] (18), [-0.18680,   inf] (18), [-0.18634,   inf] (18), [-0.18471,   inf] (18), [-0.18423,   inf] (18), [-0.18183,   inf] (18), [-0.18140,   inf] (18), [-0.17116,   inf] (18), [-0.17059,   inf] (18), [-0.16885,   inf] (18), [-0.16836,   inf] (18), 
length of domains: 321
Total time: 0.4975	 pickout: 0.0265	 decision: 0.0661	 get_bound: 0.3823	 add_domain: 0.0226
Current lb:-0.19629959762096405
1034 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.843479633331299

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([321, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([321, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1354] [1, 1354] [1, 1364] [1, 1364] [1, 1354] [1, 1364] [1, 1354] [1, 1364] [1, 1364] [1, 1364] 
regular batch size: 2*321, diving batch size 1*0
best_l after optimization: 46.338993072509766 with beta sum per layer: [0.266849160194397, 112.28779602050781, 65.29679870605469]
alpha/beta optimization time: 0.43020105361938477
This batch time : update_bounds func: 0.6156	 prepare: 0.0621	 bound: 0.4306	 transfer: 0.0276	 finalize: 0.0938
Accumulated time: update_bounds func: 1.9948	 prepare: 0.1598	 bound: 1.6074	 transfer: 0.0276	 finalize: 0.1550
batch bounding time:  0.6162929534912109
Current worst splitting domains [lb, ub] (depth):
[-0.19355,   inf] (20), [-0.19192,   inf] (20), [-0.19012,   inf] (20), [-0.19003,   inf] (20), [-0.18838,   inf] (20), [-0.18837,   inf] (20), [-0.18813,   inf] (20), [-0.18764,   inf] (20), [-0.18726,   inf] (20), [-0.18675,   inf] (20), [-0.18659,   inf] (20), [-0.18593,   inf] (20), [-0.18526,   inf] (20), [-0.18508,   inf] (20), [-0.18495,   inf] (20), [-0.18455,   inf] (20), [-0.18397,   inf] (20), [-0.18369,   inf] (20), [-0.18342,   inf] (20), [-0.18296,   inf] (20), 
length of domains: 609
Total time: 0.8249	 pickout: 0.0507	 decision: 0.1174	 get_bound: 0.6173	 add_domain: 0.0396
Current lb:-0.19355469942092896
1676 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.673711061477661

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([609, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([609, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1995] [1, 1995] [1, 1995] [1, 1995] [1, 1995] [1, 1995] [1, 1995] [1, 1995] [1, 1995] [1, 1995] 
regular batch size: 2*609, diving batch size 1*0
best_l after optimization: 85.99720764160156 with beta sum per layer: [1.769763469696045, 250.27316284179688, 117.4872055053711]
alpha/beta optimization time: 0.6557877063751221
This batch time : update_bounds func: 0.9612	 prepare: 0.1143	 bound: 0.6561	 transfer: 0.0536	 finalize: 0.1343
Accumulated time: update_bounds func: 2.9560	 prepare: 0.2741	 bound: 2.2636	 transfer: 0.0536	 finalize: 0.2894
batch bounding time:  0.9626271724700928
Current worst splitting domains [lb, ub] (depth):
[-0.19069,   inf] (22), [-0.18998,   inf] (22), [-0.18922,   inf] (22), [-0.18838,   inf] (22), [-0.18718,   inf] (22), [-0.18682,   inf] (22), [-0.18651,   inf] (22), [-0.18643,   inf] (22), [-0.18568,   inf] (22), [-0.18552,   inf] (22), [-0.18483,   inf] (22), [-0.18481,   inf] (22), [-0.18477,   inf] (22), [-0.18466,   inf] (22), [-0.18446,   inf] (22), [-0.18418,   inf] (22), [-0.18408,   inf] (22), [-0.18405,   inf] (22), [-0.18353,   inf] (22), [-0.18330,   inf] (22), 
length of domains: 1162
Total time: 1.3467	 pickout: 0.1000	 decision: 0.2010	 get_bound: 0.9647	 add_domain: 0.0810
Current lb:-0.190688818693161
2894 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.030315399169922

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1162, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([1162, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 1356] [1, 1356] [1, 1356] [1, 1356] [1, 1356] [1, 1356] [1, 1356] [1, 1356] [1, 1356] [1, 1356] 
regular batch size: 2*1162, diving batch size 1*0
best_l after optimization: 157.9516143798828 with beta sum per layer: [7.285304069519043, 644.0419921875, 218.5292205810547]
alpha/beta optimization time: 1.1331372261047363
This batch time : update_bounds func: 1.5842	 prepare: 0.2209	 bound: 1.1336	 transfer: 0.0790	 finalize: 0.1453
Accumulated time: update_bounds func: 4.5402	 prepare: 0.4950	 bound: 3.3972	 transfer: 0.0790	 finalize: 0.4347/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/utils.py:462: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  eps_temp = torch.tensor(preprocess_cifar(eps_temp, perturbation=True)).reshape(1,-1,1,1)

batch bounding time:  1.586515188217163
Current worst splitting domains [lb, ub] (depth):
[-0.18768,   inf] (24), [-0.18700,   inf] (24), [-0.18687,   inf] (24), [-0.18643,   inf] (24), [-0.18602,   inf] (24), [-0.18573,   inf] (24), [-0.18507,   inf] (24), [-0.18488,   inf] (24), [-0.18416,   inf] (24), [-0.18384,   inf] (24), [-0.18361,   inf] (24), [-0.18348,   inf] (24), [-0.18331,   inf] (24), [-0.18298,   inf] (24), [-0.18288,   inf] (24), [-0.18257,   inf] (24), [-0.18250,   inf] (24), [-0.18248,   inf] (24), [-0.18235,   inf] (24), [-0.18219,   inf] (24), 
length of domains: 2177
Total time: 2.5585	 pickout: 0.2152	 decision: 0.4568	 get_bound: 1.5901	 add_domain: 0.2964
Current lb:-0.18767781555652618
5218 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.611514568328857

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2177, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([2177, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [0, 921] [0, 921] [0, 921] [0, 921] [0, 921] [0, 921] [0, 921] [0, 921] [0, 921] [0, 921] 
regular batch size: 2*2177, diving batch size 1*0
best_l after optimization: 290.6255187988281 with beta sum per layer: [22.832202911376953, 1552.0213623046875, 395.73406982421875]
alpha/beta optimization time: 2.0208380222320557
This batch time : update_bounds func: 3.0235	 prepare: 0.4269	 bound: 2.0213	 transfer: 0.1649	 finalize: 0.4006
Accumulated time: update_bounds func: 7.5636	 prepare: 0.9219	 bound: 5.4185	 transfer: 0.1649	 finalize: 0.8353
batch bounding time:  3.027869462966919
Current worst splitting domains [lb, ub] (depth):
[-0.18529,   inf] (26), [-0.18463,   inf] (26), [-0.18449,   inf] (26), [-0.18405,   inf] (26), [-0.18365,   inf] (26), [-0.18333,   inf] (26), [-0.18318,   inf] (26), [-0.18270,   inf] (26), [-0.18251,   inf] (26), [-0.18251,   inf] (26), [-0.18237,   inf] (26), [-0.18193,   inf] (26), [-0.18178,   inf] (26), [-0.18153,   inf] (26), [-0.18148,   inf] (26), [-0.18123,   inf] (26), [-0.18122,   inf] (26), [-0.18111,   inf] (26), [-0.18094,   inf] (26), [-0.18063,   inf] (26), 
length of domains: 4077
Total time: 4.6821	 pickout: 0.4036	 decision: 0.8448	 get_bound: 3.0352	 add_domain: 0.3986
Current lb:-0.1852949857711792
9572 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 14.331421375274658

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([4077, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([4077, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 31] [1, 31] [1, 31] [1, 31] [1, 31] [1, 31] [1, 31] [1, 31] [1, 31] [1, 31] 
regular batch size: 2*4077, diving batch size 1*0
best_l after optimization: 537.20458984375 with beta sum per layer: [53.38639450073242, 3643.81298828125, 730.0950927734375]
alpha/beta optimization time: 3.6703591346740723
This batch time : update_bounds func: 5.5598	 prepare: 0.7965	 bound: 3.6709	 transfer: 0.3184	 finalize: 0.7555
Accumulated time: update_bounds func: 13.1235	 prepare: 1.7184	 bound: 9.0894	 transfer: 0.3184	 finalize: 1.5907
batch bounding time:  5.567810773849487
Current worst splitting domains [lb, ub] (depth):
[-0.18270,   inf] (28), [-0.18217,   inf] (28), [-0.18204,   inf] (28), [-0.18190,   inf] (28), [-0.18151,   inf] (28), [-0.18146,   inf] (28), [-0.18136,   inf] (28), [-0.18106,   inf] (28), [-0.18093,   inf] (28), [-0.18074,   inf] (28), [-0.18056,   inf] (28), [-0.18052,   inf] (28), [-0.18021,   inf] (28), [-0.18011,   inf] (28), [-0.18003,   inf] (28), [-0.17991,   inf] (28), [-0.17990,   inf] (28), [-0.17975,   inf] (28), [-0.17958,   inf] (28), [-0.17938,   inf] (28), 
length of domains: 7734
Total time: 8.8298	 pickout: 0.8099	 decision: 1.6921	 get_bound: 5.5815	 add_domain: 0.7463
Current lb:-0.18270060420036316
17726 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Time out!!!!!!!!
Image 194 label 4 verification end, final lower bound -0.18270060420036316, upper bound inf, time: 23.384344339370728
194 -0.18270060420036316
Result: image 194 verification failure (with branch and bound).
Wall time: 33.26853942871094

number of correctly classified examples: 1
incorrectly classified idx (total 0): []
attack success idx (total 0): []
verification success idx (total 0): []
verification failure idx (total 1): [194]
final verified acc: 0.0%[1]
verifier is called on 1 examples.
total verified: 0
mean time [cnt:1] (excluding attack success): 33.21228837966919
