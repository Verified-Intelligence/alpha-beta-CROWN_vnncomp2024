Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  sparse_alpha: true
  save_adv_example: false
  precompile_jit: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
  csv_name: cifar10_resnet_instances.csv
  results_file: null
  root_path: ../../vnncomp2021/benchmarks/cifar10_resnet
model:
  path: null
  cache_onnx_conversion: false
  onnx_quirks: null
  name: mnist_9_200
  onnx_path: null
  onnx_path_prefix: ''
  onnx_optimization_flags: none
data:
  start: 62
  end: 63
  select_instance: null
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: null
  vnnlib_path: null
  vnnlib_path_prefix: ''
solver:
  batch_size: 2000
  no_float64_last_iter: true
  no_amp: false
  early_stop_patience: 10
  start_save_best: 2
  bound_prop_method: alpha-crown
  prune_after_crown: false
  crown:
    batch_size: 1000000000
    max_crown_size: 1000000000
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
    full_conv_alpha: true
  beta-crown:
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.01
    lr_decay: 0.98
    optimizer: adam
    iteration: 50
    beta: true
    beta_warmup: true
    enable_opt_interm_bounds: false
    all_node_split_LP: false
  forward:
    refine: false
    dynamic: false
    max_dim: 10000
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  multi_class:
    multi_class_method: allclass_domain
    label_batch_size: 32
    skip_with_refined_bound: true
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
    adv_warmup: true
bab:
  initial_max_domains: 1
  max_domains: 200000
  decision_thresh: 0
  timeout: 360
  timeout_scale: 1
  override_timeout: null
  get_upper_bound: false
  dfs_percent: 0.0
  pruning_in_iteration: true
  pruning_in_iteration_ratio: 0.2
  sort_targets: false
  batched_domain_list: true
  optimized_intermediate_layers: ''
  interm_transfer: true
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1.0
    iteration: 100
    bab_iteration: -1
    early_stop_patience: -1
    lr_beta: 0.02
    number_cuts: 50
    topk_cuts_in_filter: 100
    batch_size_primal: 100
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    max_num: 1000000000
    fixed_cuts: false
    patches_cut: false
    cplex_cuts: false
    cplex_cuts_wait: 0
    cplex_cuts_revpickup: true
    cut_reference_bounds: true
    fix_intermediate_bounds: false
    lr: 0.01
  branching:
    method: kfsb
    candidates: 3
    reduceop: max
    sb_coeff_thresh: 0.001
    input_split:
      enable: false
      enhanced_bound_prop_method: alpha-crown
      enhanced_branching_method: naive
      enhanced_bound_patience: 100000000.0
      attack_patience: 100000000.0
      adv_check: 0
      sort_domain_interval: -1
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
  pgd_loss_mode: null
  attack_mode: PGD
  gama_lambda: 10.0
  gama_decay: 0.9
  check_clean: false
  input_split:
    pgd_steps: 100
    pgd_restarts: 30
    pgd_alpha: auto
  input_split_enhanced:
    pgd_steps: 200
    pgd_restarts: 5000000
    pgd_alpha: auto
  input_split_check_adv:
    pgd_steps: 5
    pgd_restarts: 5
    pgd_alpha: auto
  enable_mip_attack: false
  cex_path: ./test_cex.txt
debug:
  lp_test: null

Experiments at Tue Aug 23 12:05:40 2022 on diablo.cs.ucla.edu
saving results to a-b-crown_[cifar10_resnet_instances]_start=62_end=63_iter=50_b=2000_timeout=360_branching=kfsb-max-3_lra-init=0.1_lra=0.01_lrb=0.01_PGD=skip_cplex_cuts=False_initial_max_domains=1.npz
customized start/end sample from 62 to 63

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 62 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using onnx onnx/resnet_4b.onnx
Using vnnlib vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_14_eps_0.004.vnnlib
Precompiled vnnlib file found at ../../vnncomp2021/benchmarks/cifar10_resnet/vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_14_eps_0.004.vnnlib.compiled
Loading onnx ../../vnncomp2021/benchmarks/cifar10_resnet/onnx/resnet_4b.onnx wih quirks {}
ConvertModel(
  (Conv_27): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (Relu_28): ReLU(inplace=True)
  (Conv_29): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (Relu_30): ReLU(inplace=True)
  (Conv_31): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Conv_32): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))
  (Add_33): Add()
  (Relu_34): ReLU(inplace=True)
  (Conv_35): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_36): ReLU(inplace=True)
  (Conv_37): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Add_38): Add()
  (Relu_39): ReLU(inplace=True)
  (Conv_40): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (Relu_41): ReLU(inplace=True)
  (Conv_42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Conv_43): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2))
  (Add_44): Add()
  (Relu_45): ReLU(inplace=True)
  (Conv_46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_47): ReLU(inplace=True)
  (Conv_48): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Add_49): Add()
  (Relu_50): ReLU(inplace=True)
  (Flatten_51): Flatten()
  (Gemm_52): Linear(in_features=512, out_features=100, bias=True)
  (Relu_53): ReLU(inplace=True)
  (Gemm_54): Linear(in_features=100, out_features=10, bias=True)
)
Model prediction is: tensor([[ 1.13689613, -1.69910789, -0.77371281,  0.36231560, -1.27489316,
         -0.52829909, -1.23018956, -0.75381315,  5.67982817, -0.81278414]],
       device='cuda:0')
layer /54 using sparse-features alpha with shape [445]; unstable size 445; total size 4096 (torch.Size([1, 16, 16, 16]))
layer /54 start_node /input.4 using sparse-spec alpha with unstable size 196 total_size 2048 output_shape (32, 8, 8)
layer /54 start_node /59 using sparse-spec alpha with unstable size 189 total_size 2048 output_shape (32, 8, 8)
layer /54 start_node /input.12 using sparse-spec alpha with unstable size 242 total_size 2048 output_shape (32, 8, 8)
layer /54 start_node /64 using sparse-spec alpha with unstable size 385 total_size 2048 output_shape (32, 8, 8)
layer /54 start_node /input.20 using sparse-spec alpha with unstable size 106 total_size 512 output_shape torch.Size([32, 4, 4])
layer /54 start_node /70 using sparse-spec alpha with unstable size 144 total_size 512 output_shape torch.Size([32, 4, 4])
layer /54 start_node /input.28 using sparse-spec alpha with unstable size 152 total_size 512 output_shape torch.Size([32, 4, 4])
layer /54 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape torch.Size([32, 4, 4])
layer /54 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /54 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /56 using sparse-features alpha with shape [196]; unstable size 196; total size 2048 (torch.Size([1, 32, 8, 8]))
layer /56 start_node /59 using sparse-spec alpha with unstable size 189 total_size 2048 output_shape (32, 8, 8)
layer /56 start_node /input.12 using sparse-spec alpha with unstable size 242 total_size 2048 output_shape (32, 8, 8)
layer /56 start_node /64 using sparse-spec alpha with unstable size 385 total_size 2048 output_shape (32, 8, 8)
layer /56 start_node /input.20 using sparse-spec alpha with unstable size 106 total_size 512 output_shape torch.Size([32, 4, 4])
layer /56 start_node /70 using sparse-spec alpha with unstable size 144 total_size 512 output_shape torch.Size([32, 4, 4])
layer /56 start_node /input.28 using sparse-spec alpha with unstable size 152 total_size 512 output_shape torch.Size([32, 4, 4])
layer /56 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape torch.Size([32, 4, 4])
layer /56 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /56 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /input.8 using sparse-features alpha with shape [189]; unstable size 189; total size 2048 (torch.Size([1, 32, 8, 8]))
layer /input.8 start_node /input.12 using sparse-spec alpha with unstable size 242 total_size 2048 output_shape (32, 8, 8)
layer /input.8 start_node /64 using sparse-spec alpha with unstable size 385 total_size 2048 output_shape (32, 8, 8)
layer /input.8 start_node /input.20 using sparse-spec alpha with unstable size 106 total_size 512 output_shape (32, 4, 4)
layer /input.8 start_node /70 using sparse-spec alpha with unstable size 144 total_size 512 output_shape torch.Size([32, 4, 4])
layer /input.8 start_node /input.28 using sparse-spec alpha with unstable size 152 total_size 512 output_shape torch.Size([32, 4, 4])
layer /input.8 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape torch.Size([32, 4, 4])
layer /input.8 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /input.8 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /62 using sparse-features alpha with shape [242]; unstable size 242; total size 2048 (torch.Size([1, 32, 8, 8]))
layer /62 start_node /64 using sparse-spec alpha with unstable size 385 total_size 2048 output_shape (32, 8, 8)
layer /62 start_node /input.20 using sparse-spec alpha with unstable size 106 total_size 512 output_shape (32, 4, 4)
layer /62 start_node /70 using sparse-spec alpha with unstable size 144 total_size 512 output_shape torch.Size([32, 4, 4])
layer /62 start_node /input.28 using sparse-spec alpha with unstable size 152 total_size 512 output_shape torch.Size([32, 4, 4])
layer /62 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape torch.Size([32, 4, 4])
layer /62 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /62 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /input.16 using sparse-features alpha with shape [385]; unstable size 385; total size 2048 (torch.Size([1, 32, 8, 8]))
layer /input.16 start_node /input.20 using sparse-spec alpha with unstable size 106 total_size 512 output_shape (32, 4, 4)
layer /input.16 start_node /70 using sparse-spec alpha with unstable size 144 total_size 512 output_shape (32, 4, 4)
layer /input.16 start_node /input.28 using sparse-spec alpha with unstable size 152 total_size 512 output_shape torch.Size([32, 4, 4])
layer /input.16 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape torch.Size([32, 4, 4])
layer /input.16 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /input.16 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /67 using sparse-features alpha with shape [106]; unstable size 106; total size 512 (torch.Size([1, 32, 4, 4]))
layer /67 start_node /70 using sparse-spec alpha with unstable size 144 total_size 512 output_shape (32, 4, 4)
layer /67 start_node /input.28 using sparse-spec alpha with unstable size 152 total_size 512 output_shape torch.Size([32, 4, 4])
layer /67 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape torch.Size([32, 4, 4])
layer /67 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /67 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /input.24 using sparse-features alpha with shape [144]; unstable size 144; total size 512 (torch.Size([1, 32, 4, 4]))
layer /input.24 start_node /input.28 using sparse-spec alpha with unstable size 152 total_size 512 output_shape (32, 4, 4)
layer /input.24 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape torch.Size([32, 4, 4])
layer /input.24 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /input.24 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /73 using sparse-features alpha with shape [152]; unstable size 152; total size 512 (torch.Size([1, 32, 4, 4]))
layer /73 start_node /75 using sparse-spec alpha with unstable size 267 total_size 512 output_shape (32, 4, 4)
layer /73 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /73 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /76 using sparse-features alpha with shape [267]; unstable size 267; total size 512 (torch.Size([1, 32, 4, 4]))
layer /76 start_node /input.32 using sparse-spec alpha with unstable size 79 total_size 100 output_shape torch.Size([100])
layer /76 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /79 using sparse-features alpha with shape [79]; unstable size 79; total size 100 (torch.Size([1, 100]))
layer /79 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
Optimizable variables initialized.
initial CROWN bounds: tensor([[-2.10904527, -0.41920114, -3.07417774, -2.59466600, -2.29754019,
         -2.28992414, -1.32546949, -3.86168671, -2.47135091]], device='cuda:0') None
best_l after optimization: 0.46550047397613525 with beta sum per layer: []
alpha/beta optimization time: 36.28210186958313
initial alpha-CROWN bounds: tensor([[-0.23666668,  1.51479375, -0.30381060, -0.43293047,  0.08094025,
          0.11561728,  1.05317211, -1.09978247, -0.22583270]], device='cuda:0')
Worst class: (+ rhs) -1.0997824668884277
Total VNNLIB file length: 9, max property batch size: 1, total number of batches: 9
lA shape: [torch.Size([1, 9, 16, 16, 16]), torch.Size([1, 9, 32, 8, 8]), torch.Size([1, 9, 32, 8, 8]), torch.Size([1, 9, 32, 8, 8]), torch.Size([1, 9, 32, 8, 8]), torch.Size([1, 9, 32, 4, 4]), torch.Size([1, 9, 32, 4, 4]), torch.Size([1, 9, 32, 4, 4]), torch.Size([1, 9, 32, 4, 4]), torch.Size([1, 9, 100])]

Properties batch 0, size 1
Remaining timeout: 257.4099509716034
##### [0] Spec matrix: [[[-1.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]], thresh: [0.] ######
Remaining spec index [0] with bounds tensor([[-0.23666668]], device='cuda:0') need to verify.
Model prediction is: tensor([ 1.13689613, -1.69910789, -0.77371281,  0.36231560, -1.27489316,
        -0.52829909, -1.23018956, -0.75381315,  5.67982817, -0.81278414],
       device='cuda:0')
build_the_model_with_refined_bounds batch [0/1]
setting alpha for layer /54 start_node /80 with alignment adjustment
setting alpha for layer /56 start_node /80 with alignment adjustment
setting alpha for layer /input.8 start_node /80 with alignment adjustment
setting alpha for layer /62 start_node /80 with alignment adjustment
setting alpha for layer /input.16 start_node /80 with alignment adjustment
setting alpha for layer /67 start_node /80 with alignment adjustment
setting alpha for layer /input.24 start_node /80 with alignment adjustment
setting alpha for layer /73 start_node /80 with alignment adjustment
setting alpha for layer /76 start_node /80 with alignment adjustment
setting alpha for layer /79 start_node /80 with alignment adjustment
all slope initialized
directly get lb and ub from refined bounds
lA shapes: [torch.Size([1, 1, 16, 16, 16]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 100])]
c shape: torch.Size([1, 1, 10])
alpha-CROWN with fixed intermediate bounds: tensor([[-0.23666668]], device='cuda:0') tensor([[inf]], device='cuda:0')
Keeping slopes for these layers: ['/80']
Keeping slopes for these layers: ['/80']
layer 0 size torch.Size([4096]) unstable 445
layer 1 size torch.Size([2048]) unstable 194
layer 2 size torch.Size([2048]) unstable 187
layer 3 size torch.Size([2048]) unstable 239
layer 4 size torch.Size([2048]) unstable 367
layer 5 size torch.Size([512]) unstable 101
layer 6 size torch.Size([512]) unstable 132
layer 7 size torch.Size([512]) unstable 143
layer 8 size torch.Size([512]) unstable 237
layer 9 size torch.Size([100]) unstable 72
-----------------
# of unstable neurons: 2117
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 16, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [9, 36] 
split level 1: [9, 8] 
split level 2: [9, 51] 
split level 3: [9, 38] 
split level 4: [9, 82] 
split level 5: [9, 89] 
split level 6: [9, 81] 
regular batch size: 2*64, diving batch size 1*0
(128, 3, 32, 32) torch.Size([128, 1, 10]) torch.Size([128, 1])

all verified at 0th iter
pruning_in_iteration open status: False
ratio of positive domain = 128 / 128 = 1.0
pruning-in-iteration extra time: 0.0001125335693359375
Tensors transferred: pre=3.5244M lA=1.7622M alpha=0.5383M beta=0.0009M
This batch time : update_bounds func: 0.0776	 prepare: 0.0211	 bound: 0.0264	 transfer: 0.0222	 finalize: 0.0077
Accumulated time: update_bounds func: 0.0776	 prepare: 0.0211	 bound: 0.0264	 transfer: 0.0222	 finalize: 0.0077
batch bounding time:  0.07770156860351562
length of domains: 0
Total time: 0.6313	 pickout: 0.0023	 decision: 0.5189	 get_bound: 0.1076	 add_domain: 0.0025
Accumulated time:	 pickout: 0.0023	 decision: 0.5189	 get_bound: 0.1076	 add_domain: 0.0025
No domains left, verification finished!
128 domains visited
Cumulative time: 0.7429459095001221


Properties batch 1, size 1
Remaining timeout: 256.24639868736267
##### [0] Spec matrix: [[[ 0. -1.  0.  0.  0.  0.  0.  0.  1.  0.]]], thresh: [0.] ######
Init opt crown verified for spec index [0] with bound tensor([[1.51479375]], device='cuda:0').

Properties batch 2, size 1
Remaining timeout: 256.19836616516113
##### [0] Spec matrix: [[[ 0.  0. -1.  0.  0.  0.  0.  0.  1.  0.]]], thresh: [0.] ######
Remaining spec index [0] with bounds tensor([[-0.30381060]], device='cuda:0') need to verify.
Model prediction is: tensor([ 1.13689613, -1.69910789, -0.77371281,  0.36231560, -1.27489316,
        -0.52829909, -1.23018956, -0.75381315,  5.67982817, -0.81278414],
       device='cuda:0')
build_the_model_with_refined_bounds batch [0/1]
setting alpha for layer /54 start_node /80 with alignment adjustment
setting alpha for layer /56 start_node /80 with alignment adjustment
setting alpha for layer /input.8 start_node /80 with alignment adjustment
setting alpha for layer /62 start_node /80 with alignment adjustment
setting alpha for layer /input.16 start_node /80 with alignment adjustment
setting alpha for layer /67 start_node /80 with alignment adjustment
setting alpha for layer /input.24 start_node /80 with alignment adjustment
setting alpha for layer /73 start_node /80 with alignment adjustment
setting alpha for layer /76 start_node /80 with alignment adjustment
setting alpha for layer /79 start_node /80 with alignment adjustment
all slope initialized
directly get lb and ub from refined bounds
lA shapes: [torch.Size([1, 1, 16, 16, 16]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 100])]
c shape: torch.Size([1, 1, 10])
alpha-CROWN with fixed intermediate bounds: tensor([[-0.30381060]], device='cuda:0') tensor([[inf]], device='cuda:0')
Keeping slopes for these layers: ['/80']
Keeping slopes for these layers: ['/80']
layer 0 size torch.Size([4096]) unstable 445
layer 1 size torch.Size([2048]) unstable 194
layer 2 size torch.Size([2048]) unstable 187
layer 3 size torch.Size([2048]) unstable 239
layer 4 size torch.Size([2048]) unstable 367
layer 5 size torch.Size([512]) unstable 101
layer 6 size torch.Size([512]) unstable 132
layer 7 size torch.Size([512]) unstable 143
layer 8 size torch.Size([512]) unstable 237
layer 9 size torch.Size([100]) unstable 72
-----------------
# of unstable neurons: 2117
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 16, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [9, 76] 
split level 1: [9, 14] 
split level 2: [9, 96] 
split level 3: [9, 74] 
split level 4: [9, 31] 
split level 5: [9, 82] 
split level 6: [9, 51] 
regular batch size: 2*64, diving batch size 1*0
(128, 3, 32, 32) torch.Size([128, 1, 10]) torch.Size([128, 1])

all verified at 0th iter
pruning_in_iteration open status: False
ratio of positive domain = 128 / 128 = 1.0
pruning-in-iteration extra time: 0.00010848045349121094
Tensors transferred: pre=3.5244M lA=1.7622M alpha=0.5383M beta=0.0009M
This batch time : update_bounds func: 0.0614	 prepare: 0.0211	 bound: 0.0241	 transfer: 0.0084	 finalize: 0.0075
Accumulated time: update_bounds func: 0.1391	 prepare: 0.0422	 bound: 0.0505	 transfer: 0.0306	 finalize: 0.0152
batch bounding time:  0.061502695083618164
length of domains: 0
Total time: 0.2127	 pickout: 0.0021	 decision: 0.1169	 get_bound: 0.0913	 add_domain: 0.0023
Accumulated time:	 pickout: 0.0021	 decision: 0.1169	 get_bound: 0.0913	 add_domain: 0.0023
No domains left, verification finished!
128 domains visited
Cumulative time: 0.22321438789367676


Properties batch 3, size 1
Remaining timeout: 255.63713192939758
##### [0] Spec matrix: [[[ 0.  0.  0. -1.  0.  0.  0.  0.  1.  0.]]], thresh: [0.] ######
Remaining spec index [0] with bounds tensor([[-0.43293047]], device='cuda:0') need to verify.
Model prediction is: tensor([ 1.13689613, -1.69910789, -0.77371281,  0.36231560, -1.27489316,
        -0.52829909, -1.23018956, -0.75381315,  5.67982817, -0.81278414],
       device='cuda:0')
build_the_model_with_refined_bounds batch [0/1]
setting alpha for layer /54 start_node /80 with alignment adjustment
setting alpha for layer /56 start_node /80 with alignment adjustment
setting alpha for layer /input.8 start_node /80 with alignment adjustment
setting alpha for layer /62 start_node /80 with alignment adjustment
setting alpha for layer /input.16 start_node /80 with alignment adjustment
setting alpha for layer /67 start_node /80 with alignment adjustment
setting alpha for layer /input.24 start_node /80 with alignment adjustment
setting alpha for layer /73 start_node /80 with alignment adjustment
setting alpha for layer /76 start_node /80 with alignment adjustment
setting alpha for layer /79 start_node /80 with alignment adjustment
all slope initialized
directly get lb and ub from refined bounds
lA shapes: [torch.Size([1, 1, 16, 16, 16]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 100])]
c shape: torch.Size([1, 1, 10])
alpha-CROWN with fixed intermediate bounds: tensor([[-0.43293047]], device='cuda:0') tensor([[inf]], device='cuda:0')
Keeping slopes for these layers: ['/80']
Keeping slopes for these layers: ['/80']
layer 0 size torch.Size([4096]) unstable 445
layer 1 size torch.Size([2048]) unstable 194
layer 2 size torch.Size([2048]) unstable 187
layer 3 size torch.Size([2048]) unstable 239
layer 4 size torch.Size([2048]) unstable 367
layer 5 size torch.Size([512]) unstable 101
layer 6 size torch.Size([512]) unstable 132
layer 7 size torch.Size([512]) unstable 143
layer 8 size torch.Size([512]) unstable 237
layer 9 size torch.Size([100]) unstable 72
-----------------
# of unstable neurons: 2117
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 16, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [9, 74] 
split level 1: [9, 47] 
split level 2: [9, 51] 
split level 3: [9, 59] 
split level 4: [9, 20] 
split level 5: [9, 96] 
split level 6: [9, 38] 
regular batch size: 2*64, diving batch size 1*0
(128, 3, 32, 32) torch.Size([128, 1, 10]) torch.Size([128, 1])

all verified at 0th iter
pruning_in_iteration open status: False
ratio of positive domain = 128 / 128 = 1.0
pruning-in-iteration extra time: 0.00011277198791503906
Tensors transferred: pre=3.5244M lA=1.7622M alpha=0.5383M beta=0.0009M
This batch time : update_bounds func: 0.0602	 prepare: 0.0209	 bound: 0.0231	 transfer: 0.0084	 finalize: 0.0075
Accumulated time: update_bounds func: 0.1992	 prepare: 0.0631	 bound: 0.0736	 transfer: 0.0390	 finalize: 0.0227
batch bounding time:  0.060221195220947266
length of domains: 0
Total time: 0.2055	 pickout: 0.0021	 decision: 0.1112	 get_bound: 0.0898	 add_domain: 0.0024
Accumulated time:	 pickout: 0.0021	 decision: 0.1112	 get_bound: 0.0898	 add_domain: 0.0024
No domains left, verification finished!
128 domains visited
Cumulative time: 0.21616792678833008


Properties batch 4, size 1
Remaining timeout: 255.07745504379272
##### [0] Spec matrix: [[[ 0.  0.  0.  0. -1.  0.  0.  0.  1.  0.]]], thresh: [0.] ######
Init opt crown verified for spec index [0] with bound tensor([[0.08094025]], device='cuda:0').

Properties batch 5, size 1
Remaining timeout: 255.0261082649231
##### [0] Spec matrix: [[[ 0.  0.  0.  0.  0. -1.  0.  0.  1.  0.]]], thresh: [0.] ######
Init opt crown verified for spec index [0] with bound tensor([[0.11561728]], device='cuda:0').

Properties batch 6, size 1
Remaining timeout: 254.9828224182129
##### [0] Spec matrix: [[[ 0.  0.  0.  0.  0.  0. -1.  0.  1.  0.]]], thresh: [0.] ######
Init opt crown verified for spec index [0] with bound tensor([[1.05317211]], device='cuda:0').

Properties batch 7, size 1
Remaining timeout: 254.938640832901
##### [0] Spec matrix: [[[ 0.  0.  0.  0.  0.  0.  0. -1.  1.  0.]]], thresh: [0.] ######
Remaining spec index [0] with bounds tensor([[-1.09978247]], device='cuda:0') need to verify.
Model prediction is: tensor([ 1.13689613, -1.69910789, -0.77371281,  0.36231560, -1.27489316,
        -0.52829909, -1.23018956, -0.75381315,  5.67982817, -0.81278414],
       device='cuda:0')
build_the_model_with_refined_bounds batch [0/1]
setting alpha for layer /54 start_node /80 with alignment adjustment
setting alpha for layer /56 start_node /80 with alignment adjustment
setting alpha for layer /input.8 start_node /80 with alignment adjustment
setting alpha for layer /62 start_node /80 with alignment adjustment
setting alpha for layer /input.16 start_node /80 with alignment adjustment
setting alpha for layer /67 start_node /80 with alignment adjustment
setting alpha for layer /input.24 start_node /80 with alignment adjustment
setting alpha for layer /73 start_node /80 with alignment adjustment
setting alpha for layer /76 start_node /80 with alignment adjustment
setting alpha for layer /79 start_node /80 with alignment adjustment
all slope initialized
directly get lb and ub from refined bounds
lA shapes: [torch.Size([1, 1, 16, 16, 16]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 100])]
c shape: torch.Size([1, 1, 10])
alpha-CROWN with fixed intermediate bounds: tensor([[-1.09978247]], device='cuda:0') tensor([[inf]], device='cuda:0')
Keeping slopes for these layers: ['/80']
Keeping slopes for these layers: ['/80']
layer 0 size torch.Size([4096]) unstable 445
layer 1 size torch.Size([2048]) unstable 194
layer 2 size torch.Size([2048]) unstable 187
layer 3 size torch.Size([2048]) unstable 239
layer 4 size torch.Size([2048]) unstable 367
layer 5 size torch.Size([512]) unstable 101
layer 6 size torch.Size([512]) unstable 132
layer 7 size torch.Size([512]) unstable 143
layer 8 size torch.Size([512]) unstable 237
layer 9 size torch.Size([100]) unstable 72
-----------------
# of unstable neurons: 2117
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 16, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [9, 26] 
split level 1: [9, 31] 
split level 2: [9, 76] 
split level 3: [9, 66] 
split level 4: [9, 74] 
split level 5: [9, 97] 
split level 6: [9, 88] 
regular batch size: 2*64, diving batch size 1*0
(128, 3, 32, 32) torch.Size([128, 1, 10]) torch.Size([128, 1])

all verified at 0th iter
pruning_in_iteration open status: False
ratio of positive domain = 128 / 128 = 1.0
pruning-in-iteration extra time: 0.00011301040649414062
Tensors transferred: pre=3.5244M lA=1.7622M alpha=0.5383M beta=0.0009M
This batch time : update_bounds func: 0.0682	 prepare: 0.0208	 bound: 0.0313	 transfer: 0.0083	 finalize: 0.0075
Accumulated time: update_bounds func: 0.2674	 prepare: 0.0839	 bound: 0.1049	 transfer: 0.0473	 finalize: 0.0302
batch bounding time:  0.0682220458984375
length of domains: 0
Total time: 0.2141	 pickout: 0.0021	 decision: 0.1118	 get_bound: 0.0978	 add_domain: 0.0023
Accumulated time:	 pickout: 0.0021	 decision: 0.1118	 get_bound: 0.0978	 add_domain: 0.0023
No domains left, verification finished!
128 domains visited
Cumulative time: 0.22461795806884766


Properties batch 8, size 1
Remaining timeout: 254.37680578231812
##### [0] Spec matrix: [[[ 0.  0.  0.  0.  0.  0.  0.  0.  1. -1.]]], thresh: [0.] ######
Remaining spec index [0] with bounds tensor([[-0.22583270]], device='cuda:0') need to verify.
Model prediction is: tensor([ 1.13689613, -1.69910789, -0.77371281,  0.36231560, -1.27489316,
        -0.52829909, -1.23018956, -0.75381315,  5.67982817, -0.81278414],
       device='cuda:0')
build_the_model_with_refined_bounds batch [0/1]
setting alpha for layer /54 start_node /80 with alignment adjustment
setting alpha for layer /56 start_node /80 with alignment adjustment
setting alpha for layer /input.8 start_node /80 with alignment adjustment
setting alpha for layer /62 start_node /80 with alignment adjustment
setting alpha for layer /input.16 start_node /80 with alignment adjustment
setting alpha for layer /67 start_node /80 with alignment adjustment
setting alpha for layer /input.24 start_node /80 with alignment adjustment
setting alpha for layer /73 start_node /80 with alignment adjustment
setting alpha for layer /76 start_node /80 with alignment adjustment
setting alpha for layer /79 start_node /80 with alignment adjustment
all slope initialized
directly get lb and ub from refined bounds
lA shapes: [torch.Size([1, 1, 16, 16, 16]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 8, 8]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 32, 4, 4]), torch.Size([1, 1, 100])]
c shape: torch.Size([1, 1, 10])
alpha-CROWN with fixed intermediate bounds: tensor([[-0.22583270]], device='cuda:0') tensor([[inf]], device='cuda:0')
Keeping slopes for these layers: ['/80']
Keeping slopes for these layers: ['/80']
layer 0 size torch.Size([4096]) unstable 445
layer 1 size torch.Size([2048]) unstable 194
layer 2 size torch.Size([2048]) unstable 187
layer 3 size torch.Size([2048]) unstable 239
layer 4 size torch.Size([2048]) unstable 367
layer 5 size torch.Size([512]) unstable 101
layer 6 size torch.Size([512]) unstable 132
layer 7 size torch.Size([512]) unstable 143
layer 8 size torch.Size([512]) unstable 237
layer 9 size torch.Size([100]) unstable 72
-----------------
# of unstable neurons: 2117
-----------------

remaining dive domains: 0/-1, dive_rate:0.0/home/zhouxingshi/onnx2pytorch/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)
  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.
  "Using experimental implementation that allows 'batch_size > 1'."
/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/abcrown.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_new.cpp:210.)
  arguments.Config["bab"]["decision_thresh"] = torch.tensor([item[1] for item in vnnlib[1]]).to(data)
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/utils.py:21: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return value.ndim == 0 or value.shape == torch.Size([1])
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/operations/add.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  elif all(x == 1 for x in input[0].shape):
/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/batch_branch_and_bound.py:420: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(arguments.Config["bab"]["decision_thresh"] + 1e-7), np.inf

batch:  torch.Size([1, 16, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 16, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [9, 76] 
split level 1: [9, 48] 
split level 2: [9, 85] 
split level 3: [9, 75] 
split level 4: [9, 22] 
split level 5: [9, 69] 
split level 6: [9, 38] 
regular batch size: 2*64, diving batch size 1*0
(128, 3, 32, 32) torch.Size([128, 1, 10]) torch.Size([128, 1])

all verified at 0th iter
pruning_in_iteration open status: False
ratio of positive domain = 128 / 128 = 1.0
pruning-in-iteration extra time: 0.00011110305786132812
Tensors transferred: pre=3.5244M lA=1.7622M alpha=0.5383M beta=0.0009M
This batch time : update_bounds func: 0.0630	 prepare: 0.0210	 bound: 0.0259	 transfer: 0.0083	 finalize: 0.0075
Accumulated time: update_bounds func: 0.3304	 prepare: 0.1049	 bound: 0.1308	 transfer: 0.0556	 finalize: 0.0376
batch bounding time:  0.06304287910461426
length of domains: 0
Total time: 0.2089	 pickout: 0.0021	 decision: 0.1114	 get_bound: 0.0930	 add_domain: 0.0023
Accumulated time:	 pickout: 0.0021	 decision: 0.1114	 get_bound: 0.0930	 add_domain: 0.0023
No domains left, verification finished!
128 domains visited
Cumulative time: 0.2195420265197754

Result: safe in 46.1893 seconds
############# Summary #############
Final verified acc: 100.0% [total 1 examples]
Total verification count: 1 , total verified safe: 1 , verified unsafe: 0 , timeout: 0
mean time (bab) [total:1]: 3.598503351211548
mean time [1] 46.1892626285553 max time 46.1892626285553
safe (total 1): [0]
