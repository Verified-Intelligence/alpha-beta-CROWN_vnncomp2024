Building native CUDA modules...
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
  csv_name: cifar10_resnet_instances.csv
  onnx_path: null
  vnnlib_path: null
  results_file: null
  root_path: ../../../../../vnncomp2021/benchmarks/cifar10_resnet
model:
  path: null
  name: mnist_9_200
data:
  start: 61
  end: 62
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR
  data_filter_path: null
specification:
  type: lp
  norm: .inf
  epsilon: null
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 2000
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.01
    lr_decay: 0.98
    optimizer: adam
    iteration: 50
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 360
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: max
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 22:00:33 2022 on ubuntu
saving results to vnn-comp_[cifar10_resnet_instances]_start=61_end=62_iter=50_b=2000_timeout=360_branching=kfsb-max-3_lra-init=0.1_lra=0.01_lrb=0.01_PGD=skip.npz
customized start/end sample from 61 to 62

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Model prediction is: tensor([[ 4.6421, -2.6848,  1.0328, -2.7882, -0.4161, -2.7647, -2.5141, -3.0126,
          7.2837,  1.2901]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[-4.4581,  2.3544, -2.2993,  2.4651, -1.0741,  1.9626,  1.5149,  0.5298,
         -1.3363]], device='cuda:0') None
best_l after optimization: -14.86508846282959 with beta sum per layer: []
alpha/beta optimization time: 39.777342319488525
initial alpha-CROWN bounds: tensor([[-2.9512,  3.9495, -0.4533,  4.0280,  0.7130,  3.6588,  3.1446,  2.5424,
          0.2332]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-2.9512, device='cuda:0', grad_fn=<MinBackward1>)
##### [0] True label: 8, Tested against: 0, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
Model prediction is: tensor([[ 4.6421, -2.6848,  1.0328, -2.7882, -0.4161, -2.7647, -2.5141, -3.0126,
          7.2837,  1.2901]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /28 start_node /29
setting alpha for layer /28 start_node /33
setting alpha for layer /28 start_node /35
setting alpha for layer /28 start_node /38
setting alpha for layer /28 start_node /40
setting alpha for layer /28 start_node /44
setting alpha for layer /28 start_node /46
setting alpha for layer /28 start_node /49
setting alpha for layer /28 start_node /52
not setting layer /28 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 16, 16, 16]) != torch.Size([2, 9, 1, 16, 16, 16]))
setting alpha for layer /30 start_node /33
setting alpha for layer /30 start_node /35
setting alpha for layer /30 start_node /38
setting alpha for layer /30 start_node /40
setting alpha for layer /30 start_node /44
setting alpha for layer /30 start_node /46
setting alpha for layer /30 start_node /49
setting alpha for layer /30 start_node /52
not setting layer /30 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /34 start_node /35
setting alpha for layer /34 start_node /38
setting alpha for layer /34 start_node /40
setting alpha for layer /34 start_node /44
setting alpha for layer /34 start_node /46
setting alpha for layer /34 start_node /49
setting alpha for layer /34 start_node /52
not setting layer /34 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /36 start_node /38
setting alpha for layer /36 start_node /40
setting alpha for layer /36 start_node /44
setting alpha for layer /36 start_node /46
setting alpha for layer /36 start_node /49
setting alpha for layer /36 start_node /52
not setting layer /36 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /39 start_node /40
setting alpha for layer /39 start_node /44
setting alpha for layer /39 start_node /46
setting alpha for layer /39 start_node /49
setting alpha for layer /39 start_node /52
not setting layer /39 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /41 start_node /44
setting alpha for layer /41 start_node /46
setting alpha for layer /41 start_node /49
setting alpha for layer /41 start_node /52
not setting layer /41 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
setting alpha for layer /45 start_node /46
setting alpha for layer /45 start_node /49
setting alpha for layer /45 start_node /52
not setting layer /45 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
setting alpha for layer /47 start_node /49
setting alpha for layer /47 start_node /52
not setting layer /47 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
setting alpha for layer /50 start_node /52
not setting layer /50 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
not setting layer /53 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /27 torch.Size([1, 16, 16, 16])
1 /29 torch.Size([1, 32, 8, 8])
2 /33 torch.Size([1, 32, 8, 8])
3 /35 torch.Size([1, 32, 8, 8])
4 /38 torch.Size([1, 32, 8, 8])
5 /40 torch.Size([1, 32, 4, 4])
6 /44 torch.Size([1, 32, 4, 4])
7 /46 torch.Size([1, 32, 4, 4])
8 /49 torch.Size([1, 32, 4, 4])
9 /52 torch.Size([1, 100])
best_l after optimization: 2.950845718383789 with beta sum per layer: []
alpha/beta optimization time: 4.040818214416504
alpha-CROWN with fixed intermediate bounds: tensor([[-2.9508]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-2.950845718383789
layer 0 size torch.Size([4096]) unstable 490
layer 1 size torch.Size([2048]) unstable 242
layer 2 size torch.Size([2048]) unstable 204
layer 3 size torch.Size([2048]) unstable 225
layer 4 size torch.Size([2048]) unstable 359
layer 5 size torch.Size([512]) unstable 90
layer 6 size torch.Size([512]) unstable 141
layer 7 size torch.Size([512]) unstable 133
layer 8 size torch.Size([512]) unstable 239
layer 9 size torch.Size([100]) unstable 63
-----------------
# of unstable neurons: 2186
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 16, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [9, 41] 
split level 1: [9, 8] 
split level 2: [9, 36] 
split level 3: [9, 71] 
split level 4: [9, 39] 
split level 5: [9, 9] 
split level 6: [9, 51] 
regular batch size: 2*64, diving batch size 1*0
best_l after optimization: 35.578590393066406 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 76.83450317382812]
alpha/beta optimization time: 1.7631936073303223
This batch time : update_bounds func: 1.8369	 prepare: 0.0259	 bound: 1.7638	 transfer: 0.0233	 finalize: 0.0235
Accumulated time: update_bounds func: 1.8369	 prepare: 0.0259	 bound: 1.7638	 transfer: 0.0233	 finalize: 0.0235
batch bounding time:  1.837712049484253
Current worst splitting domains [lb, ub] (depth):
[-1.77166,   inf] (8), [-1.47815,   inf] (8), [-1.45460,   inf] (8), [-1.44738,   inf] (8), [-1.43501,   inf] (8), [-1.33266,   inf] (8), [-1.24403,   inf] (8), [-1.20937,   inf] (8), [-1.20132,   inf] (8), [-1.19901,   inf] (8), [-1.18703,   inf] (8), [-1.17514,   inf] (8), [-1.07158,   inf] (8), [-1.05732,   inf] (8), [-1.04603,   inf] (8), [-1.01035,   inf] (8), [-1.00070,   inf] (8), [-0.99440,   inf] (8), [-0.99165,   inf] (8), [-0.97512,   inf] (8), 
length of domains: 70
Total time: 1.9799	 pickout: 0.0019	 decision: 0.1031	 get_bound: 1.8682	 add_domain: 0.0066
Current lb:-1.7716639041900635
128 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.135166883468628

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([70, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([70, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 38] [9, 38] [9, 38] [9, 38] [9, 38] [9, 38] [9, 38] [9, 38] [9, 38] [9, 38] 
regular batch size: 2*70, diving batch size 1*0
best_l after optimization: 36.70905303955078 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 136.65460205078125]
alpha/beta optimization time: 1.7788853645324707
This batch time : update_bounds func: 1.8638	 prepare: 0.0328	 bound: 1.7795	 transfer: 0.0266	 finalize: 0.0244
Accumulated time: update_bounds func: 3.7007	 prepare: 0.0587	 bound: 3.5433	 transfer: 0.0266	 finalize: 0.0479
batch bounding time:  1.864436149597168
Current worst splitting domains [lb, ub] (depth):
[-1.68684,   inf] (10), [-1.36587,   inf] (10), [-1.35197,   inf] (10), [-1.33727,   inf] (10), [-1.32501,   inf] (10), [-1.23169,   inf] (10), [-1.21396,   inf] (10), [-1.14657,   inf] (10), [-1.10304,   inf] (10), [-1.10216,   inf] (10), [-1.09668,   inf] (10), [-1.08275,   inf] (10), [-1.06239,   inf] (10), [-0.97428,   inf] (10), [-0.96660,   inf] (10), [-0.93999,   inf] (10), [-0.93388,   inf] (10), [-0.91178,   inf] (10), [-0.90440,   inf] (10), [-0.90342,   inf] (10), 
length of domains: 71
Total time: 1.9843	 pickout: 0.0311	 decision: 0.0816	 get_bound: 1.8647	 add_domain: 0.0070
Current lb:-1.6868407726287842
268 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 9.125060081481934

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([71, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([71, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 89] [9, 89] [9, 89] [9, 89] [9, 89] [9, 89] [9, 89] [9, 89] [9, 89] [9, 89] 
regular batch size: 2*71, diving batch size 1*0
best_l after optimization: 48.57071304321289 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 176.57174682617188]
alpha/beta optimization time: 1.767169713973999
This batch time : update_bounds func: 1.8417	 prepare: 0.0333	 bound: 1.7678	 transfer: 0.0149	 finalize: 0.0254
Accumulated time: update_bounds func: 5.5424	 prepare: 0.0920	 bound: 5.3111	 transfer: 0.0149	 finalize: 0.0733
batch bounding time:  1.8423023223876953
Current worst splitting domains [lb, ub] (depth):
[-1.57615,   inf] (12), [-1.30112,   inf] (12), [-1.26220,   inf] (12), [-1.24414,   inf] (12), [-1.22932,   inf] (12), [-1.21432,   inf] (12), [-1.08735,   inf] (12), [-1.04312,   inf] (12), [-0.99855,   inf] (12), [-0.99744,   inf] (12), [-0.99538,   inf] (12), [-0.98541,   inf] (12), [-0.97920,   inf] (12), [-0.96267,   inf] (12), [-0.95791,   inf] (12), [-0.93758,   inf] (12), [-0.92674,   inf] (12), [-0.89613,   inf] (12), [-0.87185,   inf] (12), [-0.84831,   inf] (12), 
length of domains: 97
Total time: 1.9635	 pickout: 0.0304	 decision: 0.0816	 get_bound: 1.8426	 add_domain: 0.0090
Current lb:-1.576145052909851
410 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 11.093578815460205

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([97, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([97, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 15] [9, 15] [9, 15] [9, 15] [9, 15] [9, 15] [9, 15] [9, 15] [9, 15] [9, 15] 
regular batch size: 2*97, diving batch size 1*0
best_l after optimization: 62.441551208496094 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 282.909912109375]
alpha/beta optimization time: 1.8158535957336426
This batch time : update_bounds func: 1.9150	 prepare: 0.0446	 bound: 1.8165	 transfer: 0.0193	 finalize: 0.0335
Accumulated time: update_bounds func: 7.4574	 prepare: 0.1366	 bound: 7.1276	 transfer: 0.0193	 finalize: 0.1068
batch bounding time:  1.915724277496338
Current worst splitting domains [lb, ub] (depth):
[-1.46523,   inf] (14), [-1.34668,   inf] (14), [-1.14704,   inf] (14), [-1.13465,   inf] (14), [-1.12208,   inf] (14), [-1.10916,   inf] (14), [-1.09697,   inf] (14), [-1.04608,   inf] (14), [-1.00602,   inf] (14), [-1.00147,   inf] (14), [-0.99670,   inf] (14), [-0.97722,   inf] (14), [-0.96574,   inf] (14), [-0.92410,   inf] (14), [-0.88668,   inf] (14), [-0.88255,   inf] (14), [-0.87967,   inf] (14), [-0.85823,   inf] (14), [-0.84011,   inf] (14), [-0.83314,   inf] (14), 
length of domains: 146
Total time: 2.0668	 pickout: 0.0415	 decision: 0.0962	 get_bound: 1.9161	 add_domain: 0.0130
Current lb:-1.4652289152145386
604 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 13.167389392852783

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([146, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([146, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 34] [9, 34] [9, 34] [9, 34] [8, 270] [9, 34] [9, 34] [9, 34] [9, 34] [9, 34] 
regular batch size: 2*146, diving batch size 1*0
best_l after optimization: 83.40274047851562 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.709144949913025, 465.5718994140625]
alpha/beta optimization time: 1.9325766563415527
This batch time : update_bounds func: 2.1466	 prepare: 0.0680	 bound: 1.9332	 transfer: 0.0422	 finalize: 0.1025
Accumulated time: update_bounds func: 9.6040	 prepare: 0.2046	 bound: 9.0608	 transfer: 0.0422	 finalize: 0.2094
batch bounding time:  2.1475090980529785
Current worst splitting domains [lb, ub] (depth):
[-1.37029,   inf] (16), [-1.25475,   inf] (16), [-1.24274,   inf] (16), [-1.16802,   inf] (16), [-1.04537,   inf] (16), [-1.03475,   inf] (16), [-1.03260,   inf] (16), [-1.01128,   inf] (16), [-0.99948,   inf] (16), [-0.96537,   inf] (16), [-0.94147,   inf] (16), [-0.93243,   inf] (16), [-0.91424,   inf] (16), [-0.90491,   inf] (16), [-0.89543,   inf] (16), [-0.89308,   inf] (16), [-0.88122,   inf] (16), [-0.87495,   inf] (16), [-0.85824,   inf] (16), [-0.84905,   inf] (16), 
length of domains: 214
Total time: 2.3496	 pickout: 0.0611	 decision: 0.1211	 get_bound: 2.1480	 add_domain: 0.0193
Current lb:-1.3702850341796875
896 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 15.526376724243164

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([214, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([214, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 270] [8, 270] [8, 318] [9, 74] [9, 74] [8, 270] [9, 34] [9, 74] [8, 314] [9, 74] 
regular batch size: 2*214, diving batch size 1*0
best_l after optimization: 114.83195495605469 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.43550968170166, 673.8807373046875]
alpha/beta optimization time: 2.158181667327881
This batch time : update_bounds func: 2.3895	 prepare: 0.0979	 bound: 2.1589	 transfer: 0.0566	 finalize: 0.0751
Accumulated time: update_bounds func: 11.9936	 prepare: 0.3024	 bound: 11.2197	 transfer: 0.0566	 finalize: 0.2845
batch bounding time:  2.3905434608459473
Current worst splitting domains [lb, ub] (depth):
[-1.30784,   inf] (18), [-1.18614,   inf] (18), [-1.18412,   inf] (18), [-1.12171,   inf] (18), [-1.04592,   inf] (18), [-1.01141,   inf] (18), [-1.00676,   inf] (18), [-1.00181,   inf] (18), [-0.96953,   inf] (18), [-0.93473,   inf] (18), [-0.92264,   inf] (18), [-0.91530,   inf] (18), [-0.91436,   inf] (18), [-0.89286,   inf] (18), [-0.87706,   inf] (18), [-0.87585,   inf] (18), [-0.86707,   inf] (18), [-0.84524,   inf] (18), [-0.82964,   inf] (18), [-0.82373,   inf] (18), 
length of domains: 330
Total time: 2.6656	 pickout: 0.0865	 decision: 0.1582	 get_bound: 2.3913	 add_domain: 0.0297
Current lb:-1.3078432083129883
1324 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 18.20488691329956

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([330, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([330, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 318] [9, 74] [9, 74] [8, 318] [8, 318] [8, 318] [9, 74] [9, 74] [9, 74] [9, 24] 
regular batch size: 2*330, diving batch size 1*0
best_l after optimization: 138.3958740234375 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 47.745567321777344, 990.2879638671875]
alpha/beta optimization time: 2.5985493659973145
This batch time : update_bounds func: 2.9598	 prepare: 0.1474	 bound: 2.5993	 transfer: 0.0942	 finalize: 0.1174
Accumulated time: update_bounds func: 14.9534	 prepare: 0.4498	 bound: 13.8190	 transfer: 0.0942	 finalize: 0.4018
batch bounding time:  2.9610862731933594
Current worst splitting domains [lb, ub] (depth):
[-1.24738,   inf] (20), [-1.08163,   inf] (20), [-1.07126,   inf] (20), [-1.06997,   inf] (20), [-1.05564,   inf] (20), [-1.03734,   inf] (20), [-1.02228,   inf] (20), [-0.98905,   inf] (20), [-0.95646,   inf] (20), [-0.89249,   inf] (20), [-0.88446,   inf] (20), [-0.87088,   inf] (20), [-0.86365,   inf] (20), [-0.86174,   inf] (20), [-0.85941,   inf] (20), [-0.84970,   inf] (20), [-0.84603,   inf] (20), [-0.83690,   inf] (20), [-0.83433,   inf] (20), [-0.82369,   inf] (20), 
length of domains: 479
Total time: 3.4184	 pickout: 0.1368	 decision: 0.2745	 get_bound: 2.9623	 add_domain: 0.0447
Current lb:-1.247382402420044
1984 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 21.642432928085327

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([479, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([479, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 74] [9, 74] [9, 76] [8, 318] [9, 74] [9, 76] [9, 76] [9, 76] [8, 270] [8, 318] 
regular batch size: 2*479, diving batch size 1*0
best_l after optimization: 140.93954467773438 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 116.83721160888672, 1328.5267333984375]
alpha/beta optimization time: 3.2003159523010254
This batch time : update_bounds func: 3.7229	 prepare: 0.2174	 bound: 3.2011	 transfer: 0.1289	 finalize: 0.1729
Accumulated time: update_bounds func: 18.6763	 prepare: 0.6673	 bound: 17.0200	 transfer: 0.1289	 finalize: 0.5747
batch bounding time:  3.7258028984069824
Current worst splitting domains [lb, ub] (depth):
[-1.13387,   inf] (22), [-1.10473,   inf] (22), [-1.01929,   inf] (22), [-1.01323,   inf] (22), [-0.98789,   inf] (22), [-0.97066,   inf] (22), [-0.94693,   inf] (22), [-0.93842,   inf] (22), [-0.93665,   inf] (22), [-0.93035,   inf] (22), [-0.90138,   inf] (22), [-0.87902,   inf] (22), [-0.87488,   inf] (22), [-0.83118,   inf] (22), [-0.81880,   inf] (22), [-0.81072,   inf] (22), [-0.80633,   inf] (22), [-0.80171,   inf] (22), [-0.80083,   inf] (22), [-0.77981,   inf] (22), 
length of domains: 598
Total time: 4.4214	 pickout: 0.1966	 decision: 0.3561	 get_bound: 3.7275	 add_domain: 0.1412
Current lb:-1.1338720321655273
2942 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 26.094439029693604

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([598, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([598, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 314] [8, 314] [9, 81] [9, 76] [8, 318] [9, 65] [8, 314] [8, 314] [9, 81] [8, 314] 
regular batch size: 2*598, diving batch size 1*0
best_l after optimization: 134.1844024658203 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 203.1171875, 1609.32470703125]
alpha/beta optimization time: 3.677401065826416
This batch time : update_bounds func: 4.4229	 prepare: 0.2716	 bound: 3.6781	 transfer: 0.1900	 finalize: 0.2800
Accumulated time: update_bounds func: 23.0992	 prepare: 0.9388	 bound: 20.6982	 transfer: 0.1900	 finalize: 0.8547
batch bounding time:  4.424652814865112
Current worst splitting domains [lb, ub] (depth):
[-1.09152,   inf] (24), [-1.06044,   inf] (24), [-0.98104,   inf] (24), [-0.96282,   inf] (24), [-0.93136,   inf] (24), [-0.91183,   inf] (24), [-0.90540,   inf] (24), [-0.89880,   inf] (24), [-0.88623,   inf] (24), [-0.87866,   inf] (24), [-0.84813,   inf] (24), [-0.83073,   inf] (24), [-0.81833,   inf] (24), [-0.81230,   inf] (24), [-0.77894,   inf] (24), [-0.77176,   inf] (24), [-0.76728,   inf] (24), [-0.76021,   inf] (24), [-0.75698,   inf] (24), [-0.75292,   inf] (24), 
length of domains: 692
Total time: 5.1329	 pickout: 0.2701	 decision: 0.3669	 get_bound: 4.4270	 add_domain: 0.0689
Current lb:-1.0915155410766602
4138 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 31.26753330230713

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([692, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([692, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 76] [9, 76] [9, 65] [9, 65] [9, 65] [9, 81] [9, 76] [9, 65] [9, 76] [9, 76] 
regular batch size: 2*692, diving batch size 1*0
best_l after optimization: 118.37522888183594 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 275.43609619140625, 1799.30859375]
alpha/beta optimization time: 4.071818113327026
This batch time : update_bounds func: 4.9165	 prepare: 0.3202	 bound: 4.0726	 transfer: 0.1947	 finalize: 0.3255
Accumulated time: update_bounds func: 28.0157	 prepare: 1.2590	 bound: 24.7707	 transfer: 0.1947	 finalize: 1.1802
batch bounding time:  4.918622970581055
Current worst splitting domains [lb, ub] (depth):
[-1.04222,   inf] (26), [-1.01315,   inf] (26), [-0.92293,   inf] (26), [-0.90273,   inf] (26), [-0.87335,   inf] (26), [-0.86989,   inf] (26), [-0.85372,   inf] (26), [-0.83826,   inf] (26), [-0.83637,   inf] (26), [-0.82985,   inf] (26), [-0.78743,   inf] (26), [-0.78262,   inf] (26), [-0.78141,   inf] (26), [-0.73765,   inf] (26), [-0.71954,   inf] (26), [-0.71709,   inf] (26), [-0.71260,   inf] (26), [-0.71070,   inf] (26), [-0.70981,   inf] (26), [-0.70230,   inf] (26), 
length of domains: 723
Total time: 5.7781	 pickout: 0.2936	 decision: 0.4888	 get_bound: 4.9214	 add_domain: 0.0743
Current lb:-1.0422223806381226
5522 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 37.09865117073059

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([723, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([723, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 81] [9, 65] [8, 314] [8, 314] [8, 270] [9, 81] [9, 65] [8, 270] [9, 65] [9, 65] 
regular batch size: 2*723, diving batch size 1*0
best_l after optimization: 94.42794036865234 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12325774878263474, 315.3293151855469, 1798.865478515625]
alpha/beta optimization time: 4.204624176025391
This batch time : update_bounds func: 5.0661	 prepare: 0.3369	 bound: 4.2055	 transfer: 0.1795	 finalize: 0.3406
Accumulated time: update_bounds func: 33.0818	 prepare: 1.5960	 bound: 28.9763	 transfer: 0.1795	 finalize: 1.5208
batch bounding time:  5.0681469440460205
Current worst splitting domains [lb, ub] (depth):
[-1.00822,   inf] (28), [-0.95382,   inf] (28), [-0.88735,   inf] (28), [-0.86409,   inf] (28), [-0.83490,   inf] (28), [-0.82299,   inf] (28), [-0.79758,   inf] (28), [-0.78894,   inf] (28), [-0.77675,   inf] (28), [-0.76977,   inf] (28), [-0.74830,   inf] (28), [-0.73751,   inf] (28), [-0.73490,   inf] (28), [-0.72557,   inf] (28), [-0.70021,   inf] (28), [-0.68358,   inf] (28), [-0.68144,   inf] (28), [-0.67228,   inf] (28), [-0.67149,   inf] (28), [-0.67091,   inf] (28), 
length of domains: 699
Total time: 5.9961	 pickout: 0.3198	 decision: 0.5307	 get_bound: 5.0709	 add_domain: 0.0747
Current lb:-1.0082168579101562
6968 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 43.15096473693848

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([699, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([699, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 65] [9, 81] [9, 11] [9, 81] [8, 314] [9, 11] [9, 81] [9, 11] [9, 81] [9, 81] 
regular batch size: 2*699, diving batch size 1*0
best_l after optimization: 81.735595703125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3113600015640259, 0.0, 329.01348876953125, 1682.98876953125]
alpha/beta optimization time: 4.115095376968384
This batch time : update_bounds func: 4.9284	 prepare: 0.3282	 bound: 4.1159	 transfer: 0.1282	 finalize: 0.3523
Accumulated time: update_bounds func: 38.0102	 prepare: 1.9242	 bound: 33.0922	 transfer: 0.1282	 finalize: 1.8732
batch bounding time:  4.930520057678223
Current worst splitting domains [lb, ub] (depth):
[-0.95118,   inf] (30), [-0.91859,   inf] (30), [-0.85122,   inf] (30), [-0.83013,   inf] (30), [-0.79791,   inf] (30), [-0.78720,   inf] (30), [-0.76172,   inf] (30), [-0.75312,   inf] (30), [-0.74079,   inf] (30), [-0.73296,   inf] (30), [-0.73155,   inf] (30), [-0.71256,   inf] (30), [-0.68614,   inf] (30), [-0.67271,   inf] (30), [-0.65661,   inf] (30), [-0.64774,   inf] (30), [-0.64125,   inf] (30), [-0.63519,   inf] (30), [-0.63341,   inf] (30), [-0.63233,   inf] (30), 
length of domains: 711
Total time: 5.8211	 pickout: 0.3064	 decision: 0.5017	 get_bound: 4.9335	 add_domain: 0.0795
Current lb:-0.9511814117431641
8366 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 49.026711225509644

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([711, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([711, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 11] [9, 11] [8, 270] [9, 11] [9, 11] [8, 314] [9, 11] [9, 24] [9, 11] [9, 11] 
regular batch size: 2*711, diving batch size 1*0
best_l after optimization: 75.02468872070312 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22004875540733337, 340.4781494140625, 1636.352294921875]
alpha/beta optimization time: 4.151780843734741
This batch time : update_bounds func: 4.9280	 prepare: 0.3354	 bound: 4.1526	 transfer: 0.0940	 finalize: 0.3425
Accumulated time: update_bounds func: 42.9383	 prepare: 2.2596	 bound: 37.2448	 transfer: 0.0940	 finalize: 2.2157
batch bounding time:  4.930333375930786
Current worst splitting domains [lb, ub] (depth):
[-0.91611,   inf] (32), [-0.88593,   inf] (32), [-0.80641,   inf] (32), [-0.79429,   inf] (32), [-0.76266,   inf] (32), [-0.75475,   inf] (32), [-0.72427,   inf] (32), [-0.70622,   inf] (32), [-0.69946,   inf] (32), [-0.69290,   inf] (32), [-0.65098,   inf] (32), [-0.63746,   inf] (32), [-0.63270,   inf] (32), [-0.63007,   inf] (32), [-0.62662,   inf] (32), [-0.62087,   inf] (32), [-0.60882,   inf] (32), [-0.60476,   inf] (32), [-0.59377,   inf] (32), [-0.59138,   inf] (32), 
length of domains: 681
Total time: 5.8234	 pickout: 0.3103	 decision: 0.5026	 get_bound: 4.9335	 add_domain: 0.0770
Current lb:-0.9161098003387451
9788 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 54.90694737434387

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([681, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([681, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 24] [9, 24] [9, 24] [9, 24] [9, 24] [9, 24] [9, 24] [9, 24] [9, 55] [9, 24] 
regular batch size: 2*681, diving batch size 1*0
best_l after optimization: 90.36358642578125 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.698000431060791, 2.3788235187530518, 358.72503662109375, 1507.22265625]
alpha/beta optimization time: 4.03356409072876
This batch time : update_bounds func: 4.7698	 prepare: 0.3198	 bound: 4.0344	 transfer: 0.0749	 finalize: 0.3373
Accumulated time: update_bounds func: 47.7080	 prepare: 2.5794	 bound: 41.2791	 transfer: 0.0749	 finalize: 2.5529
batch bounding time:  4.77181077003479
Current worst splitting domains [lb, ub] (depth):
[-0.79527,   inf] (34), [-0.78331,   inf] (34), [-0.75885,   inf] (34), [-0.75817,   inf] (34), [-0.67782,   inf] (34), [-0.67771,   inf] (34), [-0.67377,   inf] (34), [-0.66404,   inf] (34), [-0.63815,   inf] (34), [-0.63025,   inf] (34), [-0.62944,   inf] (34), [-0.62096,   inf] (34), [-0.61874,   inf] (34), [-0.60748,   inf] (34), [-0.60053,   inf] (34), [-0.59517,   inf] (34), [-0.59396,   inf] (34), [-0.58463,   inf] (34), [-0.57449,   inf] (34), [-0.56422,   inf] (34), 
length of domains: 743
Total time: 5.6373	 pickout: 0.2972	 decision: 0.4809	 get_bound: 4.7748	 add_domain: 0.0844
Current lb:-0.7952656745910645
11150 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 60.595123052597046

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([743, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([743, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 55] [9, 55] [8, 271] [8, 271] [9, 55] [9, 55] [9, 55] [9, 55] [8, 271] [8, 271] 
regular batch size: 2*743, diving batch size 1*0
best_l after optimization: 88.90531158447266 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.99615478515625, 9.080528259277344, 447.2461242675781, 1525.9998779296875]
alpha/beta optimization time: 4.304121255874634
This batch time : update_bounds func: 5.1130	 prepare: 0.3504	 bound: 4.3049	 transfer: 0.0962	 finalize: 0.3575
Accumulated time: update_bounds func: 52.8211	 prepare: 2.9298	 bound: 45.5840	 transfer: 0.0962	 finalize: 2.9104
batch bounding time:  5.115402936935425
Current worst splitting domains [lb, ub] (depth):
[-0.72887,   inf] (36), [-0.72816,   inf] (36), [-0.70818,   inf] (36), [-0.69723,   inf] (36), [-0.65380,   inf] (36), [-0.64512,   inf] (36), [-0.60733,   inf] (36), [-0.59967,   inf] (36), [-0.59184,   inf] (36), [-0.59059,   inf] (36), [-0.59006,   inf] (36), [-0.58629,   inf] (36), [-0.57565,   inf] (36), [-0.54105,   inf] (36), [-0.53709,   inf] (36), [-0.53416,   inf] (36), [-0.53243,   inf] (36), [-0.52989,   inf] (36), [-0.52936,   inf] (36), [-0.52366,   inf] (36), 
length of domains: 797
Total time: 6.0693	 pickout: 0.3233	 decision: 0.5322	 get_bound: 5.1187	 add_domain: 0.0951
Current lb:-0.7288651466369629
12636 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 66.72132968902588

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([797, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([797, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 55] [9, 55] [8, 234] [8, 234] [8, 234] [8, 345] [6, 270] [6, 270] [6, 270] [8, 234] 
regular batch size: 2*797, diving batch size 1*0
best_l after optimization: 78.28004455566406 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.027792930603027, 16.607284545898438, 511.650390625, 1560.228759765625]
alpha/beta optimization time: 4.525552272796631
This batch time : update_bounds func: 5.4658	 prepare: 0.3891	 bound: 4.5264	 transfer: 0.1480	 finalize: 0.3977
Accumulated time: update_bounds func: 58.2869	 prepare: 3.3189	 bound: 50.1104	 transfer: 0.1480	 finalize: 3.3081
batch bounding time:  5.468252182006836
Current worst splitting domains [lb, ub] (depth):
[-0.66668,   inf] (38), [-0.65725,   inf] (38), [-0.64261,   inf] (38), [-0.64148,   inf] (38), [-0.61782,   inf] (38), [-0.60817,   inf] (38), [-0.58306,   inf] (38), [-0.58006,   inf] (38), [-0.57446,   inf] (38), [-0.57312,   inf] (38), [-0.56867,   inf] (38), [-0.54689,   inf] (38), [-0.54637,   inf] (38), [-0.54410,   inf] (38), [-0.54225,   inf] (38), [-0.50956,   inf] (38), [-0.50215,   inf] (38), [-0.50100,   inf] (38), [-0.50084,   inf] (38), [-0.50018,   inf] (38), 
length of domains: 832
Total time: 6.4934	 pickout: 0.3545	 decision: 0.5664	 get_bound: 5.4716	 add_domain: 0.1009
Current lb:-0.666677713394165
14230 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 73.27330017089844

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([832, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([832, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 6] [9, 6] [9, 6] [6, 270] [8, 234] [8, 345] [9, 55] [6, 270] [9, 55] [6, 270] 
regular batch size: 2*832, diving batch size 1*0
best_l after optimization: 53.34670639038086 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.90255069732666, 14.03117561340332, 532.0634765625, 1594.727294921875]
alpha/beta optimization time: 4.613903284072876
This batch time : update_bounds func: 5.6206	 prepare: 0.3931	 bound: 4.6147	 transfer: 0.2246	 finalize: 0.3833
Accumulated time: update_bounds func: 63.9075	 prepare: 3.7119	 bound: 54.7251	 transfer: 0.2246	 finalize: 3.6914
batch bounding time:  5.623314142227173
Current worst splitting domains [lb, ub] (depth):
[-0.63089,   inf] (40), [-0.62179,   inf] (40), [-0.61729,   inf] (40), [-0.60698,   inf] (40), [-0.58128,   inf] (40), [-0.57292,   inf] (40), [-0.55323,   inf] (40), [-0.54396,   inf] (40), [-0.51338,   inf] (40), [-0.51122,   inf] (40), [-0.51113,   inf] (40), [-0.51081,   inf] (40), [-0.49673,   inf] (40), [-0.48970,   inf] (40), [-0.47837,   inf] (40), [-0.47342,   inf] (40), [-0.45613,   inf] (40), [-0.45452,   inf] (40), [-0.45140,   inf] (40), [-0.44183,   inf] (40), 
length of domains: 775
Total time: 6.8017	 pickout: 0.3990	 decision: 0.5687	 get_bound: 5.6271	 add_domain: 0.2069
Current lb:-0.6308867931365967
15894 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 80.13872766494751

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([775, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([775, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 271] [8, 271] [9, 6] [9, 67] [9, 6] [9, 6] [8, 345] [9, 6] [8, 234] [9, 29] 
regular batch size: 2*775, diving batch size 1*0
best_l after optimization: 43.19295883178711 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.518259048461914, 15.945575714111328, 516.3201293945312, 1383.5650634765625]
alpha/beta optimization time: 4.409908056259155
This batch time : update_bounds func: 5.3415	 prepare: 0.3703	 bound: 4.4107	 transfer: 0.1736	 finalize: 0.3830
Accumulated time: update_bounds func: 69.2490	 prepare: 4.0822	 bound: 59.1358	 transfer: 0.1736	 finalize: 4.0743
batch bounding time:  5.343573808670044
Current worst splitting domains [lb, ub] (depth):
[-0.60141,   inf] (42), [-0.59236,   inf] (42), [-0.58249,   inf] (42), [-0.54688,   inf] (42), [-0.54129,   inf] (42), [-0.53764,   inf] (42), [-0.53528,   inf] (42), [-0.52536,   inf] (42), [-0.51037,   inf] (42), [-0.48247,   inf] (42), [-0.47917,   inf] (42), [-0.47902,   inf] (42), [-0.47267,   inf] (42), [-0.46298,   inf] (42), [-0.45574,   inf] (42), [-0.45019,   inf] (42), [-0.42927,   inf] (42), [-0.42802,   inf] (42), [-0.42076,   inf] (42), [-0.41885,   inf] (42), 
length of domains: 762
Total time: 6.3724	 pickout: 0.3510	 decision: 0.4700	 get_bound: 5.3467	 add_domain: 0.2048
Current lb:-0.6014080047607422
17444 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 86.57147192955017

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([762, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([762, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 67] [9, 29] [7, 250] [8, 271] [6, 270] [8, 271] [9, 29] [9, 6] [7, 250] [8, 345] 
regular batch size: 2*762, diving batch size 1*0
best_l after optimization: 26.121620178222656 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.09598445892334, 18.388103485107422, 504.0652160644531, 1274.5106201171875]
alpha/beta optimization time: 4.34254002571106
This batch time : update_bounds func: 5.1616	 prepare: 0.3632	 bound: 4.3434	 transfer: 0.1001	 finalize: 0.3507
Accumulated time: update_bounds func: 74.4106	 prepare: 4.4455	 bound: 63.4792	 transfer: 0.1001	 finalize: 4.4251
batch bounding time:  5.163886308670044
Current worst splitting domains [lb, ub] (depth):
[-0.55959,   inf] (44), [-0.53364,   inf] (44), [-0.53293,   inf] (44), [-0.53259,   inf] (44), [-0.51739,   inf] (44), [-0.51652,   inf] (44), [-0.50737,   inf] (44), [-0.50225,   inf] (44), [-0.50124,   inf] (44), [-0.49168,   inf] (44), [-0.46272,   inf] (44), [-0.45749,   inf] (44), [-0.45461,   inf] (44), [-0.45394,   inf] (44), [-0.44783,   inf] (44), [-0.42698,   inf] (44), [-0.40590,   inf] (44), [-0.39996,   inf] (44), [-0.39772,   inf] (44), [-0.39382,   inf] (44), 
length of domains: 705
Total time: 6.1601	 pickout: 0.3375	 decision: 0.4609	 get_bound: 5.1671	 add_domain: 0.1946
Current lb:-0.5595865249633789
18968 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 92.79046869277954

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([705, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([705, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 67] [9, 29] [9, 29] [9, 67] [7, 250] [6, 270] [9, 67] [7, 250] [9, 67] [9, 67] 
regular batch size: 2*705, diving batch size 1*0
best_l after optimization: 31.963272094726562 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.021101951599121, 17.650081634521484, 438.428955078125, 1101.18603515625]
alpha/beta optimization time: 4.107738018035889
This batch time : update_bounds func: 4.8536	 prepare: 0.3381	 bound: 4.1085	 transfer: 0.0808	 finalize: 0.3227
Accumulated time: update_bounds func: 79.2642	 prepare: 4.7836	 bound: 67.5877	 transfer: 0.0808	 finalize: 4.7477
batch bounding time:  4.8557398319244385
Current worst splitting domains [lb, ub] (depth):
[-0.50135,   inf] (46), [-0.50018,   inf] (46), [-0.49189,   inf] (46), [-0.49103,   inf] (46), [-0.49033,   inf] (46), [-0.46911,   inf] (46), [-0.46506,   inf] (46), [-0.46321,   inf] (46), [-0.45521,   inf] (46), [-0.44078,   inf] (46), [-0.43902,   inf] (46), [-0.43889,   inf] (46), [-0.43301,   inf] (46), [-0.43062,   inf] (46), [-0.43034,   inf] (46), [-0.42664,   inf] (46), [-0.42482,   inf] (46), [-0.42123,   inf] (46), [-0.41729,   inf] (46), [-0.39686,   inf] (46), 
length of domains: 675
Total time: 5.7924	 pickout: 0.3131	 decision: 0.4294	 get_bound: 4.8588	 add_domain: 0.1911
Current lb:-0.5013453960418701
20378 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 98.64506721496582

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([675, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([675, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [7, 250] [7, 250] [8, 345] [9, 29] [8, 345] [9, 29] [9, 29] [9, 29] [7, 137] [7, 137] 
regular batch size: 2*675, diving batch size 1*0
best_l after optimization: 28.6158390045166 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.528589248657227, 25.27564811706543, 389.59588623046875, 989.5595703125]
alpha/beta optimization time: 3.961771011352539
This batch time : update_bounds func: 4.7019	 prepare: 0.3267	 bound: 3.9625	 transfer: 0.0962	 finalize: 0.3128
Accumulated time: update_bounds func: 83.9661	 prepare: 5.1103	 bound: 71.5502	 transfer: 0.0962	 finalize: 5.0606
batch bounding time:  4.703927755355835
Current worst splitting domains [lb, ub] (depth):
[-0.46783,   inf] (48), [-0.46638,   inf] (48), [-0.45855,   inf] (48), [-0.45334,   inf] (48), [-0.45083,   inf] (48), [-0.43612,   inf] (48), [-0.43162,   inf] (48), [-0.43053,   inf] (48), [-0.43033,   inf] (48), [-0.42764,   inf] (48), [-0.41152,   inf] (48), [-0.40761,   inf] (48), [-0.40690,   inf] (48), [-0.40622,   inf] (48), [-0.40285,   inf] (48), [-0.39973,   inf] (48), [-0.39755,   inf] (48), [-0.39592,   inf] (48), [-0.39403,   inf] (48), [-0.39075,   inf] (48), 
length of domains: 629
Total time: 5.6084	 pickout: 0.2976	 decision: 0.4159	 get_bound: 4.7068	 add_domain: 0.1880
Current lb:-0.46782875061035156
21728 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 104.30604290962219

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([629, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([629, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [7, 250] [7, 250] [9, 67] [8, 345] [8, 345] [7, 137] [7, 137] [7, 137] [9, 12] [8, 345] 
regular batch size: 2*629, diving batch size 1*0
best_l after optimization: 38.50746154785156 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.607743740081787, 38.55573272705078, 378.98773193359375, 798.8206787109375]
alpha/beta optimization time: 3.7848527431488037
This batch time : update_bounds func: 4.4589	 prepare: 0.3023	 bound: 3.7856	 transfer: 0.0697	 finalize: 0.2982
Accumulated time: update_bounds func: 88.4250	 prepare: 5.4126	 bound: 75.3358	 transfer: 0.0697	 finalize: 5.3587
batch bounding time:  4.4608635902404785
Current worst splitting domains [lb, ub] (depth):
[-0.42827,   inf] (50), [-0.42584,   inf] (50), [-0.42093,   inf] (50), [-0.42057,   inf] (50), [-0.40409,   inf] (50), [-0.40194,   inf] (50), [-0.40140,   inf] (50), [-0.39336,   inf] (50), [-0.39214,   inf] (50), [-0.39128,   inf] (50), [-0.38926,   inf] (50), [-0.38813,   inf] (50), [-0.38641,   inf] (50), [-0.38350,   inf] (50), [-0.38201,   inf] (50), [-0.37851,   inf] (50), [-0.36232,   inf] (50), [-0.36095,   inf] (50), [-0.35692,   inf] (50), [-0.35621,   inf] (50), 
length of domains: 632
Total time: 5.2152	 pickout: 0.2791	 decision: 0.3859	 get_bound: 4.4637	 add_domain: 0.0865
Current lb:-0.4282670021057129
22986 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 109.56957077980042

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([632, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([632, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [6, 270] [6, 270] [9, 12] [9, 12] [6, 270] [7, 137] [7, 137] [8, 345] [7, 137] [8, 269] 
regular batch size: 2*632, diving batch size 1*0
best_l after optimization: 50.57003402709961 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.778140544891357, 54.03265380859375, 409.923583984375, 653.0948486328125]
alpha/beta optimization time: 3.7953295707702637
This batch time : update_bounds func: 4.4974	 prepare: 0.3124	 bound: 3.7961	 transfer: 0.0698	 finalize: 0.3160
Accumulated time: update_bounds func: 92.9224	 prepare: 5.7250	 bound: 79.1319	 transfer: 0.0698	 finalize: 5.6747
batch bounding time:  4.4994282722473145
Current worst splitting domains [lb, ub] (depth):
[-0.40660,   inf] (52), [-0.40299,   inf] (52), [-0.38024,   inf] (52), [-0.36845,   inf] (52), [-0.36662,   inf] (52), [-0.36636,   inf] (52), [-0.36518,   inf] (52), [-0.36298,   inf] (52), [-0.36293,   inf] (52), [-0.36167,   inf] (52), [-0.35647,   inf] (52), [-0.35580,   inf] (52), [-0.34604,   inf] (52), [-0.34348,   inf] (52), [-0.34006,   inf] (52), [-0.33903,   inf] (52), [-0.33852,   inf] (52), [-0.33767,   inf] (52), [-0.33683,   inf] (52), [-0.33640,   inf] (52), 
length of domains: 638
Total time: 5.3206	 pickout: 0.2795	 decision: 0.4507	 get_bound: 4.5022	 add_domain: 0.0882
Current lb:-0.4066023826599121
24250 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 114.93715643882751

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([638, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([638, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 12] [9, 12] [8, 269] [8, 269] [6, 270] [6, 270] [8, 234] [6, 270] [6, 270] [6, 270] 
regular batch size: 2*638, diving batch size 1*0
best_l after optimization: 59.56690216064453 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.798259735107422, 70.4952621459961, 419.9053955078125, 599.84619140625]
alpha/beta optimization time: 3.8503150939941406
This batch time : update_bounds func: 4.5372	 prepare: 0.3186	 bound: 3.8511	 transfer: 0.0701	 finalize: 0.2322
Accumulated time: update_bounds func: 97.4596	 prepare: 6.0436	 bound: 82.9830	 transfer: 0.0701	 finalize: 5.9069
batch bounding time:  4.539224147796631
Current worst splitting domains [lb, ub] (depth):
[-0.35215,   inf] (54), [-0.34823,   inf] (54), [-0.34793,   inf] (54), [-0.34507,   inf] (54), [-0.34451,   inf] (54), [-0.34419,   inf] (54), [-0.34217,   inf] (54), [-0.34056,   inf] (54), [-0.33990,   inf] (54), [-0.33359,   inf] (54), [-0.33326,   inf] (54), [-0.33162,   inf] (54), [-0.33105,   inf] (54), [-0.31728,   inf] (54), [-0.31607,   inf] (54), [-0.31371,   inf] (54), [-0.31261,   inf] (54), [-0.30587,   inf] (54), [-0.30549,   inf] (54), [-0.30490,   inf] (54), 
length of domains: 741
Total time: 5.4216	 pickout: 0.2859	 decision: 0.3930	 get_bound: 4.5420	 add_domain: 0.2006
Current lb:-0.3521472215652466
25526 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 120.40408611297607

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([741, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([741, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [7, 137] [8, 269] [8, 269] [7, 137] [8, 269] [7, 137] [7, 137] [8, 269] [7, 137] [8, 269] 
regular batch size: 2*741, diving batch size 1*0
best_l after optimization: 52.69191360473633 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.159749984741211, 100.0018310546875, 502.7256164550781, 687.3770751953125]
alpha/beta optimization time: 4.27827525138855
This batch time : update_bounds func: 5.2672	 prepare: 0.3678	 bound: 4.2791	 transfer: 0.1737	 finalize: 0.4428
Accumulated time: update_bounds func: 102.7268	 prepare: 6.4114	 bound: 87.2621	 transfer: 0.1737	 finalize: 6.3497
batch bounding time:  5.269609212875366
Current worst splitting domains [lb, ub] (depth):
[-0.31045,   inf] (56), [-0.30159,   inf] (56), [-0.30083,   inf] (56), [-0.29921,   inf] (56), [-0.29876,   inf] (56), [-0.29805,   inf] (56), [-0.29703,   inf] (56), [-0.29543,   inf] (56), [-0.29362,   inf] (56), [-0.29295,   inf] (56), [-0.28936,   inf] (56), [-0.28793,   inf] (56), [-0.28681,   inf] (56), [-0.28645,   inf] (56), [-0.28635,   inf] (56), [-0.28518,   inf] (56), [-0.28189,   inf] (56), [-0.28112,   inf] (56), [-0.27965,   inf] (56), [-0.27955,   inf] (56), 
length of domains: 804
Total time: 6.1827	 pickout: 0.3279	 decision: 0.4632	 get_bound: 5.2729	 add_domain: 0.1187
Current lb:-0.31044888496398926
27008 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 126.64277338981628

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([804, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([804, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 269] [8, 269] [8, 269] [7, 137] [8, 269] [8, 287] [8, 269] [8, 287] [9, 12] [7, 137] 
regular batch size: 2*804, diving batch size 1*0
best_l after optimization: 42.141578674316406 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.970019578933716, 129.76235961914062, 518.6400146484375, 732.4111328125]
alpha/beta optimization time: 4.541973352432251
This batch time : update_bounds func: 5.5602	 prepare: 0.4020	 bound: 4.5428	 transfer: 0.2274	 finalize: 0.3836
Accumulated time: update_bounds func: 108.2871	 prepare: 6.8134	 bound: 91.8048	 transfer: 0.2274	 finalize: 6.7333
batch bounding time:  5.562711000442505
Current worst splitting domains [lb, ub] (depth):
[-0.26287,   inf] (58), [-0.26060,   inf] (58), [-0.25638,   inf] (58), [-0.25467,   inf] (58), [-0.25425,   inf] (58), [-0.25413,   inf] (58), [-0.25364,   inf] (58), [-0.25200,   inf] (58), [-0.25066,   inf] (58), [-0.24981,   inf] (58), [-0.24648,   inf] (58), [-0.24459,   inf] (58), [-0.24198,   inf] (58), [-0.24163,   inf] (58), [-0.23982,   inf] (58), [-0.23935,   inf] (58), [-0.23759,   inf] (58), [-0.23670,   inf] (58), [-0.23619,   inf] (58), [-0.23612,   inf] (58), 
length of domains: 776
Total time: 6.6068	 pickout: 0.3722	 decision: 0.5535	 get_bound: 5.5664	 add_domain: 0.1147
Current lb:-0.26286616921424866
28616 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 133.30885887145996

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([776, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([776, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 287] [8, 287] [9, 12] [7, 137] [9, 12] [8, 267] [8, 267] [8, 267] [8, 267] [7, 137] 
regular batch size: 2*776, diving batch size 1*0
best_l after optimization: 33.203861236572266 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.16058258712291718, 1.4740506410598755, 134.22630310058594, 419.358154296875, 632.0408935546875]
alpha/beta optimization time: 4.408994674682617
This batch time : update_bounds func: 5.3658	 prepare: 0.3919	 bound: 4.4098	 transfer: 0.1582	 finalize: 0.4018
Accumulated time: update_bounds func: 113.6528	 prepare: 7.2053	 bound: 96.2146	 transfer: 0.1582	 finalize: 7.1351
batch bounding time:  5.368067264556885
Current worst splitting domains [lb, ub] (depth):
[-0.22664,   inf] (60), [-0.22651,   inf] (60), [-0.22513,   inf] (60), [-0.22359,   inf] (60), [-0.21981,   inf] (60), [-0.21588,   inf] (60), [-0.21494,   inf] (60), [-0.21379,   inf] (60), [-0.21088,   inf] (60), [-0.20949,   inf] (60), [-0.20810,   inf] (60), [-0.20597,   inf] (60), [-0.20472,   inf] (60), [-0.20428,   inf] (60), [-0.20408,   inf] (60), [-0.20263,   inf] (60), [-0.20253,   inf] (60), [-0.20190,   inf] (60), [-0.20187,   inf] (60), [-0.20110,   inf] (60), 
length of domains: 782
Total time: 6.3881	 pickout: 0.3479	 decision: 0.5459	 get_bound: 5.3715	 add_domain: 0.1228
Current lb:-0.22664237022399902
30168 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 139.75524759292603

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([782, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([782, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 150] [8, 150] [8, 219] [8, 150] [8, 150] [8, 267] [8, 150] [8, 150] [8, 267] [8, 150] 
regular batch size: 2*782, diving batch size 1*0
best_l after optimization: 26.502059936523438 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 1.3787541389465332, 0.37000155448913574, 135.02462768554688, 328.88031005859375, 629.7481689453125]
alpha/beta optimization time: 4.460269451141357
This batch time : update_bounds func: 5.3377	 prepare: 0.3977	 bound: 4.4611	 transfer: 0.1022	 finalize: 0.3724
Accumulated time: update_bounds func: 118.9905	 prepare: 7.6030	 bound: 100.6757	 transfer: 0.1022	 finalize: 7.5075
batch bounding time:  5.339811325073242
Current worst splitting domains [lb, ub] (depth):
[-0.20811,   inf] (62), [-0.18989,   inf] (62), [-0.18831,   inf] (62), [-0.18763,   inf] (62), [-0.18645,   inf] (62), [-0.18564,   inf] (62), [-0.18536,   inf] (62), [-0.18432,   inf] (62), [-0.18401,   inf] (62), [-0.18308,   inf] (62), [-0.17888,   inf] (62), [-0.17795,   inf] (62), [-0.17564,   inf] (62), [-0.17355,   inf] (62), [-0.17313,   inf] (62), [-0.16808,   inf] (62), [-0.16786,   inf] (62), [-0.16766,   inf] (62), [-0.16611,   inf] (62), [-0.16571,   inf] (62), 
length of domains: 783
Total time: 6.3754	 pickout: 0.3569	 decision: 0.5556	 get_bound: 5.3431	 add_domain: 0.1199
Current lb:-0.20811128616333008
31732 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 146.1926052570343

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([783, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([783, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 150] [8, 150] [8, 219] [8, 219] [8, 150] [8, 150] [8, 150] [8, 267] [8, 219] [8, 150] 
regular batch size: 2*783, diving batch size 1*0
best_l after optimization: 23.860414505004883 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 3.4088406562805176, 0.5540429353713989, 128.26512145996094, 224.28466796875, 612.222412109375]
alpha/beta optimization time: 4.477731227874756
This batch time : update_bounds func: 5.3601	 prepare: 0.3997	 bound: 4.4786	 transfer: 0.1187	 finalize: 0.3590
Accumulated time: update_bounds func: 124.3506	 prepare: 8.0027	 bound: 105.1543	 transfer: 0.1187	 finalize: 7.8666
batch bounding time:  5.362425088882446
Current worst splitting domains [lb, ub] (depth):
[-0.17194,   inf] (64), [-0.17167,   inf] (64), [-0.16853,   inf] (64), [-0.16747,   inf] (64), [-0.16048,   inf] (64), [-0.15474,   inf] (64), [-0.15472,   inf] (64), [-0.15397,   inf] (64), [-0.15138,   inf] (64), [-0.15076,   inf] (64), [-0.14983,   inf] (64), [-0.14944,   inf] (64), [-0.14883,   inf] (64), [-0.14743,   inf] (64), [-0.14584,   inf] (64), [-0.14537,   inf] (64), [-0.14436,   inf] (64), [-0.14416,   inf] (64), [-0.14372,   inf] (64), [-0.14368,   inf] (64), 
length of domains: 861
Total time: 6.5142	 pickout: 0.3517	 decision: 0.5523	 get_bound: 5.3660	 add_domain: 0.2442
Current lb:-0.17193569242954254
33298 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 152.76652145385742

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([861, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([861, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 287] [8, 287] [8, 287] [8, 287] [8, 102] [8, 102] [8, 102] [8, 102] [8, 287] [8, 287] 
regular batch size: 2*861, diving batch size 1*0
best_l after optimization: 6.03261661529541 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 5.796438694000244, 0.5836437940597534, 152.325927734375, 249.50921630859375, 567.399169921875]
alpha/beta optimization time: 4.748643636703491
This batch time : update_bounds func: 5.8420	 prepare: 0.4473	 bound: 4.7494	 transfer: 0.1396	 finalize: 0.5011
Accumulated time: update_bounds func: 130.1926	 prepare: 8.4500	 bound: 109.9037	 transfer: 0.1396	 finalize: 8.3676
batch bounding time:  5.8444294929504395
Current worst splitting domains [lb, ub] (depth):
[-0.13077,   inf] (66), [-0.13009,   inf] (66), [-0.12591,   inf] (66), [-0.12588,   inf] (66), [-0.12443,   inf] (66), [-0.12351,   inf] (66), [-0.12108,   inf] (66), [-0.12086,   inf] (66), [-0.12079,   inf] (66), [-0.11809,   inf] (66), [-0.11464,   inf] (66), [-0.11449,   inf] (66), [-0.11435,   inf] (66), [-0.11430,   inf] (66), [-0.11342,   inf] (66), [-0.11212,   inf] (66), [-0.11014,   inf] (66), [-0.11009,   inf] (66), [-0.10980,   inf] (66), [-0.10970,   inf] (66), 
length of domains: 731
Total time: 6.8734	 pickout: 0.3912	 decision: 0.5172	 get_bound: 5.8480	 add_domain: 0.1170
Current lb:-0.13077354431152344
35020 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 159.7071807384491

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([731, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([731, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [9, 99] [9, 99] [9, 99] [9, 99] [9, 99] [9, 99] [9, 99] [9, 99] [9, 99] [9, 99] 
regular batch size: 2*731, diving batch size 1*0
best_l after optimization: -2.557788610458374 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 10.337518692016602, 1.7678258419036865, 125.50382995605469, 196.96737670898438, 397.44000244140625]
alpha/beta optimization time: 4.178337335586548
This batch time : update_bounds func: 5.0623	 prepare: 0.3708	 bound: 4.1791	 transfer: 0.0811	 finalize: 0.3594
Accumulated time: update_bounds func: 135.2549	 prepare: 8.8208	 bound: 114.0828	 transfer: 0.0811	 finalize: 8.7270
batch bounding time:  5.064697027206421
Current worst splitting domains [lb, ub] (depth):
[-0.10433,   inf] (68), [-0.10397,   inf] (68), [-0.09962,   inf] (68), [-0.09958,   inf] (68), [-0.09833,   inf] (68), [-0.09757,   inf] (68), [-0.09747,   inf] (68), [-0.09479,   inf] (68), [-0.09457,   inf] (68), [-0.09425,   inf] (68), [-0.09236,   inf] (68), [-0.08906,   inf] (68), [-0.08899,   inf] (68), [-0.08840,   inf] (68), [-0.08785,   inf] (68), [-0.08561,   inf] (68), [-0.08418,   inf] (68), [-0.08364,   inf] (68), [-0.08349,   inf] (68), [-0.08326,   inf] (68), 
length of domains: 521
Total time: 5.9317	 pickout: 0.3299	 decision: 0.4455	 get_bound: 5.0688	 add_domain: 0.0875
Current lb:-0.10433268547058105
36482 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 165.70177006721497

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([521, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([521, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 473] [5, 473] [5, 473] [5, 473] [5, 473] [5, 473] [9, 99] [5, 473] [8, 219] [8, 219] 
regular batch size: 2*521, diving batch size 1*0
best_l after optimization: -5.12784481048584 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 15.802928924560547, 2.3971002101898193, 97.27117919921875, 109.02598571777344, 237.49928283691406]
alpha/beta optimization time: 3.3183321952819824
This batch time : update_bounds func: 3.9071	 prepare: 0.2719	 bound: 3.3191	 transfer: 0.0602	 finalize: 0.2532
Accumulated time: update_bounds func: 139.1620	 prepare: 9.0927	 bound: 117.4019	 transfer: 0.0602	 finalize: 8.9802
batch bounding time:  3.9086761474609375
Current worst splitting domains [lb, ub] (depth):
[-0.07871,   inf] (70), [-0.07865,   inf] (70), [-0.07656,   inf] (70), [-0.07589,   inf] (70), [-0.07322,   inf] (70), [-0.07291,   inf] (70), [-0.07191,   inf] (70), [-0.07114,   inf] (70), [-0.07057,   inf] (70), [-0.07043,   inf] (70), [-0.06811,   inf] (70), [-0.06796,   inf] (70), [-0.06716,   inf] (70), [-0.06553,   inf] (70), [-0.06515,   inf] (70), [-0.06461,   inf] (70), [-0.06360,   inf] (70), [-0.06327,   inf] (70), [-0.06229,   inf] (70), [-0.06173,   inf] (70), 
length of domains: 362
Total time: 4.5313	 pickout: 0.2378	 decision: 0.3239	 get_bound: 3.9107	 add_domain: 0.0590
Current lb:-0.0787053108215332
37524 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 170.28106093406677

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([362, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([362, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 473] [5, 473] [8, 392] [8, 392] [6, 159] [8, 392] [8, 392] [6, 159] [7, 22] [8, 392] 
regular batch size: 2*362, diving batch size 1*0
best_l after optimization: -6.363602161407471 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 14.16073226928711, 4.773929595947266, 59.618492126464844, 72.8386001586914, 100.73504638671875]
alpha/beta optimization time: 2.6843438148498535
This batch time : update_bounds func: 3.1069	 prepare: 0.1927	 bound: 2.6853	 transfer: 0.0419	 finalize: 0.1851
Accumulated time: update_bounds func: 142.2688	 prepare: 9.2854	 bound: 120.0872	 transfer: 0.0419	 finalize: 9.1654
batch bounding time:  3.108217239379883
Current worst splitting domains [lb, ub] (depth):
[-0.05441,   inf] (72), [-0.05370,   inf] (72), [-0.05167,   inf] (72), [-0.05166,   inf] (72), [-0.04974,   inf] (72), [-0.04919,   inf] (72), [-0.04885,   inf] (72), [-0.04827,   inf] (72), [-0.04810,   inf] (72), [-0.04792,   inf] (72), [-0.04640,   inf] (72), [-0.04561,   inf] (72), [-0.04470,   inf] (72), [-0.04384,   inf] (72), [-0.04300,   inf] (72), [-0.04165,   inf] (72), [-0.04135,   inf] (72), [-0.04011,   inf] (72), [-0.03977,   inf] (72), [-0.03930,   inf] (72), 
length of domains: 240
Total time: 3.5578	 pickout: 0.1654	 decision: 0.2383	 get_bound: 3.1097	 add_domain: 0.0445
Current lb:-0.05440692976117134
38248 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 173.88664364814758

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([240, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([240, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 143] [7, 22] [5, 473] [8, 376] [6, 159] [6, 159] [8, 376] [6, 159] [6, 159] [8, 376] 
regular batch size: 2*240, diving batch size 1*0
best_l after optimization: -7.379355430603027 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 7.477317810058594, 3.4941463470458984, 26.061229705810547, 39.12339782714844, 42.892513275146484]
alpha/beta optimization time: 2.2313437461853027
This batch time : update_bounds func: 2.5479	 prepare: 0.1265	 bound: 2.2321	 transfer: 0.0452	 finalize: 0.1429
Accumulated time: update_bounds func: 144.8167	 prepare: 9.4119	 bound: 122.3193	 transfer: 0.0452	 finalize: 9.3083
batch bounding time:  2.5489208698272705
Current worst splitting domains [lb, ub] (depth):
[-0.03729,   inf] (74), [-0.03025,   inf] (74), [-0.02931,   inf] (74), [-0.02898,   inf] (74), [-0.02864,   inf] (74), [-0.02783,   inf] (74), [-0.02651,   inf] (74), [-0.02524,   inf] (74), [-0.02316,   inf] (74), [-0.02219,   inf] (74), [-0.02153,   inf] (74), [-0.02146,   inf] (74), [-0.02044,   inf] (74), [-0.02032,   inf] (74), [-0.02016,   inf] (74), [-0.02001,   inf] (74), [-0.01923,   inf] (74), [-0.01915,   inf] (74), [-0.01775,   inf] (74), [-0.01693,   inf] (74), 
length of domains: 86
Total time: 2.8586	 pickout: 0.1233	 decision: 0.1711	 get_bound: 2.5498	 add_domain: 0.0144
Current lb:-0.037294864654541016
38728 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 176.76808881759644

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([86, 16, 16, 16]) pre split depth:  1
batch:  torch.Size([86, 16, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [8, 376] [8, 143] [8, 376] [7, 22] [6, 159] [8, 143] [8, 143] [8, 460] [8, 143] [8, 143] 
regular batch size: 2*86, diving batch size 1*0
best_l after optimization: -3.30220365524292 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 1.5714327096939087, 0.506812334060669, 4.987723350524902, 5.302263259887695, 10.171476364135742]
alpha/beta optimization time: 1.857422113418579
This batch time : update_bounds func: 1.9567	 prepare: 0.0476	 bound: 1.8582	 transfer: 0.0204	 finalize: 0.0300
Accumulated time: update_bounds func: 146.7735	 prepare: 9.4595	 bound: 124.1774	 transfer: 0.0204	 finalize: 9.3383
batch bounding time:  1.957512617111206
Current worst splitting domains [lb, ub] (depth):
[-0.01327,   inf] (76), [-0.00972,   inf] (76), [-0.00956,   inf] (76), [-0.00877,   inf] (76), [-0.00775,   inf] (76), [-0.00593,   inf] (76), [-0.00515,   inf] (76), [-0.00306,   inf] (76), [-0.00293,   inf] (76), [-0.00280,   inf] (76), [-0.00210,   inf] (76), [-0.00206,   inf] (76), [-0.00193,   inf] (76), [-0.00134,   inf] (76), [-0.00095,   inf] (76), 
length of domains: 15
Total time: 2.0952	 pickout: 0.0414	 decision: 0.0915	 get_bound: 1.9578	 add_domain: 0.0045
Current lb:-0.013271093368530273
38900 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 178.87842392921448

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([15, 16, 16, 16]) pre split depth:  3
batch:  torch.Size([15, 16, 16, 16]) post split depth:  3
splitting decisions: 
split level 0: [8, 376] [8, 460] [8, 460] [6, 159] [8, 460] [6, 159] [8, 376] [8, 460] [8, 460] [8, 392] 
split level 1: [7, 22] [8, 376] [8, 376] [7, 22] [8, 376] [8, 21] [7, 22] [8, 376] [8, 376] [8, 21] 
split level 2: [8, 190] [7, 22] [8, 21] [8, 190] [7, 22] [7, 22] [8, 190] [8, 21] [7, 22] [8, 376] 
regular batch size: 2*60, diving batch size 1*0

all verified at 0th iter
best_l after optimization: -9.783527374267578 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.5969809293746948, 0.0, 0.0, 0.824508786201477, 2.8607728481292725]
alpha/beta optimization time: 0.02579975128173828
This batch time : update_bounds func: 0.0976	 prepare: 0.0333	 bound: 0.0265	 transfer: 0.0153	 finalize: 0.0222
Accumulated time: update_bounds func: 146.8711	 prepare: 9.4927	 bound: 124.2040	 transfer: 0.0153	 finalize: 9.3604
batch bounding time:  0.09781551361083984
Current worst splitting domains [lb, ub] (depth):

length of domains: 0
Total time: 0.1939	 pickout: 0.0093	 decision: 0.0598	 get_bound: 0.1248	 add_domain: 0.0001
No domains left, verification finished!
Global ub: inf, batch ub: inf
Cumulative time: 179.07676887512207

Image 0 against label 0 verification end, Time cost: 179.3179268836975
##### [0] True label: 8, Tested against: 1, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
init opt crown verified for label 1 with bound 3.9495091438293457
Image 0 against label 1 verification end, Time cost: 0.00036787986755371094
##### [0] True label: 8, Tested against: 2, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
Model prediction is: tensor([[ 4.6421, -2.6848,  1.0328, -2.7882, -0.4161, -2.7647, -2.5141, -3.0126,
          7.2837,  1.2901]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /28 start_node /29
setting alpha for layer /28 start_node /33
setting alpha for layer /28 start_node /35
setting alpha for layer /28 start_node /38
setting alpha for layer /28 start_node /40
setting alpha for layer /28 start_node /44
setting alpha for layer /28 start_node /46
setting alpha for layer /28 start_node /49
setting alpha for layer /28 start_node /52
not setting layer /28 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 16, 16, 16]) != torch.Size([2, 9, 1, 16, 16, 16]))
setting alpha for layer /30 start_node /33
setting alpha for layer /30 start_node /35
setting alpha for layer /30 start_node /38
setting alpha for layer /30 start_node /40
setting alpha for layer /30 start_node /44
setting alpha for layer /30 start_node /46
setting alpha for layer /30 start_node /49
setting alpha for layer /30 start_node /52
not setting layer /30 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /34 start_node /35
setting alpha for layer /34 start_node /38
setting alpha for layer /34 start_node /40
setting alpha for layer /34 start_node /44
setting alpha for layer /34 start_node /46
setting alpha for layer /34 start_node /49
setting alpha for layer /34 start_node /52
not setting layer /34 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /36 start_node /38
setting alpha for layer /36 start_node /40
setting alpha for layer /36 start_node /44
setting alpha for layer /36 start_node /46
setting alpha for layer /36 start_node /49
setting alpha for layer /36 start_node /52
not setting layer /36 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /39 start_node /40
setting alpha for layer /39 start_node /44
setting alpha for layer /39 start_node /46
setting alpha for layer /39 start_node /49
setting alpha for layer /39 start_node /52
not setting layer /39 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 8, 8]) != torch.Size([2, 9, 1, 32, 8, 8]))
setting alpha for layer /41 start_node /44
setting alpha for layer /41 start_node /46
setting alpha for layer /41 start_node /49
setting alpha for layer /41 start_node /52
not setting layer /41 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
setting alpha for layer /45 start_node /46
setting alpha for layer /45 start_node /49
setting alpha for layer /45 start_node /52
not setting layer /45 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
setting alpha for layer /47 start_node /49
setting alpha for layer /47 start_node /52
not setting layer /47 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
setting alpha for layer /50 start_node /52
not setting layer /50 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 32, 4, 4]) != torch.Size([2, 9, 1, 32, 4, 4]))
not setting layer /53 start_node /54 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /27 torch.Size([1, 16, 16, 16])
1 /29 torch.Size([1, 32, 8, 8])
2 /33 torch.Size([1, 32, 8, 8])
3 /35 torch.Size([1, 32, 8, 8])
4 /38 torch.Size([1, 32, 8, 8])
5 /40 torch.Size([1, 32, 4, 4])
6 /44 torch.Size([1, 32, 4, 4])
7 /46 torch.Size([1, 32, 4, 4])
8 /49 torch.Size([1, 32, 4, 4])
9 /52 torch.Size([1, 100])
best_l after optimization: 0.4528517723083496 with beta sum per layer: []
alpha/beta optimization time: 3.119980573654175
alpha-CROWN with fixed intermediate bounds: tensor([[-0.4529]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-0.4528517723083496
layer 0 size torch.Size([4096]) unstable 490
layer 1 size torch.Size([2048]) unstable 242
layer 2 size torch.Size([2048]) unstable 204
layer 3 size torch.Size([2048]) unstable 225
layer 4 size torch.Size([2048]) unstable 359
layer 5 size torch.Size([512]) unstable 90
layer 6 size torch.Size([512]) unstable 141
layer 7 size torch.Size([512]) unstable 133
layer 8 size torch.Size([512]) unstable 239
layer 9 size torch.Size([100]) unstable 63
-----------------
# of unstable neurons: 2186
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 16, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [9, 96] 
split level 1: [9, 76] 
split level 2: [9, 9] 
split level 3: [9, 71] 
split level 4: [9, 74] 
split level 5: [9, 51] 
split level 6: [9, 26] 
regular batch size: 2*64, diving batch size 1*0

all verified at 0th iter
best_l after optimization: -225.94674682617188 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
alpha/beta optimization time: 0.02799367904663086
This batch time : update_bounds func: 0.0879	 prepare: 0.0258	 bound: 0.0286	 transfer: 0.0108	 finalize: 0.0223
Accumulated time: update_bounds func: 146.9590	 prepare: 9.5185	 bound: 124.2326	 transfer: 0.0108	 finalize: 9.3828
batch bounding time:  0.08816146850585938
Current worst splitting domains [lb, ub] (depth):

length of domains: 0
Total time: 0.2243	 pickout: 0.0017	 decision: 0.1040	 get_bound: 0.1185	 add_domain: 0.0001
No domains left, verification finished!
Global ub: inf, batch ub: inf
Cumulative time: 3.477433443069458

Image 0 against label 2 verification end, Time cost: 3.7115721702575684
##### [0] True label: 8, Tested against: 3, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
init opt crown verified for label 3 with bound 4.028042793273926
Image 0 against label 3 verification end, Time cost: 0.00034332275390625
##### [0] True label: 8, Tested against: 4, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
init opt crown verified for label 4 with bound 0.7130427360534668
Image 0 against label 4 verification end, Time cost: 0.0003247261047363281
##### [0] True label: 8, Tested against: 5, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
init opt crown verified for label 5 with bound 3.6588075160980225
Image 0 against label 5 verification end, Time cost: 0.0003330707550048828
##### [0] True label: 8, Tested against: 6, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
init opt crown verified for label 6 with bound 3.1445746421813965
Image 0 against label 6 verification end, Time cost: 0.0003292560577392578
##### [0] True label: 8, Tested against: 7, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
init opt crown verified for label 7 with bound 2.5424296855926514
Image 0 against label 7 verification end, Time cost: 0.0004444122314453125
##### [0] True label: 8, Tested against: 9, onnx_path: onnx/resnet_4b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet4b_pgd_filtered/prop_13_eps_0.004.vnnlib ######
init opt crown verified for label 9 with bound 0.23316189646720886
Image 0 against label 9 verification end, Time cost: 0.000335693359375/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:25: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1627336316785/work/torch/csrc/utils/tensor_numpy.cpp:143.)
  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:105: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not self.experimental and input[0].shape[self.batch_dim] > 1:
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/utils.py:21: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return value.ndim == 0 or value.shape == torch.Size([1])
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/operations/add.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  elif all(x == 1 for x in input[0].shape):

Result: safe-bab in 230.5682 seconds


[[    0.             0.0000001  39020.           179.31792688
      0.        ]
 [    0.             3.94950914     0.             0.00036788
      1.        ]
 [    0.             0.0000001    128.             3.71157217
      2.        ]
 [    0.             4.02804279     0.             0.00034332
      3.        ]
 [    0.             0.71304274     0.             0.00032473
      4.        ]
 [    0.             3.65880752     0.             0.00033307
      5.        ]
 [    0.             3.14457464     0.             0.00032926
      6.        ]
 [    0.             2.54242969     0.             0.00044441
      7.        ]
 [    0.             0.2331619      0.             0.00033569
      9.        ]]
############# Summary #############
Final verified acc: 100.0% [total 1 examples]
Total verification count: 1 , total verified safe: 1 , verified unsafe: 0 , timeout: 0
mean time [total:1]: 183.03197741508484
mean time [cnt:1]: 183.03197741508484
max time 230.56822800636292
safe-bab (total 1): [0]
