Building native CUDA modules...
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
  csv_name: cifar10_resnet_instances.csv
  onnx_path: null
  vnnlib_path: null
  results_file: null
  root_path: ../../../../../vnncomp2021/benchmarks/cifar10_resnet
model:
  path: null
  name: mnist_9_200
data:
  start: 0
  end: 1
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR
  data_filter_path: null
specification:
  type: lp
  norm: .inf
  epsilon: null
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 2000
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.01
    lr_decay: 0.98
    optimizer: adam
    iteration: 50
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 360
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: max
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 21:53:34 2022 on ubuntu
saving results to vnn-comp_[cifar10_resnet_instances]_start=0_end=1_iter=50_b=2000_timeout=360_branching=kfsb-max-3_lra-init=0.1_lra=0.01_lrb=0.01_PGD=skip.npz
customized start/end sample from 0 to 1

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Model prediction is: tensor([[ 1.5050, -3.7480,  2.3614,  1.7544,  0.2412,  1.7022,  1.1098, -0.0875,
         -2.1250, -2.7137]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[-0.5429,  4.3252, -0.5282,  0.8300, -0.5984,  0.1081,  0.6772,  2.7272,
          3.1053]], device='cuda:0') None
best_l after optimization: -10.927806854248047 with beta sum per layer: []
alpha/beta optimization time: 17.08154845237732
initial alpha-CROWN bounds: tensor([[-0.4049,  4.4225, -0.4635,  0.8909, -0.5251,  0.2131,  0.7737,  2.8083,
          3.2127]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-0.5251, device='cuda:0', grad_fn=<MinBackward1>)
##### [0] True label: 2, Tested against: 0, onnx_path: onnx/resnet_2b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet2b_pgd_filtered/prop_0_eps_0.008.vnnlib ######
Model prediction is: tensor([[ 1.5050, -3.7480,  2.3614,  1.7544,  0.2412,  1.7022,  1.1098, -0.0875,
         -2.1250, -2.7137]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /18 start_node /19
setting alpha for layer /18 start_node /23
setting alpha for layer /18 start_node /25
setting alpha for layer /18 start_node /28
setting alpha for layer /18 start_node /31
not setting layer /18 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 8, 16, 16]) != torch.Size([2, 9, 1, 8, 16, 16]))
setting alpha for layer /20 start_node /23
setting alpha for layer /20 start_node /25
setting alpha for layer /20 start_node /28
setting alpha for layer /20 start_node /31
not setting layer /20 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
setting alpha for layer /24 start_node /25
setting alpha for layer /24 start_node /28
setting alpha for layer /24 start_node /31
not setting layer /24 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
setting alpha for layer /26 start_node /28
setting alpha for layer /26 start_node /31
not setting layer /26 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
setting alpha for layer /29 start_node /31
not setting layer /29 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
not setting layer /32 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /17 torch.Size([1, 8, 16, 16])
1 /19 torch.Size([1, 16, 8, 8])
2 /23 torch.Size([1, 16, 8, 8])
3 /25 torch.Size([1, 16, 8, 8])
4 /28 torch.Size([1, 16, 8, 8])
5 /31 torch.Size([1, 100])
best_l after optimization: 0.4045954942703247 with beta sum per layer: []
alpha/beta optimization time: 2.7204792499542236
alpha-CROWN with fixed intermediate bounds: tensor([[-0.4046]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-0.4045954942703247
layer 0 size torch.Size([2048]) unstable 164
layer 1 size torch.Size([1024]) unstable 131
layer 2 size torch.Size([1024]) unstable 118
layer 3 size torch.Size([1024]) unstable 84
layer 4 size torch.Size([1024]) unstable 170
layer 5 size torch.Size([100]) unstable 26
-----------------
# of unstable neurons: 693
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 8, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 8, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [5, 27] 
split level 1: [5, 17] 
split level 2: [5, 31] 
split level 3: [5, 90] 
split level 4: [5, 4] 
split level 5: [3, 556] 
split level 6: [3, 557] 
regular batch size: 2*64, diving batch size 1*0
best_l after optimization: -1.2861052751541138 with beta sum per layer: [0.0, 0.0, 0.0, 5.800532341003418, 0.0, 15.960451126098633]
alpha/beta optimization time: 1.1724262237548828
This batch time : update_bounds func: 1.2139	 prepare: 0.0166	 bound: 1.1733	 transfer: 0.0091	 finalize: 0.0146
Accumulated time: update_bounds func: 1.2139	 prepare: 0.0166	 bound: 1.1733	 transfer: 0.0091	 finalize: 0.0146
batch bounding time:  1.2143685817718506
Current worst splitting domains [lb, ub] (depth):
[-0.12400,   inf] (8), [-0.11961,   inf] (8), [-0.11651,   inf] (8), [-0.11384,   inf] (8), [-0.11061,   inf] (8), [-0.10993,   inf] (8), [-0.08529,   inf] (8), [-0.07714,   inf] (8), [-0.05962,   inf] (8), [-0.05389,   inf] (8), [-0.05114,   inf] (8), [-0.05020,   inf] (8), [-0.04971,   inf] (8), [-0.04897,   inf] (8), [-0.04686,   inf] (8), [-0.04651,   inf] (8), [-0.04401,   inf] (8), [-0.04316,   inf] (8), [-0.03953,   inf] (8), [-0.03605,   inf] (8), 
length of domains: 31
Total time: 1.3058	 pickout: 0.0015	 decision: 0.0671	 get_bound: 1.2353	 add_domain: 0.0019
Current lb:-0.12400192022323608
128 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.023227691650391

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([31, 8, 16, 16]) pre split depth:  2
batch:  torch.Size([31, 8, 16, 16]) post split depth:  2
splitting decisions: 
split level 0: [3, 923] [3, 923] [3, 923] [4, 410] [4, 410] [4, 410] [3, 923] [4, 410] [3, 923] [3, 923] 
split level 1: [4, 548] [4, 548] [4, 548] [5, 20] [4, 548] [5, 20] [4, 548] [4, 548] [4, 548] [4, 548] 
regular batch size: 2*62, diving batch size 1*0
best_l after optimization: 0.23016297817230225 with beta sum per layer: [0.0, 0.0, 0.0, 18.990022659301758, 2.4700732231140137, 32.15460968017578]
alpha/beta optimization time: 1.1300263404846191
This batch time : update_bounds func: 1.1744	 prepare: 0.0199	 bound: 1.1305	 transfer: 0.0100	 finalize: 0.0136
Accumulated time: update_bounds func: 2.3883	 prepare: 0.0365	 bound: 2.3038	 transfer: 0.0100	 finalize: 0.0281
batch bounding time:  1.1748218536376953
Current worst splitting domains [lb, ub] (depth):
[-0.09585,   inf] (11), [-0.09209,   inf] (11), [-0.09171,   inf] (11), [-0.09140,   inf] (11), [-0.09016,   inf] (11), [-0.08924,   inf] (11), [-0.08773,   inf] (11), [-0.08639,   inf] (11), [-0.08215,   inf] (11), [-0.08170,   inf] (11), [-0.07853,   inf] (11), [-0.07523,   inf] (11), [-0.05956,   inf] (11), [-0.05370,   inf] (11), [-0.05158,   inf] (11), [-0.04825,   inf] (11), [-0.04743,   inf] (11), [-0.04325,   inf] (11), [-0.03908,   inf] (11), [-0.03746,   inf] (11), 
length of domains: 43
Total time: 1.2405	 pickout: 0.0085	 decision: 0.0435	 get_bound: 1.1858	 add_domain: 0.0027
Current lb:-0.09585157781839371
252 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.265690565109253

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([43, 8, 16, 16]) pre split depth:  2
batch:  torch.Size([43, 8, 16, 16]) post split depth:  2
splitting decisions: 
split level 0: [4, 662] [4, 662] [3, 923] [4, 662] [4, 662] [4, 410] [3, 923] [3, 923] [4, 662] [4, 662] 
split level 1: [4, 410] [4, 410] [4, 548] [4, 410] [4, 410] [4, 662] [4, 548] [4, 662] [4, 410] [3, 923] 
regular batch size: 2*86, diving batch size 1*0
best_l after optimization: 0.18397317826747894 with beta sum per layer: [0.0, 0.0, 0.0, 20.76785659790039, 17.519542694091797, 29.33681869506836]
alpha/beta optimization time: 1.1310908794403076
This batch time : update_bounds func: 1.1878	 prepare: 0.0278	 bound: 1.1315	 transfer: 0.0099	 finalize: 0.0181
Accumulated time: update_bounds func: 3.5761	 prepare: 0.0643	 bound: 3.4353	 transfer: 0.0099	 finalize: 0.0462
batch bounding time:  1.1883111000061035
Current worst splitting domains [lb, ub] (depth):
[-0.08178,   inf] (14), [-0.07771,   inf] (14), [-0.07753,   inf] (14), [-0.07602,   inf] (14), [-0.07520,   inf] (14), [-0.07225,   inf] (14), [-0.06779,   inf] (14), [-0.06713,   inf] (14), [-0.06699,   inf] (14), [-0.06480,   inf] (14), [-0.06381,   inf] (14), [-0.06241,   inf] (14), [-0.06201,   inf] (14), [-0.06061,   inf] (14), [-0.04657,   inf] (14), [-0.04467,   inf] (14), [-0.04151,   inf] (14), [-0.04041,   inf] (14), [-0.04000,   inf] (14), [-0.03803,   inf] (14), 
length of domains: 63
Total time: 1.2666	 pickout: 0.0109	 decision: 0.0476	 get_bound: 1.2037	 add_domain: 0.0044
Current lb:-0.08178463578224182
424 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.535250425338745

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([63, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([63, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 20] [5, 20] [5, 20] [5, 20] [5, 20] [5, 20] [5, 20] [4, 662] [5, 20] [4, 662] 
regular batch size: 2*63, diving batch size 1*0
best_l after optimization: 0.7278119325637817 with beta sum per layer: [0.0, 0.0, 0.0, 13.500972747802734, 18.661985397338867, 9.74493408203125]
alpha/beta optimization time: 1.1291093826293945
This batch time : update_bounds func: 1.1698	 prepare: 0.0215	 bound: 1.1296	 transfer: 0.0044	 finalize: 0.0139
Accumulated time: update_bounds func: 4.7459	 prepare: 0.0858	 bound: 4.5649	 transfer: 0.0044	 finalize: 0.0602
batch bounding time:  1.1701254844665527
Current worst splitting domains [lb, ub] (depth):
[-0.06720,   inf] (16), [-0.06297,   inf] (16), [-0.06284,   inf] (16), [-0.06161,   inf] (16), [-0.06041,   inf] (16), [-0.06039,   inf] (16), [-0.05949,   inf] (16), [-0.05807,   inf] (16), [-0.05782,   inf] (16), [-0.05635,   inf] (16), [-0.05610,   inf] (16), [-0.05578,   inf] (16), [-0.05528,   inf] (16), [-0.05319,   inf] (16), [-0.05247,   inf] (16), [-0.05234,   inf] (16), [-0.05073,   inf] (16), [-0.04890,   inf] (16), [-0.04876,   inf] (16), [-0.04385,   inf] (16), 
length of domains: 70
Total time: 1.2419	 pickout: 0.0159	 decision: 0.0507	 get_bound: 1.1704	 add_domain: 0.0049
Current lb:-0.06719929724931717
550 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 8.779891967773438

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([70, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([70, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 683] [5, 88] [4, 683] [4, 683] [5, 88] [5, 88] [4, 548] [5, 88] [5, 88] [4, 683] 
regular batch size: 2*70, diving batch size 1*0
best_l after optimization: -0.666672945022583 with beta sum per layer: [0.0, 0.0, 0.0, 15.854850769042969, 21.6640625, 7.036323070526123]
alpha/beta optimization time: 1.1368956565856934
This batch time : update_bounds func: 1.1816	 prepare: 0.0238	 bound: 1.1374	 transfer: 0.0048	 finalize: 0.0153
Accumulated time: update_bounds func: 5.9274	 prepare: 0.1096	 bound: 5.7022	 transfer: 0.0048	 finalize: 0.0754
batch bounding time:  1.182042121887207
Current worst splitting domains [lb, ub] (depth):
[-0.06274,   inf] (18), [-0.05860,   inf] (18), [-0.05856,   inf] (18), [-0.05704,   inf] (18), [-0.05619,   inf] (18), [-0.05605,   inf] (18), [-0.05400,   inf] (18), [-0.05354,   inf] (18), [-0.05229,   inf] (18), [-0.05159,   inf] (18), [-0.05147,   inf] (18), [-0.05122,   inf] (18), [-0.04869,   inf] (18), [-0.04827,   inf] (18), [-0.04764,   inf] (18), [-0.04625,   inf] (18), [-0.04436,   inf] (18), [-0.04398,   inf] (18), [-0.04303,   inf] (18), [-0.04127,   inf] (18), 
length of domains: 58
Total time: 1.2556	 pickout: 0.0173	 decision: 0.0517	 get_bound: 1.1823	 add_domain: 0.0043
Current lb:-0.06274387240409851
690 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.038748741149902

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([58, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([58, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 88] [4, 683] [5, 88] [5, 88] [5, 0] [5, 0] [5, 0] [5, 0] [5, 0] [5, 0] 
regular batch size: 2*58, diving batch size 1*0
best_l after optimization: -0.08347553014755249 with beta sum per layer: [0.0, 0.0, 0.0, 10.816577911376953, 18.761762619018555, 6.842126846313477]
alpha/beta optimization time: 1.13997220993042
This batch time : update_bounds func: 1.1823	 prepare: 0.0203	 bound: 1.1405	 transfer: 0.0041	 finalize: 0.0171
Accumulated time: update_bounds func: 7.1097	 prepare: 0.1298	 bound: 6.8427	 transfer: 0.0041	 finalize: 0.0925
batch bounding time:  1.1832680702209473
Current worst splitting domains [lb, ub] (depth):
[-0.05836,   inf] (20), [-0.05416,   inf] (20), [-0.05394,   inf] (20), [-0.05254,   inf] (20), [-0.05147,   inf] (20), [-0.05114,   inf] (20), [-0.04932,   inf] (20), [-0.04882,   inf] (20), [-0.04761,   inf] (20), [-0.04716,   inf] (20), [-0.04676,   inf] (20), [-0.04647,   inf] (20), [-0.04430,   inf] (20), [-0.04360,   inf] (20), [-0.04275,   inf] (20), [-0.03838,   inf] (20), [-0.03708,   inf] (20), [-0.03676,   inf] (20), [-0.03558,   inf] (20), [-0.03510,   inf] (20), 
length of domains: 52
Total time: 1.2523	 pickout: 0.0147	 decision: 0.0497	 get_bound: 1.1835	 add_domain: 0.0044
Current lb:-0.058355093002319336
806 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 11.294734001159668

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([52, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([52, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 683] [5, 0] [2, 683] [2, 683] [4, 683] [4, 683] [4, 683] [4, 683] [4, 548] [2, 683] 
regular batch size: 2*52, diving batch size 1*0
best_l after optimization: 0.8045655488967896 with beta sum per layer: [0.0, 0.0, 0.7121995091438293, 8.81386947631836, 19.624847412109375, 5.0464911460876465]
alpha/beta optimization time: 1.2026646137237549
This batch time : update_bounds func: 1.2402	 prepare: 0.0185	 bound: 1.2033	 transfer: 0.0069	 finalize: 0.0113
Accumulated time: update_bounds func: 8.3499	 prepare: 0.1483	 bound: 8.0460	 transfer: 0.0069	 finalize: 0.1037
batch bounding time:  1.2406253814697266
Current worst splitting domains [lb, ub] (depth):
[-0.04934,   inf] (22), [-0.04924,   inf] (22), [-0.04672,   inf] (22), [-0.04655,   inf] (22), [-0.04630,   inf] (22), [-0.04469,   inf] (22), [-0.04409,   inf] (22), [-0.04359,   inf] (22), [-0.04347,   inf] (22), [-0.04200,   inf] (22), [-0.04184,   inf] (22), [-0.04030,   inf] (22), [-0.03944,   inf] (22), [-0.03888,   inf] (22), [-0.03874,   inf] (22), [-0.03850,   inf] (22), [-0.03749,   inf] (22), [-0.03429,   inf] (22), [-0.03289,   inf] (22), [-0.03288,   inf] (22), 
length of domains: 54
Total time: 1.3112	 pickout: 0.0163	 decision: 0.0496	 get_bound: 1.2408	 add_domain: 0.0045
Current lb:-0.049343228340148926
910 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 12.608484983444214

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([54, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([54, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 683] [5, 0] [2, 683] [2, 683] [5, 0] [2, 683] [4, 550] [5, 0] [5, 0] [2, 683] 
regular batch size: 2*54, diving batch size 1*0
best_l after optimization: 0.9603453278541565 with beta sum per layer: [0.0, 0.0, 4.027894496917725, 6.490688323974609, 12.456901550292969, 6.175780296325684]
alpha/beta optimization time: 1.1847715377807617
This batch time : update_bounds func: 1.2224	 prepare: 0.0193	 bound: 1.1853	 transfer: 0.0058	 finalize: 0.0117
Accumulated time: update_bounds func: 9.5723	 prepare: 0.1676	 bound: 9.2313	 transfer: 0.0058	 finalize: 0.1154
batch bounding time:  1.2227895259857178
Current worst splitting domains [lb, ub] (depth):
[-0.04436,   inf] (24), [-0.04127,   inf] (24), [-0.04034,   inf] (24), [-0.03934,   inf] (24), [-0.03910,   inf] (24), [-0.03860,   inf] (24), [-0.03850,   inf] (24), [-0.03755,   inf] (24), [-0.03593,   inf] (24), [-0.03587,   inf] (24), [-0.03541,   inf] (24), [-0.03507,   inf] (24), [-0.03432,   inf] (24), [-0.03362,   inf] (24), [-0.03246,   inf] (24), [-0.03211,   inf] (24), [-0.03193,   inf] (24), [-0.03158,   inf] (24), [-0.03081,   inf] (24), [-0.02939,   inf] (24), 
length of domains: 62
Total time: 1.2919	 pickout: 0.0141	 decision: 0.0489	 get_bound: 1.2230	 add_domain: 0.0059
Current lb:-0.04435586929321289
1018 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 13.902884006500244

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([62, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([62, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 550] [4, 550] [4, 550] [4, 550] [4, 550] [4, 550] [4, 550] [4, 550] [4, 550] [4, 550] 
regular batch size: 2*62, diving batch size 1*0
best_l after optimization: 1.344088077545166 with beta sum per layer: [0.0, 0.0, 7.574280738830566, 4.319741249084473, 4.58089542388916, 8.576996803283691]
alpha/beta optimization time: 1.1489622592926025
This batch time : update_bounds func: 1.2595	 prepare: 0.0222	 bound: 1.1495	 transfer: 0.0054	 finalize: 0.0820
Accumulated time: update_bounds func: 10.8318	 prepare: 0.1898	 bound: 10.3808	 transfer: 0.0054	 finalize: 0.1975
batch bounding time:  1.2599382400512695
Current worst splitting domains [lb, ub] (depth):
[-0.03574,   inf] (26), [-0.03342,   inf] (26), [-0.03243,   inf] (26), [-0.03158,   inf] (26), [-0.03086,   inf] (26), [-0.03078,   inf] (26), [-0.03020,   inf] (26), [-0.02986,   inf] (26), [-0.02980,   inf] (26), [-0.02960,   inf] (26), [-0.02920,   inf] (26), [-0.02770,   inf] (26), [-0.02753,   inf] (26), [-0.02714,   inf] (26), [-0.02687,   inf] (26), [-0.02646,   inf] (26), [-0.02565,   inf] (26), [-0.02565,   inf] (26), [-0.02543,   inf] (26), [-0.02500,   inf] (26), 
length of domains: 93
Total time: 1.3343	 pickout: 0.0163	 decision: 0.0501	 get_bound: 1.2602	 add_domain: 0.0077
Current lb:-0.03574061393737793
1142 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 15.239816427230835

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([93, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([93, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 484] [3, 484] [3, 484] [3, 484] [3, 484] [3, 484] [3, 484] [3, 484] [3, 484] [3, 653] 
regular batch size: 2*93, diving batch size 1*0
best_l after optimization: 1.5089480876922607 with beta sum per layer: [0.0, 0.0, 11.524415969848633, 10.481343269348145, 5.456690311431885, 11.712654113769531]
alpha/beta optimization time: 1.185356616973877
This batch time : update_bounds func: 1.2504	 prepare: 0.0333	 bound: 1.1859	 transfer: 0.0099	 finalize: 0.0208
Accumulated time: update_bounds func: 12.0821	 prepare: 0.2230	 bound: 11.5667	 transfer: 0.0099	 finalize: 0.2183
batch bounding time:  1.2508811950683594
Current worst splitting domains [lb, ub] (depth):
[-0.02960,   inf] (28), [-0.02723,   inf] (28), [-0.02681,   inf] (28), [-0.02614,   inf] (28), [-0.02538,   inf] (28), [-0.02512,   inf] (28), [-0.02501,   inf] (28), [-0.02460,   inf] (28), [-0.02450,   inf] (28), [-0.02436,   inf] (28), [-0.02384,   inf] (28), [-0.02314,   inf] (28), [-0.02270,   inf] (28), [-0.02236,   inf] (28), [-0.02213,   inf] (28), [-0.02187,   inf] (28), [-0.02165,   inf] (28), [-0.02139,   inf] (28), [-0.02114,   inf] (28), [-0.02107,   inf] (28), 
length of domains: 133
Total time: 1.3467	 pickout: 0.0237	 decision: 0.0600	 get_bound: 1.2512	 add_domain: 0.0117
Current lb:-0.029601216316223145
1328 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 16.59073305130005

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([133, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([133, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 794] [4, 794] [4, 794] [4, 537] [4, 537] [4, 428] [4, 794] [4, 794] [4, 537] [4, 537] 
regular batch size: 2*133, diving batch size 1*0
best_l after optimization: 1.2398405075073242 with beta sum per layer: [0.0, 0.0, 12.082535743713379, 13.266982078552246, 11.338120460510254, 13.395235061645508]
alpha/beta optimization time: 1.189429759979248
This batch time : update_bounds func: 1.2900	 prepare: 0.0462	 bound: 1.1900	 transfer: 0.0221	 finalize: 0.0311
Accumulated time: update_bounds func: 13.3721	 prepare: 0.2692	 bound: 12.7566	 transfer: 0.0221	 finalize: 0.2494
batch bounding time:  1.290635347366333
Current worst splitting domains [lb, ub] (depth):
[-0.02393,   inf] (30), [-0.02172,   inf] (30), [-0.02163,   inf] (30), [-0.02152,   inf] (30), [-0.02108,   inf] (30), [-0.02087,   inf] (30), [-0.02080,   inf] (30), [-0.02007,   inf] (30), [-0.01983,   inf] (30), [-0.01913,   inf] (30), [-0.01894,   inf] (30), [-0.01892,   inf] (30), [-0.01888,   inf] (30), [-0.01884,   inf] (30), [-0.01867,   inf] (30), [-0.01857,   inf] (30), [-0.01822,   inf] (30), [-0.01769,   inf] (30), [-0.01748,   inf] (30), [-0.01739,   inf] (30), 
length of domains: 181
Total time: 1.4180	 pickout: 0.0352	 decision: 0.0751	 get_bound: 1.2911	 add_domain: 0.0166
Current lb:-0.023929476737976074
1594 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 18.014990091323853

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([181, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([181, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 428] [4, 428] [4, 537] [4, 428] [4, 537] [4, 428] [4, 428] [4, 428] [4, 428] [4, 537] 
regular batch size: 2*181, diving batch size 1*0
best_l after optimization: 0.27753645181655884 with beta sum per layer: [0.0, 0.0217535849660635, 13.214825630187988, 14.318111419677734, 22.89505386352539, 11.928655624389648]
alpha/beta optimization time: 1.1932978630065918
This batch time : update_bounds func: 1.3207	 prepare: 0.0613	 bound: 1.1938	 transfer: 0.0257	 finalize: 0.0390
Accumulated time: update_bounds func: 14.6928	 prepare: 0.3305	 bound: 13.9504	 transfer: 0.0257	 finalize: 0.2884
batch bounding time:  1.3215153217315674
Current worst splitting domains [lb, ub] (depth):
[-0.02105,   inf] (32), [-0.01883,   inf] (32), [-0.01859,   inf] (32), [-0.01793,   inf] (32), [-0.01790,   inf] (32), [-0.01716,   inf] (32), [-0.01684,   inf] (32), [-0.01665,   inf] (32), [-0.01645,   inf] (32), [-0.01598,   inf] (32), [-0.01586,   inf] (32), [-0.01532,   inf] (32), [-0.01425,   inf] (32), [-0.01404,   inf] (32), [-0.01397,   inf] (32), [-0.01389,   inf] (32), [-0.01388,   inf] (32), [-0.01363,   inf] (32), [-0.01333,   inf] (32), [-0.01304,   inf] (32), 
length of domains: 161
Total time: 1.4735	 pickout: 0.0455	 decision: 0.0896	 get_bound: 1.3222	 add_domain: 0.0162
Current lb:-0.021046042442321777
1956 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 19.49655532836914

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([161, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([161, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 653] [3, 653] [3, 653] [3, 653] [3, 653] [3, 653] [3, 653] [4, 890] [4, 428] [3, 653] 
regular batch size: 2*161, diving batch size 1*0
best_l after optimization: -0.17762979865074158 with beta sum per layer: [0.0, 0.049332015216350555, 9.473566055297852, 9.079721450805664, 19.285524368286133, 6.784721851348877]
alpha/beta optimization time: 1.2263731956481934
This batch time : update_bounds func: 1.3998	 prepare: 0.0548	 bound: 1.2269	 transfer: 0.0096	 finalize: 0.1077
Accumulated time: update_bounds func: 16.0926	 prepare: 0.3853	 bound: 15.1773	 transfer: 0.0096	 finalize: 0.3961
batch bounding time:  1.4005367755889893
Current worst splitting domains [lb, ub] (depth):
[-0.01696,   inf] (34), [-0.01474,   inf] (34), [-0.01450,   inf] (34), [-0.01379,   inf] (34), [-0.01377,   inf] (34), [-0.01353,   inf] (34), [-0.01344,   inf] (34), [-0.01306,   inf] (34), [-0.01267,   inf] (34), [-0.01182,   inf] (34), [-0.01179,   inf] (34), [-0.01122,   inf] (34), [-0.01112,   inf] (34), [-0.01104,   inf] (34), [-0.00963,   inf] (34), [-0.00937,   inf] (34), [-0.00937,   inf] (34), [-0.00922,   inf] (34), [-0.00914,   inf] (34), [-0.00893,   inf] (34), 
length of domains: 115
Total time: 1.5330	 pickout: 0.0399	 decision: 0.0811	 get_bound: 1.4011	 add_domain: 0.0109
Current lb:-0.016959071159362793
2278 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 21.03761887550354

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([115, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([115, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 323] [4, 323] [4, 323] [4, 794] [4, 537] [3, 653] [4, 794] [4, 323] [4, 794] [4, 537] 
regular batch size: 2*115, diving batch size 1*0
best_l after optimization: -0.6833487749099731 with beta sum per layer: [0.0, 0.020150739699602127, 5.851372718811035, 3.8780112266540527, 10.246456146240234, 3.41805362701416]
alpha/beta optimization time: 1.1821088790893555
This batch time : update_bounds func: 1.2562	 prepare: 0.0400	 bound: 1.1826	 transfer: 0.0074	 finalize: 0.0256
Accumulated time: update_bounds func: 17.3489	 prepare: 0.4253	 bound: 16.3599	 transfer: 0.0074	 finalize: 0.4217
batch bounding time:  1.2567808628082275
Current worst splitting domains [lb, ub] (depth):
[-0.01496,   inf] (36), [-0.01276,   inf] (36), [-0.01250,   inf] (36), [-0.01108,   inf] (36), [-0.00980,   inf] (36), [-0.00942,   inf] (36), [-0.00922,   inf] (36), [-0.00905,   inf] (36), [-0.00758,   inf] (36), [-0.00726,   inf] (36), [-0.00722,   inf] (36), [-0.00711,   inf] (36), [-0.00700,   inf] (36), [-0.00694,   inf] (36), [-0.00691,   inf] (36), [-0.00653,   inf] (36), [-0.00640,   inf] (36), [-0.00565,   inf] (36), [-0.00553,   inf] (36), [-0.00483,   inf] (36), 
length of domains: 72
Total time: 1.3626	 pickout: 0.0296	 decision: 0.0670	 get_bound: 1.2572	 add_domain: 0.0088
Current lb:-0.014964938163757324
2508 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 22.409231901168823

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([72, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([72, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 881] [4, 881] [4, 881] [4, 881] [4, 881] [4, 323] [4, 881] [4, 323] [1, 36] [4, 492] 
regular batch size: 2*72, diving batch size 1*0
best_l after optimization: -0.7171111106872559 with beta sum per layer: [0.0, 0.0827665776014328, 2.743962526321411, 1.4672861099243164, 4.504283905029297, 1.0620741844177246]
alpha/beta optimization time: 1.3476943969726562
This batch time : update_bounds func: 1.4162	 prepare: 0.0396	 bound: 1.3483	 transfer: 0.0102	 finalize: 0.0176
Accumulated time: update_bounds func: 18.7651	 prepare: 0.4649	 bound: 17.7082	 transfer: 0.0102	 finalize: 0.4393
batch bounding time:  1.4167261123657227
Current worst splitting domains [lb, ub] (depth):
[-0.01280,   inf] (38), [-0.01059,   inf] (38), [-0.01034,   inf] (38), [-0.00890,   inf] (38), [-0.00760,   inf] (38), [-0.00741,   inf] (38), [-0.00705,   inf] (38), [-0.00654,   inf] (38), [-0.00511,   inf] (38), [-0.00502,   inf] (38), [-0.00499,   inf] (38), [-0.00450,   inf] (38), [-0.00425,   inf] (38), [-0.00341,   inf] (38), [-0.00339,   inf] (38), [-0.00258,   inf] (38), [-0.00257,   inf] (38), [-0.00251,   inf] (38), [-0.00234,   inf] (38), [-0.00201,   inf] (38), 
length of domains: 38
Total time: 1.5206	 pickout: 0.0283	 decision: 0.0710	 get_bound: 1.4170	 add_domain: 0.0042
Current lb:-0.012804985046386719
2652 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 23.934423685073853

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([38, 8, 16, 16]) pre split depth:  2
batch:  torch.Size([38, 8, 16, 16]) post split depth:  2
splitting decisions: 
split level 0: [4, 537] [4, 794] [4, 537] [4, 794] [4, 537] [4, 881] [4, 881] [4, 537] [4, 881] [4, 537] 
split level 1: [4, 675] [4, 675] [4, 675] [4, 675] [4, 675] [4, 675] [1, 36] [4, 675] [1, 36] [4, 675] 
regular batch size: 2*76, diving batch size 1*0
best_l after optimization: -1.5976964235305786 with beta sum per layer: [0.0, 0.0, 2.848477840423584, 1.4783824682235718, 4.992232799530029, 0.5286752581596375]
alpha/beta optimization time: 1.2488622665405273
This batch time : update_bounds func: 1.3035	 prepare: 0.0272	 bound: 1.2494	 transfer: 0.0098	 finalize: 0.0163
Accumulated time: update_bounds func: 20.0685	 prepare: 0.4921	 bound: 18.9576	 transfer: 0.0098	 finalize: 0.4557
batch bounding time:  1.303995132446289
Current worst splitting domains [lb, ub] (depth):
[-0.00300,   inf] (41), [-0.00055,   inf] (41), [-0.00037,   inf] (41), 
length of domains: 3
Total time: 1.3774	 pickout: 0.0110	 decision: 0.0467	 get_bound: 1.3192	 add_domain: 0.0005
Current lb:-0.0030023653525859118
2804 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 25.31591010093689

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([3, 8, 16, 16]) pre split depth:  6
batch:  torch.Size([3, 8, 16, 16]) post split depth:  6
splitting decisions: 
split level 0: [4, 890] [4, 890] [4, 890] 
split level 1: [1, 604] [1, 604] [1, 35] 
split level 2: [2, 732] [2, 732] [4, 675] 
split level 3: [4, 146] [4, 146] [4, 146] 
split level 4: [4, 722] [4, 722] [1, 604] 
split level 5: [3, 861] [3, 861] [4, 722] 
regular batch size: 2*96, diving batch size 1*0

all verified at 0th iter
best_l after optimization: -5.675597667694092 with beta sum per layer: [0.0, 0.0, 0.0, 0.0, 2.93123197555542, 0.0]
alpha/beta optimization time: 0.016343355178833008
This batch time : update_bounds func: 0.0853	 prepare: 0.0339	 bound: 0.0167	 transfer: 0.0125	 finalize: 0.0214
Accumulated time: update_bounds func: 20.1538	 prepare: 0.5260	 bound: 18.9743	 transfer: 0.0125	 finalize: 0.4770
batch bounding time:  0.08552098274230957
Current worst splitting domains [lb, ub] (depth):

length of domains: 0
Total time: 0.2019	 pickout: 0.0038	 decision: 0.0622	 get_bound: 0.1358	 add_domain: 0.0001
No domains left, verification finished!
Global ub: inf, batch ub: inf
Cumulative time: 25.52051091194153

Image 0 against label 0 verification end, Time cost: 25.65178370475769
##### [0] True label: 2, Tested against: 1, onnx_path: onnx/resnet_2b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet2b_pgd_filtered/prop_0_eps_0.008.vnnlib ######
init opt crown verified for label 1 with bound 4.422472953796387
Image 0 against label 1 verification end, Time cost: 0.00030684471130371094
##### [0] True label: 2, Tested against: 3, onnx_path: onnx/resnet_2b.onnx, vnnlib_path: vnnlib_properties_pgd_filtered/resnet2b_pgd_filtered/prop_0_eps_0.008.vnnlib ######
Model prediction is: tensor([[ 1.5050, -3.7480,  2.3614,  1.7544,  0.2412,  1.7022,  1.1098, -0.0875,
         -2.1250, -2.7137]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /18 start_node /19
setting alpha for layer /18 start_node /23
setting alpha for layer /18 start_node /25
setting alpha for layer /18 start_node /28
setting alpha for layer /18 start_node /31
not setting layer /18 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 8, 16, 16]) != torch.Size([2, 9, 1, 8, 16, 16]))
setting alpha for layer /20 start_node /23
setting alpha for layer /20 start_node /25
setting alpha for layer /20 start_node /28
setting alpha for layer /20 start_node /31
not setting layer /20 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
setting alpha for layer /24 start_node /25
setting alpha for layer /24 start_node /28
setting alpha for layer /24 start_node /31
not setting layer /24 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
setting alpha for layer /26 start_node /28
setting alpha for layer /26 start_node /31
not setting layer /26 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
setting alpha for layer /29 start_node /31
not setting layer /29 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 16, 8, 8]) != torch.Size([2, 9, 1, 16, 8, 8]))
not setting layer /32 start_node /33 because shape mismatch (torch.Size([2, 1, 1, 100]) != torch.Size([2, 9, 1, 100]))
0 /17 torch.Size([1, 8, 16, 16])
1 /19 torch.Size([1, 16, 8, 8])
2 /23 torch.Size([1, 16, 8, 8])
3 /25 torch.Size([1, 16, 8, 8])
4 /28 torch.Size([1, 16, 8, 8])
5 /31 torch.Size([1, 100])
best_l after optimization: 0.4631694555282593 with beta sum per layer: []
alpha/beta optimization time: 1.917037010192871
alpha-CROWN with fixed intermediate bounds: tensor([[-0.4632]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-0.4631694555282593
layer 0 size torch.Size([2048]) unstable 164
layer 1 size torch.Size([1024]) unstable 131
layer 2 size torch.Size([1024]) unstable 118
layer 3 size torch.Size([1024]) unstable 84
layer 4 size torch.Size([1024]) unstable 170
layer 5 size torch.Size([100]) unstable 26
-----------------
# of unstable neurons: 693
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 8, 16, 16]) pre split depth:  7
batch:  torch.Size([1, 8, 16, 16]) post split depth:  7
splitting decisions: 
split level 0: [5, 32] 
split level 1: [5, 6] 
split level 2: [5, 17] 
split level 3: [5, 90] 
split level 4: [5, 31] 
split level 5: [3, 556] 
split level 6: [4, 548] 
regular batch size: 2*64, diving batch size 1*0
best_l after optimization: 7.508181571960449 with beta sum per layer: [0.0, 0.0, 0.0, 7.530381202697754, 0.015239682979881763, 63.555076599121094]
alpha/beta optimization time: 1.2760491371154785
This batch time : update_bounds func: 1.3182	 prepare: 0.0166	 bound: 1.2770	 transfer: 0.0092	 finalize: 0.0150
Accumulated time: update_bounds func: 21.4720	 prepare: 0.5425	 bound: 20.2513	 transfer: 0.0092	 finalize: 0.4920
batch bounding time:  1.3186836242675781
Current worst splitting domains [lb, ub] (depth):
[-0.25930,   inf] (8), [-0.25646,   inf] (8), [-0.23760,   inf] (8), [-0.23674,   inf] (8), [-0.23573,   inf] (8), [-0.23461,   inf] (8), [-0.21581,   inf] (8), [-0.21385,   inf] (8), [-0.21246,   inf] (8), [-0.21175,   inf] (8), [-0.19603,   inf] (8), [-0.19560,   inf] (8), [-0.19100,   inf] (8), [-0.19012,   inf] (8), [-0.18433,   inf] (8), [-0.18111,   inf] (8), [-0.17448,   inf] (8), [-0.17332,   inf] (8), [-0.17329,   inf] (8), [-0.17224,   inf] (8), 
length of domains: 82
Total time: 1.4125	 pickout: 0.0014	 decision: 0.0669	 get_bound: 1.3394	 add_domain: 0.0048
Current lb:-0.25930148363113403
128 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 3.3813486099243164

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([82, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([82, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 13] [5, 13] [5, 13] [5, 13] [5, 13] [5, 13] [5, 13] [5, 13] [5, 13] [5, 13] 
regular batch size: 2*82, diving batch size 1*0
best_l after optimization: 5.799556732177734 with beta sum per layer: [0.0, 0.0, 0.0, 10.10569953918457, 0.6268649697303772, 111.79198455810547]
alpha/beta optimization time: 1.2130162715911865
This batch time : update_bounds func: 1.2697	 prepare: 0.0273	 bound: 1.2135	 transfer: 0.0109	 finalize: 0.0176
Accumulated time: update_bounds func: 22.7417	 prepare: 0.5698	 bound: 21.4648	 transfer: 0.0109	 finalize: 0.5097
batch bounding time:  1.2702436447143555
Current worst splitting domains [lb, ub] (depth):
[-0.25120,   inf] (10), [-0.24859,   inf] (10), [-0.22963,   inf] (10), [-0.22884,   inf] (10), [-0.22749,   inf] (10), [-0.22623,   inf] (10), [-0.20718,   inf] (10), [-0.20426,   inf] (10), [-0.20262,   inf] (10), [-0.20022,   inf] (10), [-0.18524,   inf] (10), [-0.18460,   inf] (10), [-0.17799,   inf] (10), [-0.17716,   inf] (10), [-0.16396,   inf] (10), [-0.16182,   inf] (10), [-0.15962,   inf] (10), [-0.15694,   inf] (10), [-0.15338,   inf] (10), [-0.15315,   inf] (10), 
length of domains: 77
Total time: 1.3552	 pickout: 0.0207	 decision: 0.0581	 get_bound: 1.2706	 add_domain: 0.0058
Current lb:-0.2512037456035614
292 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.740068197250366

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([77, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([77, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 532] [4, 532] [4, 532] [4, 532] [3, 557] [4, 532] [3, 557] [4, 532] [4, 532] [4, 532] 
regular batch size: 2*77, diving batch size 1*0
best_l after optimization: 8.85423469543457 with beta sum per layer: [0.0, 0.0, 0.0, 14.208282470703125, 4.529610633850098, 110.280029296875]
alpha/beta optimization time: 1.2077527046203613
This batch time : update_bounds func: 1.2573	 prepare: 0.0259	 bound: 1.2082	 transfer: 0.0057	 finalize: 0.0171
Accumulated time: update_bounds func: 23.9990	 prepare: 0.5957	 bound: 22.6730	 transfer: 0.0057	 finalize: 0.5268
batch bounding time:  1.2578110694885254
Current worst splitting domains [lb, ub] (depth):
[-0.24199,   inf] (12), [-0.23911,   inf] (12), [-0.22807,   inf] (12), [-0.22685,   inf] (12), [-0.21988,   inf] (12), [-0.21909,   inf] (12), [-0.21868,   inf] (12), [-0.21675,   inf] (12), [-0.20754,   inf] (12), [-0.20652,   inf] (12), [-0.20324,   inf] (12), [-0.19553,   inf] (12), [-0.19440,   inf] (12), [-0.18834,   inf] (12), [-0.18582,   inf] (12), [-0.18223,   inf] (12), [-0.17916,   inf] (12), [-0.17462,   inf] (12), [-0.17388,   inf] (12), [-0.17090,   inf] (12), 
length of domains: 88
Total time: 1.3374	 pickout: 0.0193	 decision: 0.0546	 get_bound: 1.2581	 add_domain: 0.0054
Current lb:-0.24198698997497559
446 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.080898761749268

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([88, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([88, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 64] [5, 64] [5, 64] [5, 64] [5, 64] [5, 64] [5, 64] [5, 64] [5, 64] [5, 64] 
regular batch size: 2*88, diving batch size 1*0
best_l after optimization: 11.368087768554688 with beta sum per layer: [0.0, 0.42930710315704346, 0.0, 24.46674346923828, 11.475576400756836, 128.71066284179688]
alpha/beta optimization time: 1.2644767761230469
This batch time : update_bounds func: 1.3214	 prepare: 0.0297	 bound: 1.2650	 transfer: 0.0071	 finalize: 0.0192
Accumulated time: update_bounds func: 25.3205	 prepare: 0.6254	 bound: 23.9380	 transfer: 0.0071	 finalize: 0.5459
batch bounding time:  1.3219633102416992
Current worst splitting domains [lb, ub] (depth):
[-0.23683,   inf] (14), [-0.23388,   inf] (14), [-0.22273,   inf] (14), [-0.22137,   inf] (14), [-0.21440,   inf] (14), [-0.21363,   inf] (14), [-0.21352,   inf] (14), [-0.21155,   inf] (14), [-0.20185,   inf] (14), [-0.20082,   inf] (14), [-0.19777,   inf] (14), [-0.19012,   inf] (14), [-0.18889,   inf] (14), [-0.18302,   inf] (14), [-0.18055,   inf] (14), [-0.17648,   inf] (14), [-0.16554,   inf] (14), [-0.16553,   inf] (14), [-0.16482,   inf] (14), [-0.16427,   inf] (14), 
length of domains: 135
Total time: 1.4127	 pickout: 0.0220	 decision: 0.0596	 get_bound: 1.3223	 add_domain: 0.0088
Current lb:-0.23683471977710724
622 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.497262477874756

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([135, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([135, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 29] [5, 29] [5, 29] [5, 29] [5, 29] [5, 29] [5, 29] [5, 29] [5, 29] [5, 29] 
regular batch size: 2*135, diving batch size 1*0
best_l after optimization: 14.789353370666504 with beta sum per layer: [0.0, 2.2178492546081543, 0.0, 41.55420684814453, 24.085844039916992, 195.0606689453125]
alpha/beta optimization time: 1.2549843788146973
This batch time : update_bounds func: 1.3554	 prepare: 0.0461	 bound: 1.2555	 transfer: 0.0210	 finalize: 0.0321
Accumulated time: update_bounds func: 26.6759	 prepare: 0.6715	 bound: 25.1935	 transfer: 0.0210	 finalize: 0.5780
batch bounding time:  1.3561582565307617
Current worst splitting domains [lb, ub] (depth):
[-0.23130,   inf] (16), [-0.22836,   inf] (16), [-0.21717,   inf] (16), [-0.21581,   inf] (16), [-0.20881,   inf] (16), [-0.20804,   inf] (16), [-0.20781,   inf] (16), [-0.20580,   inf] (16), [-0.19631,   inf] (16), [-0.19524,   inf] (16), [-0.19206,   inf] (16), [-0.18454,   inf] (16), [-0.18315,   inf] (16), [-0.17069,   inf] (16), [-0.16954,   inf] (16), [-0.16839,   inf] (16), [-0.16467,   inf] (16), [-0.15938,   inf] (16), [-0.15785,   inf] (16), [-0.15654,   inf] (16), 
length of domains: 198
Total time: 1.4814	 pickout: 0.0337	 decision: 0.0771	 get_bound: 1.3567	 add_domain: 0.0140
Current lb:-0.2313019037246704
892 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 8.984869003295898

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([198, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([198, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 987] [5, 93] [5, 93] [5, 93] [5, 93] [3, 980] [5, 93] [3, 987] [5, 93] [3, 980] 
regular batch size: 2*198, diving batch size 1*0
best_l after optimization: 19.570556640625 with beta sum per layer: [0.0, 7.827923774719238, 0.0, 76.98043060302734, 54.42197799682617, 287.42474365234375]
alpha/beta optimization time: 1.2744040489196777
This batch time : update_bounds func: 1.4137	 prepare: 0.0649	 bound: 1.2749	 transfer: 0.0290	 finalize: 0.0439
Accumulated time: update_bounds func: 28.0896	 prepare: 0.7364	 bound: 26.4684	 transfer: 0.0290	 finalize: 0.6219
batch bounding time:  1.4144554138183594
Current worst splitting domains [lb, ub] (depth):
[-0.22563,   inf] (18), [-0.22368,   inf] (18), [-0.21253,   inf] (18), [-0.21108,   inf] (18), [-0.20368,   inf] (18), [-0.20325,   inf] (18), [-0.19983,   inf] (18), [-0.19283,   inf] (18), [-0.19125,   inf] (18), [-0.19119,   inf] (18), [-0.18756,   inf] (18), [-0.18438,   inf] (18), [-0.18241,   inf] (18), [-0.17914,   inf] (18), [-0.17461,   inf] (18), [-0.16835,   inf] (18), [-0.16536,   inf] (18), [-0.16256,   inf] (18), [-0.16121,   inf] (18), [-0.16085,   inf] (18), 
length of domains: 288
Total time: 1.6519	 pickout: 0.0490	 decision: 0.1661	 get_bound: 1.4152	 add_domain: 0.0217
Current lb:-0.22563445568084717
1288 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.644752740859985

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([288, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([288, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 93] [1, 35] [3, 980] [3, 557] [3, 557] [4, 532] [5, 93] [5, 93] [5, 93] [3, 557] 
regular batch size: 2*288, diving batch size 1*0
best_l after optimization: 26.301490783691406 with beta sum per layer: [0.0, 18.831836700439453, 0.0, 128.904296875, 123.64781188964844, 423.7193603515625]
alpha/beta optimization time: 1.345046043395996
This batch time : update_bounds func: 1.5442	 prepare: 0.0955	 bound: 1.3456	 transfer: 0.0369	 finalize: 0.0649
Accumulated time: update_bounds func: 29.6338	 prepare: 0.8319	 bound: 27.8140	 transfer: 0.0369	 finalize: 0.6867
batch bounding time:  1.5451502799987793
Current worst splitting domains [lb, ub] (depth):
[-0.22101,   inf] (20), [-0.21946,   inf] (20), [-0.20293,   inf] (20), [-0.20074,   inf] (20), [-0.19971,   inf] (20), [-0.19667,   inf] (20), [-0.19561,   inf] (20), [-0.19535,   inf] (20), [-0.19373,   inf] (20), [-0.18757,   inf] (20), [-0.18670,   inf] (20), [-0.18624,   inf] (20), [-0.18602,   inf] (20), [-0.18113,   inf] (20), [-0.17931,   inf] (20), [-0.17792,   inf] (20), [-0.17717,   inf] (20), [-0.17518,   inf] (20), [-0.17416,   inf] (20), [-0.17393,   inf] (20), 
length of domains: 437
Total time: 1.7732	 pickout: 0.0710	 decision: 0.1232	 get_bound: 1.5462	 add_domain: 0.0328
Current lb:-0.2210051268339157
1864 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 12.429057598114014

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([437, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([437, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 979] [3, 987] [3, 987] [3, 557] [3, 980] [1, 35] [1, 35] [3, 979] [3, 979] [3, 557] 
regular batch size: 2*437, diving batch size 1*0
best_l after optimization: 37.83433151245117 with beta sum per layer: [0.0, 37.04377365112305, 1.4094767570495605, 201.59628295898438, 258.794921875, 639.702880859375]
alpha/beta optimization time: 1.66731858253479
This batch time : update_bounds func: 1.9824	 prepare: 0.1441	 bound: 1.6679	 transfer: 0.0648	 finalize: 0.1034
Accumulated time: update_bounds func: 31.6162	 prepare: 0.9760	 bound: 29.4819	 transfer: 0.0648	 finalize: 0.7901
batch bounding time:  1.9837138652801514
Current worst splitting domains [lb, ub] (depth):
[-0.21513,   inf] (22), [-0.21398,   inf] (22), [-0.19802,   inf] (22), [-0.19191,   inf] (22), [-0.19087,   inf] (22), [-0.18943,   inf] (22), [-0.18939,   inf] (22), [-0.18890,   inf] (22), [-0.18770,   inf] (22), [-0.18583,   inf] (22), [-0.18380,   inf] (22), [-0.18148,   inf] (22), [-0.17653,   inf] (22), [-0.17309,   inf] (22), [-0.17298,   inf] (22), [-0.17289,   inf] (22), [-0.17279,   inf] (22), [-0.17226,   inf] (22), [-0.17150,   inf] (22), [-0.17141,   inf] (22), 
length of domains: 696
Total time: 2.3706	 pickout: 0.1071	 decision: 0.2221	 get_bound: 1.9854	 add_domain: 0.0560
Current lb:-0.21512699127197266
2738 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 14.817678689956665

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([696, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([696, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 980] [3, 979] [3, 557] [3, 980] [3, 980] [3, 987] [3, 980] [3, 987] [3, 987] [3, 979] 
regular batch size: 2*696, diving batch size 1*0
best_l after optimization: 53.691383361816406 with beta sum per layer: [0.0, 61.60972595214844, 10.096555709838867, 346.8646545410156, 474.7498779296875, 1001.7493896484375]
alpha/beta optimization time: 2.078603982925415
This batch time : update_bounds func: 2.6358	 prepare: 0.2278	 bound: 2.0792	 transfer: 0.1005	 finalize: 0.2247
Accumulated time: update_bounds func: 34.2521	 prepare: 1.2038	 bound: 31.5611	 transfer: 0.1005	 finalize: 1.0149
batch bounding time:  2.63785719871521
Current worst splitting domains [lb, ub] (depth):
[-0.20816,   inf] (24), [-0.20120,   inf] (24), [-0.19988,   inf] (24), [-0.18671,   inf] (24), [-0.18622,   inf] (24), [-0.18415,   inf] (24), [-0.18361,   inf] (24), [-0.18223,   inf] (24), [-0.18193,   inf] (24), [-0.18108,   inf] (24), [-0.17990,   inf] (24), [-0.17927,   inf] (24), [-0.17836,   inf] (24), [-0.17781,   inf] (24), [-0.17558,   inf] (24), [-0.17379,   inf] (24), [-0.16687,   inf] (24), [-0.16685,   inf] (24), [-0.16675,   inf] (24), [-0.16637,   inf] (24), 
length of domains: 1097
Total time: 3.2734	 pickout: 0.1771	 decision: 0.3502	 get_bound: 2.6414	 add_domain: 0.1048
Current lb:-0.20815730094909668
4130 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 18.116471529006958

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1097, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1097, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 557] [3, 557] [3, 557] [4, 550] [4, 550] [4, 550] [4, 550] [1, 36] [1, 35] [1, 36] 
regular batch size: 2*1097, diving batch size 1*0
best_l after optimization: 71.06468963623047 with beta sum per layer: [0.0, 108.07694244384766, 32.251766204833984, 566.7635498046875, 810.4195556640625, 1527.0858154296875]
alpha/beta optimization time: 2.9258170127868652
This batch time : update_bounds func: 3.9293	 prepare: 0.4440	 bound: 2.9266	 transfer: 0.1718	 finalize: 0.3806
Accumulated time: update_bounds func: 38.1814	 prepare: 1.6478	 bound: 34.4877	 transfer: 0.1718	 finalize: 1.3954
batch bounding time:  3.9324214458465576
Current worst splitting domains [lb, ub] (depth):
[-0.19304,   inf] (26), [-0.19199,   inf] (26), [-0.18958,   inf] (26), [-0.18918,   inf] (26), [-0.18825,   inf] (26), [-0.18793,   inf] (26), [-0.17948,   inf] (26), [-0.17913,   inf] (26), [-0.17773,   inf] (26), [-0.17770,   inf] (26), [-0.17704,   inf] (26), [-0.17667,   inf] (26), [-0.17624,   inf] (26), [-0.17619,   inf] (26), [-0.17508,   inf] (26), [-0.17491,   inf] (26), [-0.17441,   inf] (26), [-0.17353,   inf] (26), [-0.17342,   inf] (26), [-0.17239,   inf] (26), 
length of domains: 1564
Total time: 4.8566	 pickout: 0.2727	 decision: 0.5123	 get_bound: 3.9375	 add_domain: 0.1340
Current lb:-0.19304287433624268
6324 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 23.018089532852173

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1564, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([1564, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 980] [3, 980] [4, 550] [4, 550] [4, 550] [4, 550] [5, 75] [5, 75] [3, 987] [3, 980] 
regular batch size: 2*1564, diving batch size 1*0
best_l after optimization: 102.17609405517578 with beta sum per layer: [0.0, 191.38909912109375, 70.6651840209961, 894.2262573242188, 1170.5047607421875, 1997.870361328125]
alpha/beta optimization time: 3.8298163414001465
This batch time : update_bounds func: 5.1859	 prepare: 0.5454	 bound: 3.8306	 transfer: 0.2395	 finalize: 0.5620
Accumulated time: update_bounds func: 43.3672	 prepare: 2.1932	 bound: 38.3183	 transfer: 0.2395	 finalize: 1.9574
batch bounding time:  5.190751075744629
Current worst splitting domains [lb, ub] (depth):
[-0.18267,   inf] (28), [-0.18235,   inf] (28), [-0.18139,   inf] (28), [-0.18113,   inf] (28), [-0.17934,   inf] (28), [-0.17835,   inf] (28), [-0.17820,   inf] (28), [-0.17758,   inf] (28), [-0.17716,   inf] (28), [-0.17662,   inf] (28), [-0.17661,   inf] (28), [-0.17551,   inf] (28), [-0.17351,   inf] (28), [-0.17315,   inf] (28), [-0.17297,   inf] (28), [-0.17142,   inf] (28), [-0.17116,   inf] (28), [-0.17076,   inf] (28), [-0.17021,   inf] (28), [-0.17006,   inf] (28), 
length of domains: 2252
Total time: 6.8159	 pickout: 0.5253	 decision: 0.7575	 get_bound: 5.1988	 add_domain: 0.3344
Current lb:-0.18266737461090088
9452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 29.899110078811646

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 75] [5, 75] [5, 75] [5, 75] [1, 36] [5, 75] [1, 36] [1, 36] [5, 75] [2, 404] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 148.93051147460938 with beta sum per layer: [0.0, 302.1600341796875, 127.97419738769531, 1133.852783203125, 1467.76904296875, 2263.595458984375]
alpha/beta optimization time: 4.830686092376709
This batch time : update_bounds func: 6.7208	 prepare: 0.7619	 bound: 4.8316	 transfer: 0.2970	 finalize: 0.8171
Accumulated time: update_bounds func: 50.0881	 prepare: 2.9551	 bound: 43.1499	 transfer: 0.2970	 finalize: 2.7745
batch bounding time:  6.726459980010986
Current worst splitting domains [lb, ub] (depth):
[-0.17665,   inf] (30), [-0.17635,   inf] (30), [-0.17539,   inf] (30), [-0.17514,   inf] (30), [-0.17473,   inf] (30), [-0.17328,   inf] (30), [-0.17314,   inf] (30), [-0.17231,   inf] (30), [-0.17195,   inf] (30), [-0.17111,   inf] (30), [-0.17093,   inf] (30), [-0.16988,   inf] (30), [-0.16969,   inf] (30), [-0.16765,   inf] (30), [-0.16741,   inf] (30), [-0.16698,   inf] (30), [-0.16477,   inf] (30), [-0.16356,   inf] (30), [-0.16353,   inf] (30), [-0.16331,   inf] (30), 
length of domains: 3425
Total time: 8.6588	 pickout: 0.5631	 decision: 1.0634	 get_bound: 6.7357	 add_domain: 0.2967
Current lb:-0.17665186524391174
13452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 38.63822364807129

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 923] [3, 923] [3, 923] [3, 923] [1, 292] [4, 550] [1, 292] [3, 923] [4, 550] [3, 923] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 215.51516723632812 with beta sum per layer: [0.0, 265.37322998046875, 153.531494140625, 1179.9398193359375, 1362.1669921875, 1692.3018798828125]
alpha/beta optimization time: 4.879562616348267
This batch time : update_bounds func: 6.6676	 prepare: 0.7206	 bound: 4.8805	 transfer: 0.3347	 finalize: 0.7203
Accumulated time: update_bounds func: 56.7556	 prepare: 3.6758	 bound: 48.0304	 transfer: 0.3347	 finalize: 3.4948
batch bounding time:  6.673077583312988
Current worst splitting domains [lb, ub] (depth):
[-0.17271,   inf] (32), [-0.17131,   inf] (32), [-0.17130,   inf] (32), [-0.17099,   inf] (32), [-0.17013,   inf] (32), [-0.16986,   inf] (32), [-0.16686,   inf] (32), [-0.16611,   inf] (32), [-0.16558,   inf] (32), [-0.16505,   inf] (32), [-0.16488,   inf] (32), [-0.16384,   inf] (32), [-0.16282,   inf] (32), [-0.16266,   inf] (32), [-0.16163,   inf] (32), [-0.16129,   inf] (32), [-0.16071,   inf] (32), [-0.15973,   inf] (32), [-0.15909,   inf] (32), [-0.15853,   inf] (32), 
length of domains: 5112
Total time: 8.9954	 pickout: 0.6454	 decision: 1.0918	 get_bound: 6.6828	 add_domain: 0.5754
Current lb:-0.1727064698934555
17452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 47.70937156677246

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 550] [4, 550] [4, 525] [4, 525] [4, 525] [4, 525] [4, 525] [1, 292] [4, 525] [1, 292] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 266.1480712890625 with beta sum per layer: [0.0, 256.19158935546875, 148.72048950195312, 1194.412353515625, 1275.8057861328125, 1078.6907958984375]
alpha/beta optimization time: 4.874351739883423
This batch time : update_bounds func: 6.6903	 prepare: 0.6895	 bound: 4.8750	 transfer: 0.3311	 finalize: 0.7831
Accumulated time: update_bounds func: 63.4460	 prepare: 4.3652	 bound: 52.9054	 transfer: 0.3311	 finalize: 4.2779
batch bounding time:  6.6957056522369385
Current worst splitting domains [lb, ub] (depth):
[-0.16547,   inf] (34), [-0.16484,   inf] (34), [-0.16454,   inf] (34), [-0.16430,   inf] (34), [-0.16429,   inf] (34), [-0.16351,   inf] (34), [-0.16338,   inf] (34), [-0.16329,   inf] (34), [-0.16327,   inf] (34), [-0.16299,   inf] (34), [-0.16271,   inf] (34), [-0.16213,   inf] (34), [-0.16182,   inf] (34), [-0.16045,   inf] (34), [-0.16035,   inf] (34), [-0.15982,   inf] (34), [-0.15912,   inf] (34), [-0.15895,   inf] (34), [-0.15888,   inf] (34), [-0.15874,   inf] (34), 
length of domains: 6885
Total time: 9.1307	 pickout: 0.6324	 decision: 1.0245	 get_bound: 6.7050	 add_domain: 0.7687
Current lb:-0.16547103226184845
21452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 56.95666861534119

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [5, 75] [1, 35] [1, 35] [5, 75] [5, 75] [2, 404] [5, 75] [1, 35] [2, 404] [1, 604] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 321.58294677734375 with beta sum per layer: [0.0, 290.6140441894531, 176.39405822753906, 976.3551025390625, 1157.567626953125, 740.4833984375]
alpha/beta optimization time: 4.855126142501831
This batch time : update_bounds func: 6.7722	 prepare: 0.7254	 bound: 4.8558	 transfer: 0.3241	 finalize: 0.8553
Accumulated time: update_bounds func: 70.2182	 prepare: 5.0907	 bound: 57.7612	 transfer: 0.3241	 finalize: 5.1332
batch bounding time:  6.777737379074097
Current worst splitting domains [lb, ub] (depth):
[-0.16165,   inf] (36), [-0.16121,   inf] (36), [-0.16005,   inf] (36), [-0.15947,   inf] (36), [-0.15894,   inf] (36), [-0.15830,   inf] (36), [-0.15822,   inf] (36), [-0.15773,   inf] (36), [-0.15749,   inf] (36), [-0.15732,   inf] (36), [-0.15705,   inf] (36), [-0.15670,   inf] (36), [-0.15645,   inf] (36), [-0.15622,   inf] (36), [-0.15572,   inf] (36), [-0.15497,   inf] (36), [-0.15439,   inf] (36), [-0.15380,   inf] (36), [-0.15365,   inf] (36), [-0.15345,   inf] (36), 
length of domains: 8825
Total time: 8.9783	 pickout: 0.6906	 decision: 1.0955	 get_bound: 6.7871	 add_domain: 0.4051
Current lb:-0.16164900362491608
25452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 66.00905847549438

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 475] [4, 475] [1, 604] [3, 923] [1, 35] [2, 404] [1, 604] [1, 35] [1, 35] [2, 404] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 358.2106628417969 with beta sum per layer: [0.0, 291.49822998046875, 185.148193359375, 640.419921875, 1092.149169921875, 672.48046875]
alpha/beta optimization time: 4.939007520675659
This batch time : update_bounds func: 6.9418	 prepare: 0.7001	 bound: 4.9398	 transfer: 0.3236	 finalize: 0.9670
Accumulated time: update_bounds func: 77.1600	 prepare: 5.7907	 bound: 62.7010	 transfer: 0.3236	 finalize: 6.1002
batch bounding time:  6.947473049163818
Current worst splitting domains [lb, ub] (depth):
[-0.15699,   inf] (38), [-0.15661,   inf] (38), [-0.15590,   inf] (38), [-0.15556,   inf] (38), [-0.15426,   inf] (38), [-0.15422,   inf] (38), [-0.15394,   inf] (38), [-0.15379,   inf] (38), [-0.15295,   inf] (38), [-0.15285,   inf] (38), [-0.15234,   inf] (38), [-0.15215,   inf] (38), [-0.15207,   inf] (38), [-0.15168,   inf] (38), [-0.15132,   inf] (38), [-0.15090,   inf] (38), [-0.15060,   inf] (38), [-0.15041,   inf] (38), [-0.14960,   inf] (38), [-0.14913,   inf] (38), 
length of domains: 10807
Total time: 9.0313	 pickout: 0.5708	 decision: 1.0748	 get_bound: 6.9601	 add_domain: 0.4256
Current lb:-0.15699417889118195
29452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 75.12384939193726

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 476] [1, 604] [4, 475] [4, 475] [2, 476] [2, 476] [1, 604] [1, 604] [1, 604] [1, 35] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 378.0970764160156 with beta sum per layer: [0.0, 313.7999267578125, 163.274169921875, 570.5466918945312, 962.266845703125, 579.3648681640625]
alpha/beta optimization time: 4.851177930831909
This batch time : update_bounds func: 6.8261	 prepare: 0.7059	 bound: 4.8519	 transfer: 0.2980	 finalize: 0.9587
Accumulated time: update_bounds func: 83.9861	 prepare: 6.4966	 bound: 67.5529	 transfer: 0.2980	 finalize: 7.0590
batch bounding time:  6.831746339797974
Current worst splitting domains [lb, ub] (depth):
[-0.15274,   inf] (40), [-0.15131,   inf] (40), [-0.15099,   inf] (40), [-0.15089,   inf] (40), [-0.15027,   inf] (40), [-0.14978,   inf] (40), [-0.14964,   inf] (40), [-0.14945,   inf] (40), [-0.14935,   inf] (40), [-0.14840,   inf] (40), [-0.14823,   inf] (40), [-0.14765,   inf] (40), [-0.14737,   inf] (40), [-0.14706,   inf] (40), [-0.14704,   inf] (40), [-0.14668,   inf] (40), [-0.14629,   inf] (40), [-0.14620,   inf] (40), [-0.14607,   inf] (40), [-0.14579,   inf] (40), 
length of domains: 12806
Total time: 9.1684	 pickout: 0.6700	 decision: 1.2182	 get_bound: 6.8415	 add_domain: 0.4387
Current lb:-0.15274250507354736
33452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 84.37576007843018

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 476] [2, 476] [2, 476] [4, 475] [1, 604] [4, 525] [4, 475] [4, 475] [1, 604] [3, 923] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 392.943603515625 with beta sum per layer: [0.0, 323.88763427734375, 143.21051025390625, 542.6393432617188, 954.8948974609375, 421.67938232421875]
alpha/beta optimization time: 4.853085041046143
This batch time : update_bounds func: 7.0693	 prepare: 0.7319	 bound: 4.8538	 transfer: 0.3187	 finalize: 1.1541
Accumulated time: update_bounds func: 91.0554	 prepare: 7.2285	 bound: 72.4067	 transfer: 0.3187	 finalize: 8.2131
batch bounding time:  7.074882984161377
Current worst splitting domains [lb, ub] (depth):
[-0.14642,   inf] (42), [-0.14640,   inf] (42), [-0.14609,   inf] (42), [-0.14549,   inf] (42), [-0.14534,   inf] (42), [-0.14522,   inf] (42), [-0.14504,   inf] (42), [-0.14482,   inf] (42), [-0.14444,   inf] (42), [-0.14374,   inf] (42), [-0.14367,   inf] (42), [-0.14366,   inf] (42), [-0.14347,   inf] (42), [-0.14345,   inf] (42), [-0.14320,   inf] (42), [-0.14299,   inf] (42), [-0.14175,   inf] (42), [-0.14131,   inf] (42), [-0.14124,   inf] (42), [-0.14118,   inf] (42), 
length of domains: 14806
Total time: 9.6418	 pickout: 0.7662	 decision: 1.3388	 get_bound: 7.0842	 add_domain: 0.4527
Current lb:-0.14641845226287842
37452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 94.09794068336487

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 212] [4, 483] [5, 46] [4, 483] [5, 46] [4, 212] [4, 212] [4, 212] [5, 46] [4, 525] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 396.35589599609375 with beta sum per layer: [0.0, 346.9249267578125, 136.3304443359375, 500.50048828125, 982.83349609375, 301.0171203613281]
alpha/beta optimization time: 4.853719472885132
This batch time : update_bounds func: 7.0185	 prepare: 0.7198	 bound: 4.8544	 transfer: 0.3226	 finalize: 0.4884
Accumulated time: update_bounds func: 98.0739	 prepare: 7.9483	 bound: 77.2611	 transfer: 0.3226	 finalize: 8.7015
batch bounding time:  7.024103164672852
Current worst splitting domains [lb, ub] (depth):
[-0.14213,   inf] (44), [-0.14135,   inf] (44), [-0.14108,   inf] (44), [-0.14044,   inf] (44), [-0.14031,   inf] (44), [-0.14023,   inf] (44), [-0.14004,   inf] (44), [-0.13994,   inf] (44), [-0.13975,   inf] (44), [-0.13962,   inf] (44), [-0.13959,   inf] (44), [-0.13947,   inf] (44), [-0.13915,   inf] (44), [-0.13884,   inf] (44), [-0.13841,   inf] (44), [-0.13713,   inf] (44), [-0.13713,   inf] (44), [-0.13639,   inf] (44), [-0.13623,   inf] (44), [-0.13586,   inf] (44), 
length of domains: 16806
Total time: 8.8249	 pickout: 0.5751	 decision: 0.7340	 get_bound: 7.0344	 add_domain: 0.4815
Current lb:-0.14212624728679657
41452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 103.0338454246521

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 656] [4, 656] [5, 46] [4, 656] [4, 212] [4, 212] [4, 212] [5, 46] [5, 46] [4, 212] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 390.6790771484375 with beta sum per layer: [0.0, 331.5494079589844, 132.7153778076172, 447.17822265625, 1036.1646728515625, 271.73333740234375]
alpha/beta optimization time: 4.85713791847229
This batch time : update_bounds func: 7.1897	 prepare: 0.7324	 bound: 4.8578	 transfer: 0.3307	 finalize: 1.2569
Accumulated time: update_bounds func: 105.2636	 prepare: 8.6808	 bound: 82.1189	 transfer: 0.3307	 finalize: 9.9584
batch bounding time:  7.195235729217529
Current worst splitting domains [lb, ub] (depth):
[-0.13964,   inf] (46), [-0.13884,   inf] (46), [-0.13796,   inf] (46), [-0.13702,   inf] (46), [-0.13697,   inf] (46), [-0.13579,   inf] (46), [-0.13563,   inf] (46), [-0.13552,   inf] (46), [-0.13516,   inf] (46), [-0.13487,   inf] (46), [-0.13483,   inf] (46), [-0.13478,   inf] (46), [-0.13438,   inf] (46), [-0.13430,   inf] (46), [-0.13409,   inf] (46), [-0.13374,   inf] (46), [-0.13322,   inf] (46), [-0.13308,   inf] (46), [-0.13276,   inf] (46), [-0.13259,   inf] (46), 
length of domains: 18806
Total time: 9.7691	 pickout: 0.5959	 decision: 1.4925	 get_bound: 7.2047	 add_domain: 0.4761
Current lb:-0.13963818550109863
45452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 112.8851010799408

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 212] [4, 483] [4, 212] [4, 656] [4, 212] [4, 656] [4, 656] [4, 656] [4, 656] [5, 46] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 385.00067138671875 with beta sum per layer: [0.0, 302.8696594238281, 135.4853515625, 395.86395263671875, 1134.3306884765625, 211.97369384765625]
alpha/beta optimization time: 4.855980634689331
This batch time : update_bounds func: 7.2708	 prepare: 0.7257	 bound: 4.8566	 transfer: 0.3116	 finalize: 1.3660
Accumulated time: update_bounds func: 112.5344	 prepare: 9.4064	 bound: 86.9755	 transfer: 0.3116	 finalize: 11.3244
batch bounding time:  7.2766313552856445
Current worst splitting domains [lb, ub] (depth):
[-0.13464,   inf] (48), [-0.13432,   inf] (48), [-0.13343,   inf] (48), [-0.13321,   inf] (48), [-0.13311,   inf] (48), [-0.13274,   inf] (48), [-0.13269,   inf] (48), [-0.13255,   inf] (48), [-0.13234,   inf] (48), [-0.13211,   inf] (48), [-0.13188,   inf] (48), [-0.13168,   inf] (48), [-0.13078,   inf] (48), [-0.13069,   inf] (48), [-0.13051,   inf] (48), [-0.13032,   inf] (48), [-0.12996,   inf] (48), [-0.12968,   inf] (48), [-0.12959,   inf] (48), [-0.12916,   inf] (48), 
length of domains: 20806
Total time: 9.1040	 pickout: 0.5805	 decision: 0.7391	 get_bound: 7.2862	 add_domain: 0.4983
Current lb:-0.1346447467803955
49452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 122.07939600944519

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 483] [4, 483] [2, 476] [4, 483] [4, 483] [4, 483] [4, 212] [4, 212] [4, 212] [4, 212] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 380.78216552734375 with beta sum per layer: [0.0, 288.24029541015625, 130.72161865234375, 324.89959716796875, 1203.58154296875, 171.98190307617188]
alpha/beta optimization time: 4.832384347915649
This batch time : update_bounds func: 7.1726	 prepare: 0.7259	 bound: 4.8331	 transfer: 0.2831	 finalize: 1.3192
Accumulated time: update_bounds func: 119.7070	 prepare: 10.1323	 bound: 91.8086	 transfer: 0.2831	 finalize: 12.6435
batch bounding time:  7.178400993347168
Current worst splitting domains [lb, ub] (depth):
[-0.12843,   inf] (50), [-0.12837,   inf] (50), [-0.12830,   inf] (50), [-0.12808,   inf] (50), [-0.12799,   inf] (50), [-0.12790,   inf] (50), [-0.12750,   inf] (50), [-0.12745,   inf] (50), [-0.12738,   inf] (50), [-0.12718,   inf] (50), [-0.12713,   inf] (50), [-0.12712,   inf] (50), [-0.12702,   inf] (50), [-0.12700,   inf] (50), [-0.12681,   inf] (50), [-0.12678,   inf] (50), [-0.12659,   inf] (50), [-0.12651,   inf] (50), [-0.12649,   inf] (50), [-0.12634,   inf] (50), 
length of domains: 22806
Total time: 9.0445	 pickout: 0.6177	 decision: 0.7396	 get_bound: 7.1875	 add_domain: 0.4998
Current lb:-0.1284283846616745
53452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 131.2360155582428

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 483] [4, 476] [2, 476] [4, 476] [4, 483] [4, 434] [2, 476] [4, 476] [4, 483] [5, 46] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 373.6201171875 with beta sum per layer: [0.0, 255.1142578125, 139.3648223876953, 291.8145446777344, 1190.036376953125, 164.06597900390625]
alpha/beta optimization time: 4.8345794677734375
This batch time : update_bounds func: 7.2695	 prepare: 0.7267	 bound: 4.8352	 transfer: 0.3313	 finalize: 0.4699
Accumulated time: update_bounds func: 126.9765	 prepare: 10.8591	 bound: 96.6438	 transfer: 0.3313	 finalize: 13.1134
batch bounding time:  7.275190114974976
Current worst splitting domains [lb, ub] (depth):
[-0.12429,   inf] (52), [-0.12402,   inf] (52), [-0.12384,   inf] (52), [-0.12338,   inf] (52), [-0.12304,   inf] (52), [-0.12297,   inf] (52), [-0.12296,   inf] (52), [-0.12289,   inf] (52), [-0.12274,   inf] (52), [-0.12272,   inf] (52), [-0.12257,   inf] (52), [-0.12246,   inf] (52), [-0.12244,   inf] (52), [-0.12225,   inf] (52), [-0.12216,   inf] (52), [-0.12214,   inf] (52), [-0.12205,   inf] (52), [-0.12184,   inf] (52), [-0.12177,   inf] (52), [-0.12168,   inf] (52), 
length of domains: 24800
Total time: 9.1806	 pickout: 0.6102	 decision: 0.7462	 get_bound: 7.2852	 add_domain: 0.5390
Current lb:-0.12429416179656982
57452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 140.51686000823975

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 434] [4, 434] [4, 492] [4, 434] [2, 476] [4, 483] [4, 483] [4, 434] [4, 434] [4, 492] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 368.281494140625 with beta sum per layer: [0.0, 242.39508056640625, 141.04217529296875, 261.7427978515625, 1265.0516357421875, 113.84812927246094]
alpha/beta optimization time: 4.830968379974365
This batch time : update_bounds func: 7.4344	 prepare: 0.7418	 bound: 4.8317	 transfer: 0.3253	 finalize: 0.4874
Accumulated time: update_bounds func: 134.4109	 prepare: 11.6008	 bound: 101.4756	 transfer: 0.3253	 finalize: 13.6008
batch bounding time:  7.439812421798706
Current worst splitting domains [lb, ub] (depth):
[-0.12029,   inf] (54), [-0.12008,   inf] (54), [-0.11998,   inf] (54), [-0.11946,   inf] (54), [-0.11940,   inf] (54), [-0.11898,   inf] (54), [-0.11885,   inf] (54), [-0.11879,   inf] (54), [-0.11873,   inf] (54), [-0.11846,   inf] (54), [-0.11842,   inf] (54), [-0.11822,   inf] (54), [-0.11780,   inf] (54), [-0.11759,   inf] (54), [-0.11753,   inf] (54), [-0.11751,   inf] (54), [-0.11741,   inf] (54), [-0.11734,   inf] (54), [-0.11721,   inf] (54), [-0.11699,   inf] (54), 
length of domains: 26788
Total time: 9.3516	 pickout: 0.5893	 decision: 0.7559	 get_bound: 7.4514	 add_domain: 0.5551
Current lb:-0.12028618156909943
61452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 149.9834861755371

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 492] [4, 476] [4, 492] [4, 476] [4, 492] [4, 476] [4, 492] [4, 476] [4, 492] [4, 492] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 358.83245849609375 with beta sum per layer: [0.0, 238.38760375976562, 143.27328491210938, 250.8988037109375, 1284.716796875, 99.98480987548828]
alpha/beta optimization time: 4.846431255340576
This batch time : update_bounds func: 7.8207	 prepare: 0.7761	 bound: 4.8472	 transfer: 0.3250	 finalize: 1.8593
Accumulated time: update_bounds func: 142.2316	 prepare: 12.3770	 bound: 106.3228	 transfer: 0.3250	 finalize: 15.4601
batch bounding time:  7.826364517211914
Current worst splitting domains [lb, ub] (depth):
[-0.11648,   inf] (56), [-0.11617,   inf] (56), [-0.11596,   inf] (56), [-0.11563,   inf] (56), [-0.11550,   inf] (56), [-0.11507,   inf] (56), [-0.11499,   inf] (56), [-0.11476,   inf] (56), [-0.11476,   inf] (56), [-0.11470,   inf] (56), [-0.11462,   inf] (56), [-0.11446,   inf] (56), [-0.11410,   inf] (56), [-0.11382,   inf] (56), [-0.11369,   inf] (56), [-0.11364,   inf] (56), [-0.11346,   inf] (56), [-0.11325,   inf] (56), [-0.11319,   inf] (56), [-0.11311,   inf] (56), 
length of domains: 28737
Total time: 9.7462	 pickout: 0.6228	 decision: 0.7539	 get_bound: 7.8363	 add_domain: 0.5332
Current lb:-0.11648225784301758
65452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 159.8597207069397

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 28] [1, 28] [4, 537] [1, 28] [4, 434] [2, 732] [1, 28] [4, 434] [4, 537] [4, 537] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 356.3213195800781 with beta sum per layer: [0.0, 228.55422973632812, 144.9842987060547, 286.59979248046875, 1211.4996337890625, 115.65570068359375]
alpha/beta optimization time: 4.827965021133423
This batch time : update_bounds func: 7.8910	 prepare: 0.7903	 bound: 4.8287	 transfer: 0.3250	 finalize: 1.9350
Accumulated time: update_bounds func: 150.1226	 prepare: 13.1673	 bound: 111.1515	 transfer: 0.3250	 finalize: 17.3951
batch bounding time:  7.897478818893433
Current worst splitting domains [lb, ub] (depth):
[-0.11226,   inf] (58), [-0.11198,   inf] (58), [-0.11152,   inf] (58), [-0.11143,   inf] (58), [-0.11134,   inf] (58), [-0.11126,   inf] (58), [-0.11078,   inf] (58), [-0.11070,   inf] (58), [-0.11044,   inf] (58), [-0.11040,   inf] (58), [-0.11026,   inf] (58), [-0.11022,   inf] (58), [-0.11018,   inf] (58), [-0.11010,   inf] (58), [-0.11008,   inf] (58), [-0.11004,   inf] (58), [-0.11002,   inf] (58), [-0.10991,   inf] (58), [-0.10987,   inf] (58), [-0.10985,   inf] (58), 
length of domains: 30720
Total time: 9.9098	 pickout: 0.6817	 decision: 0.7755	 get_bound: 7.9079	 add_domain: 0.5447
Current lb:-0.1122584342956543
69452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 169.90664887428284

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [2, 732] [2, 732] [4, 537] [4, 492] [1, 28] [3, 659] [4, 492] [2, 732] [2, 732] [2, 732] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 348.6375732421875 with beta sum per layer: [0.0, 213.38699340820312, 154.03109741210938, 297.7413024902344, 1183.68798828125, 123.6211166381836]
alpha/beta optimization time: 4.8402509689331055
This batch time : update_bounds func: 8.0233	 prepare: 0.7631	 bound: 4.8410	 transfer: 0.3221	 finalize: 2.0837
Accumulated time: update_bounds func: 158.1460	 prepare: 13.9304	 bound: 115.9925	 transfer: 0.3221	 finalize: 19.4788
batch bounding time:  8.028984785079956
Current worst splitting domains [lb, ub] (depth):
[-0.10866,   inf] (60), [-0.10830,   inf] (60), [-0.10805,   inf] (60), [-0.10763,   inf] (60), [-0.10751,   inf] (60), [-0.10749,   inf] (60), [-0.10709,   inf] (60), [-0.10701,   inf] (60), [-0.10692,   inf] (60), [-0.10670,   inf] (60), [-0.10656,   inf] (60), [-0.10641,   inf] (60), [-0.10632,   inf] (60), [-0.10630,   inf] (60), [-0.10609,   inf] (60), [-0.10605,   inf] (60), [-0.10583,   inf] (60), [-0.10577,   inf] (60), [-0.10577,   inf] (60), [-0.10567,   inf] (60), 
length of domains: 32688
Total time: 10.0511	 pickout: 0.7045	 decision: 0.7648	 get_bound: 8.0386	 add_domain: 0.5432
Current lb:-0.10865885019302368
73452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 180.06356692314148

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 537] [3, 659] [3, 659] [2, 844] [2, 732] [3, 660] [2, 732] [2, 844] [3, 659] [3, 659] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 339.45068359375 with beta sum per layer: [0.0, 246.59915161132812, 153.3423309326172, 317.84967041015625, 1175.9599609375, 162.26840209960938]
alpha/beta optimization time: 4.84154486656189
This batch time : update_bounds func: 6.4040	 prepare: 0.7460	 bound: 4.8422	 transfer: 0.3025	 finalize: 0.5000
Accumulated time: update_bounds func: 164.5500	 prepare: 14.6763	 bound: 120.8347	 transfer: 0.3025	 finalize: 19.9788
batch bounding time:  6.410238981246948
Current worst splitting domains [lb, ub] (depth):
[-0.10587,   inf] (62), [-0.10560,   inf] (62), [-0.10506,   inf] (62), [-0.10448,   inf] (62), [-0.10427,   inf] (62), [-0.10410,   inf] (62), [-0.10396,   inf] (62), [-0.10392,   inf] (62), [-0.10382,   inf] (62), [-0.10376,   inf] (62), [-0.10367,   inf] (62), [-0.10331,   inf] (62), [-0.10325,   inf] (62), [-0.10315,   inf] (62), [-0.10315,   inf] (62), [-0.10306,   inf] (62), [-0.10298,   inf] (62), [-0.10262,   inf] (62), [-0.10258,   inf] (62), [-0.10253,   inf] (62), 
length of domains: 34667
Total time: 8.4765	 pickout: 0.7386	 decision: 0.7669	 get_bound: 6.4206	 add_domain: 0.5505
Current lb:-0.1058729887008667
77452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 188.67971539497375

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 786] [4, 537] [4, 537] [1, 995] [3, 660] [4, 786] [3, 660] [1, 995] [4, 537] [1, 36] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 333.4477233886719 with beta sum per layer: [0.0, 268.7265625, 161.1919403076172, 353.99853515625, 1200.86865234375, 188.95159912109375]
alpha/beta optimization time: 4.835797548294067
This batch time : update_bounds func: 6.4529	 prepare: 0.7434	 bound: 4.8365	 transfer: 0.3264	 finalize: 0.5332
Accumulated time: update_bounds func: 171.0029	 prepare: 15.4197	 bound: 125.6712	 transfer: 0.3264	 finalize: 20.5120
batch bounding time:  6.458899021148682
Current worst splitting domains [lb, ub] (depth):
[-0.10257,   inf] (64), [-0.10193,   inf] (64), [-0.10161,   inf] (64), [-0.10086,   inf] (64), [-0.10079,   inf] (64), [-0.10076,   inf] (64), [-0.10052,   inf] (64), [-0.10022,   inf] (64), [-0.10010,   inf] (64), [-0.10007,   inf] (64), [-0.09991,   inf] (64), [-0.09989,   inf] (64), [-0.09982,   inf] (64), [-0.09973,   inf] (64), [-0.09969,   inf] (64), [-0.09957,   inf] (64), [-0.09940,   inf] (64), [-0.09931,   inf] (64), [-0.09927,   inf] (64), [-0.09924,   inf] (64), 
length of domains: 36644
Total time: 10.0505	 pickout: 0.7355	 decision: 2.2578	 get_bound: 6.4709	 add_domain: 0.5864
Current lb:-0.102569580078125
81452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 198.85844349861145

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 537] [4, 786] [4, 537] [4, 786] [3, 995] [4, 537] [1, 995] [3, 995] [4, 537] [1, 995] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 331.1770324707031 with beta sum per layer: [0.0, 276.4373474121094, 167.87001037597656, 354.83734130859375, 1212.391845703125, 215.3057098388672]
alpha/beta optimization time: 4.845836877822876
This batch time : update_bounds func: 8.0821	 prepare: 0.7427	 bound: 4.8465	 transfer: 0.2993	 finalize: 2.1802
Accumulated time: update_bounds func: 179.0851	 prepare: 16.1624	 bound: 130.5177	 transfer: 0.2993	 finalize: 22.6922
batch bounding time:  8.088311195373535
Current worst splitting domains [lb, ub] (depth):
[-0.09861,   inf] (66), [-0.09783,   inf] (66), [-0.09778,   inf] (66), [-0.09757,   inf] (66), [-0.09688,   inf] (66), [-0.09688,   inf] (66), [-0.09677,   inf] (66), [-0.09604,   inf] (66), [-0.09601,   inf] (66), [-0.09597,   inf] (66), [-0.09592,   inf] (66), [-0.09577,   inf] (66), [-0.09568,   inf] (66), [-0.09558,   inf] (66), [-0.09542,   inf] (66), [-0.09529,   inf] (66), [-0.09528,   inf] (66), [-0.09528,   inf] (66), [-0.09528,   inf] (66), [-0.09526,   inf] (66), 
length of domains: 38627
Total time: 10.1750	 pickout: 0.7480	 decision: 0.7645	 get_bound: 8.0988	 add_domain: 0.5637
Current lb:-0.09860849380493164
85452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 209.14411306381226

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 537] [1, 9] [4, 786] [1, 9] [3, 995] [4, 786] [1, 9] [4, 786] [4, 786] [3, 995] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 325.8323974609375 with beta sum per layer: [0.0, 286.1674499511719, 166.17469787597656, 383.357666015625, 1266.4853515625, 234.59930419921875]
alpha/beta optimization time: 4.857213735580444
This batch time : update_bounds func: 6.4389	 prepare: 0.7550	 bound: 4.8579	 transfer: 0.3049	 finalize: 0.5073
Accumulated time: update_bounds func: 185.5239	 prepare: 16.9174	 bound: 135.3756	 transfer: 0.3049	 finalize: 23.1995
batch bounding time:  6.445061206817627
Current worst splitting domains [lb, ub] (depth):
[-0.09567,   inf] (68), [-0.09542,   inf] (68), [-0.09465,   inf] (68), [-0.09434,   inf] (68), [-0.09387,   inf] (68), [-0.09361,   inf] (68), [-0.09358,   inf] (68), [-0.09309,   inf] (68), [-0.09289,   inf] (68), [-0.09283,   inf] (68), [-0.09282,   inf] (68), [-0.09281,   inf] (68), [-0.09276,   inf] (68), [-0.09255,   inf] (68), [-0.09252,   inf] (58), [-0.09252,   inf] (60), [-0.09252,   inf] (58), [-0.09252,   inf] (46), [-0.09252,   inf] (52), [-0.09252,   inf] (62), 
length of domains: 40615
Total time: 8.5441	 pickout: 0.7503	 decision: 0.7884	 get_bound: 6.4554	 add_domain: 0.5500
Current lb:-0.09566664695739746
89452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 217.81629586219788

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 597] [1, 597] [1, 995] [1, 597] [1, 9] [3, 995] [1, 9] [1, 28] [4, 794] [4, 786] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 321.6091613769531 with beta sum per layer: [0.0, 310.1380615234375, 172.3323974609375, 384.88385009765625, 1321.8642578125, 227.65823364257812]
alpha/beta optimization time: 4.8685078620910645
This batch time : update_bounds func: 6.4319	 prepare: 0.7445	 bound: 4.8692	 transfer: 0.3048	 finalize: 0.4998
Accumulated time: update_bounds func: 191.9558	 prepare: 17.6618	 bound: 140.2447	 transfer: 0.3048	 finalize: 23.6993
batch bounding time:  6.437618255615234
Current worst splitting domains [lb, ub] (depth):
[-0.09322,   inf] (70), [-0.09297,   inf] (70), [-0.09191,   inf] (70), [-0.09175,   inf] (48), [-0.09175,   inf] (48), [-0.09175,   inf] (52), [-0.09175,   inf] (56), [-0.09175,   inf] (58), [-0.09175,   inf] (42), [-0.09175,   inf] (60), [-0.09175,   inf] (58), [-0.09175,   inf] (42), [-0.09175,   inf] (56), [-0.09175,   inf] (50), [-0.09175,   inf] (56), [-0.09175,   inf] (44), [-0.09175,   inf] (42), [-0.09175,   inf] (50), [-0.09174,   inf] (40), [-0.09174,   inf] (54), 
length of domains: 42593
Total time: 10.3084	 pickout: 0.7241	 decision: 2.5822	 get_bound: 6.4478	 add_domain: 0.5543
Current lb:-0.0932244285941124
93452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 228.24561262130737

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 995] [1, 995] [1, 995] [1, 28] [4, 475] [5, 46] [4, 537] [4, 492] [2, 404] [4, 786] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 318.12225341796875 with beta sum per layer: [0.0, 306.2420959472656, 174.83758544921875, 415.45562744140625, 1353.7850341796875, 213.39146423339844]
alpha/beta optimization time: 4.867552995681763
This batch time : update_bounds func: 8.4835	 prepare: 0.7446	 bound: 4.8683	 transfer: 0.3189	 finalize: 2.5382
Accumulated time: update_bounds func: 200.4393	 prepare: 18.4064	 bound: 145.1130	 transfer: 0.3189	 finalize: 26.2375
batch bounding time:  8.489313125610352
Current worst splitting domains [lb, ub] (depth):
[-0.09107,   inf] (48), [-0.09107,   inf] (52), [-0.09107,   inf] (54), [-0.09107,   inf] (60), [-0.09107,   inf] (56), [-0.09107,   inf] (44), [-0.09107,   inf] (50), [-0.09107,   inf] (60), [-0.09107,   inf] (50), [-0.09107,   inf] (50), [-0.09107,   inf] (44), [-0.09107,   inf] (50), [-0.09107,   inf] (64), [-0.09107,   inf] (62), [-0.09107,   inf] (60), [-0.09107,   inf] (42), [-0.09107,   inf] (54), [-0.09107,   inf] (52), [-0.09107,   inf] (52), [-0.09107,   inf] (48), 
length of domains: 44570
Total time: 10.6227	 pickout: 0.7854	 decision: 0.7808	 get_bound: 8.4993	 add_domain: 0.5572
Current lb:-0.09107208251953125
97452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 238.9853971004486

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [3, 979] [5, 46] [4, 434] [1, 995] [4, 537] [4, 656] [3, 653] [4, 656] [5, 46] [3, 653] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 316.27069091796875 with beta sum per layer: [0.0, 305.77093505859375, 172.92135620117188, 401.6649169921875, 1362.2901611328125, 224.9890594482422]
alpha/beta optimization time: 4.847135782241821
This batch time : update_bounds func: 6.4236	 prepare: 0.7495	 bound: 4.8479	 transfer: 0.3149	 finalize: 0.4969
Accumulated time: update_bounds func: 206.8629	 prepare: 19.1560	 bound: 149.9608	 transfer: 0.3149	 finalize: 26.7344
batch bounding time:  6.429477214813232
Current worst splitting domains [lb, ub] (depth):
[-0.09045,   inf] (58), [-0.09045,   inf] (44), [-0.09045,   inf] (58), [-0.09045,   inf] (50), [-0.09045,   inf] (44), [-0.09044,   inf] (60), [-0.09044,   inf] (50), [-0.09044,   inf] (56), [-0.09044,   inf] (52), [-0.09044,   inf] (58), [-0.09044,   inf] (56), [-0.09044,   inf] (58), [-0.09044,   inf] (48), [-0.09044,   inf] (42), [-0.09044,   inf] (52), [-0.09044,   inf] (48), [-0.09044,   inf] (56)/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:25: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1627336316785/work/torch/csrc/utils/tensor_numpy.cpp:143.)
  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/convert/model.py:105: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not self.experimental and input[0].shape[self.batch_dim] > 1:
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/utils.py:21: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return value.ndim == 0 or value.shape == torch.Size([1])
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/operations/add.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  elif all(x == 1 for x in input[0].shape):
, [-0.09044,   inf] (62), [-0.09044,   inf] (44), [-0.09044,   inf] (56), 
length of domains: 46542
Total time: 8.5409	 pickout: 0.7618	 decision: 0.7831	 get_bound: 6.4400	 add_domain: 0.5560
Current lb:-0.0904456228017807
101452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 247.64322757720947

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 476] [3, 653] [4, 492] [1, 28] [1, 35] [1, 597] [1, 36] [4, 492] [5, 46] [4, 476] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 313.0160217285156 with beta sum per layer: [0.0, 303.3706359863281, 170.52825927734375, 375.212890625, 1326.609375, 240.23342895507812]
alpha/beta optimization time: 4.847135305404663
This batch time : update_bounds func: 8.4588	 prepare: 0.7364	 bound: 4.8478	 transfer: 0.3041	 finalize: 2.5560
Accumulated time: update_bounds func: 215.3216	 prepare: 19.8924	 bound: 154.8086	 transfer: 0.3041	 finalize: 29.2905
batch bounding time:  8.464600324630737
Current worst splitting domains [lb, ub] (depth):
[-0.08984,   inf] (50), [-0.08984,   inf] (52), [-0.08984,   inf] (56), [-0.08984,   inf] (48), [-0.08984,   inf] (62), [-0.08984,   inf] (42), [-0.08984,   inf] (48), [-0.08984,   inf] (60), [-0.08984,   inf] (62), [-0.08984,   inf] (56), [-0.08984,   inf] (46), [-0.08983,   inf] (56), [-0.08983,   inf] (58), [-0.08983,   inf] (52), [-0.08983,   inf] (48), [-0.08983,   inf] (48), [-0.08983,   inf] (54), [-0.08983,   inf] (42), [-0.08983,   inf] (68), [-0.08983,   inf] (58), 
length of domains: 48512
Total time: 10.5644	 pickout: 0.7494	 decision: 0.7873	 get_bound: 8.4750	 add_domain: 0.5527
Current lb:-0.08983749151229858
105452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 258.32423520088196

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [4, 492] [1, 28] [4, 492] [2, 844] [4, 786] [4, 212] [4, 212] [5, 46] [4, 786] [3, 653] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 313.0272216796875 with beta sum per layer: [0.0, 306.2648010253906, 167.54977416992188, 357.0347595214844, 1364.815185546875, 230.40066528320312]
alpha/beta optimization time: 4.850609302520752
This batch time : update_bounds func: 6.4977	 prepare: 0.7509	 bound: 4.8513	 transfer: 0.3064	 finalize: 0.5755
Accumulated time: update_bounds func: 221.8193	 prepare: 20.6433	 bound: 159.6600	 transfer: 0.3064	 finalize: 29.8659
batch bounding time:  6.504029989242554
Current worst splitting domains [lb, ub] (depth):
[-0.08927,   inf] (60), [-0.08927,   inf] (68), [-0.08927,   inf] (48), [-0.08927,   inf] (48), [-0.08927,   inf] (50), [-0.08927,   inf] (56), [-0.08927,   inf] (64), [-0.08927,   inf] (58), [-0.08926,   inf] (66), [-0.08926,   inf] (46), [-0.08926,   inf] (48), [-0.08926,   inf] (50), [-0.08926,   inf] (46), [-0.08926,   inf] (58), [-0.08926,   inf] (46), [-0.08926,   inf] (48), [-0.08926,   inf] (48), [-0.08926,   inf] (50), [-0.08926,   inf] (50), [-0.08926,   inf] (48), 
length of domains: 50490
Total time: 8.6192	 pickout: 0.7377	 decision: 0.7924	 get_bound: 6.5142	 add_domain: 0.5749
Current lb:-0.08926641196012497
109452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 267.1052224636078

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([2000, 8, 16, 16]) pre split depth:  1
batch:  torch.Size([2000, 8, 16, 16]) post split depth:  1
splitting decisions: 
split level 0: [1, 597] [1, 9] [4, 212] [3, 659] [4, 537] [4, 476] [2, 732] [3, 653] [2, 414] [3, 979] 
regular batch size: 2*2000, diving batch size 1*0
best_l after optimization: 309.58319091796875 with beta sum per layer: [0.0, 297.2843322753906, 162.92596435546875, 388.78857421875, 1344.451171875, 243.0267791748047]
alpha/beta optimization time: 4.861410140991211
This batch time : update_bounds func: 6.5571	 prepare: 0.7756	 bound: 4.8622	 transfer: 0.3175	 finalize: 0.5851
Accumulated time: update_bounds func: 228.3764	 prepare: 21.4190	 bound: 164.5221	 transfer: 0.3175	 finalize: 30.4511
batch bounding time:  6.56411337852478
Current worst splitting domains [lb, ub] (depth):
[-0.08871,   inf] (54), [-0.08871,   inf] (42), [-0.08871,   inf] (58), [-0.08871,   inf] (48), [-0.08871,   inf] (42), [-0.08871,   inf] (64), [-0.08871,   inf] (56), [-0.08871,   inf] (58), [-0.08871,   inf] (60), [-0.08871,   inf] (48), [-0.08871,   inf] (48), [-0.08871,   inf] (56), [-0.08871,   inf] (56), [-0.08871,   inf] (50), [-0.08871,   inf] (50), [-0.08871,   inf] (56), [-0.08871,   inf] (66), [-0.08871,   inf] (56), [-0.08871,   inf] (50), [-0.08871,   inf] (46), 
length of domains: 52466
Total time: 11.4169	 pickout: 0.8301	 decision: 3.4126	 get_bound: 6.5774	 add_domain: 0.5968
Current lb:-0.08871376514434814
113452 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Time out!!!!!!!!
Image 0 against label 3 verification end, Time cost: 281.11521220207214
Result: unknown in 330.0935 seconds


[[     0.              0.0000001    2996.             25.6517837
       0.        ]
 [     0.              4.42247295      0.              0.00030684
       1.        ]
 [     0.             -0.08871377 113452.            281.1152122
       3.        ]]
############# Summary #############
Final verified acc: 0.0% [total 1 examples]
Total verification count: 1 , total verified safe: 0 , verified unsafe: 0 , timeout: 1
mean time [total:1]: 306.76730275154114
mean time [cnt:1]: 306.76730275154114
max time 330.09353399276733
unknown (total 1): [0]
