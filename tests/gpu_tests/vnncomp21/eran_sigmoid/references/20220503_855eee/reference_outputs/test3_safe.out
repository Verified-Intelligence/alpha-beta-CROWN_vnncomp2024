Building native CUDA modules...
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/convert/operations.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1627336316785/work/torch/csrc/utils/tensor_numpy.cpp:143.)
  op = value_wrapper(torch.from_numpy(extract_attributes(node)["constant"]))
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py:680: UserWarning: ONNX export mode is set to inference mode, but operator batch_norm is set to inference mode. The model will be exported in inference, as specified by the export mode.
  training_mode + ", as specified by the export mode.")
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
  csv_name: eran_instances.csv
  onnx_path: null
  vnnlib_path: null
  results_file: null
  root_path: ../../../../../vnncomp2021/benchmarks/eran
model:
  path: null
  name: mnist_9_200
data:
  start: 59
  end: 60
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: MNIST
  data_filter_path: null
specification:
  type: lp
  norm: .inf
  epsilon: null
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 64
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 50
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 360
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 3
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: before
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 22:32:33 2022 on ubuntu
saving results to vnn-comp_[eran_instances]_start=59_end=60_iter=50_b=64_timeout=360_branching=kfsb-min-3_lra-init=0.1_lra=0.01_lrb=0.05_PGD=before.npz
customized start/end sample from 59 to 60

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
arguments.Config["general"]["loss_reduction_func"] change: sum -> min
arguments.Config["solver"]["alpha-crown"]["iteration"] change: 100 -> 1000
arguments.Config["solver"]["alpha-crown"]["lr_decay"] change: 0.98 -> 0.999
arguments.Config["solver"]["beta-crown"]["lr_decay"] change: 0.98 -> 0.999
arguments.Config["attack"]["pgd_order"] change: before -> before
arguments.Config["general"]["complete_verifier"] change: bab -> skip
##### PGD attack: True label: 1, Tested against: [0, 2, 3, 4, 5, 6, 7, 8, 9] ######
pgd prediction: tensor([-15.4687,   0.3406,  -8.0588, -11.4311,  -7.8029, -10.5218,  -9.4028,
         -9.1984,  -6.2522,  -9.7026], device='cuda:0',
       grad_fn=<SqueezeBackward1>)
attack margin tensor([15.8093,     inf,  8.3994, 11.7717,  8.1434, 10.8624,  9.7434,  9.5390,
         6.5928, 10.0432], device='cuda:0', grad_fn=<RsubBackward1>)
untargeted pgd failed
Model prediction is: tensor([[-19.6652,   0.2360,  -9.2802, -13.7092,  -9.7208, -12.9633, -11.6576,
         -10.2555,  -9.2037, -12.3011]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[-106.4819,  -61.5817, -100.7387, -103.3482,  -80.3464, -112.3934,
          -78.7609,  -71.0683,  -96.2570]], device='cuda:0') None

all verified at 91th iter
best_l after optimization: -0.021759033203125 with beta sum per layer: []
alpha/beta optimization time: 13.096301317214966
initial alpha-CROWN bounds: tensor([[0.2915, 0.1447, 1.8557, 0.1238, 1.0686, 0.1573, 1.5356, 0.0908, 0.0218]],
       device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(0.0218, device='cuda:0', grad_fn=<MinBackward1>)
verified with init bound!
Result: safe-incomplete in 17.5533 seconds


[]
############# Summary #############
Final verified acc: 100.0% [total 1 examples]
Total verification count: 1 , total verified safe: 1 , verified unsafe: 0 , timeout: 0
max time 17.553337812423706
safe-incomplete (total 1): [0]
