Building native CUDA modules...
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  csv_name: acasxu_instances.csv
  onnx_path: null
  vnnlib_path: null
  results_file: null
  root_path: ../../../../../vnncomp2021/benchmarks/acasxu
model:
  path: null
data:
  start: 118
  end: 119
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: ACASXU
  data_filter_path: null
specification:
  type: lp
  norm: .inf
  epsilon: null
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: true
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 1000
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 10
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 360
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: sb
    candidates: 3
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
  solve_slope: false
attack:
  pgd_order: before

Experiments at Mon May  2 22:12:19 2022 on ubuntu

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
##### [118] True label: [0 0 0 0], Tested against: [1 2 3 4], onnx_path: ACASXU_run2a_4_2_batch_2000.onnx, vnnlib_path: prop_3.vnnlib ######
calculate grad on input: False
initial CROWN bounds: tensor([[-0.3810, -0.6152, -0.9345, -0.9113]], device='cuda:0') tensor([[0.6025, 0.6637, 1.1410, 1.0031]], device='cuda:0')
tensor([[-0.3810, -0.6152, -0.9345, -0.9113]], device='cuda:0')
decision time: 1.3103809356689453
insert to domain / total batch time: 0.000357/1.333390
length of domains: 2
Current lb:-0.34297117590904236
2 neurons visited

decision time: 0.1138463020324707
insert to domain / total batch time: 0.000430/0.137066
length of domains: 4
Current lb:-0.34986844658851624
6 neurons visited

decision time: 0.11442232131958008
insert to domain / total batch time: 0.000797/0.138313
length of domains: 8
Current lb:-0.3453248143196106
14 neurons visited

decision time: 0.11704468727111816
insert to domain / total batch time: 0.001705/0.142124
length of domains: 16
Current lb:-0.33730798959732056
30 neurons visited

decision time: 0.11878752708435059
insert to domain / total batch time: 0.003043/0.145585
length of domains: 32
Current lb:-0.34976282715797424
62 neurons visited

decision time: 0.12405776977539062
insert to domain / total batch time: 0.006042/0.154841
length of domains: 64
Current lb:-0.34783339500427246
126 neurons visited

decision time: 0.12830567359924316
insert to domain / total batch time: 0.015254/0.169401
length of domains: 128
Current lb:-0.3585943877696991
254 neurons visited

decision time: 0.14219927787780762
insert to domain / total batch time: 0.043398/0.213071
length of domains: 256
Current lb:-0.3580569922924042
510 neurons visited

decision time: 0.15947389602661133
insert to domain / total batch time: 0.060100/0.252259
length of domains: 512
Current lb:-0.35789644718170166
1022 neurons visited

decision time: 0.20708250999450684
insert to domain / total batch time: 0.145303/0.394615
length of domains: 1024
Current lb:-0.3577888607978821
2046 neurons visited

decision time: 0.3300619125366211
insert to domain / total batch time: 0.604591/1.000283
length of domains: 2024
Current lb:-0.3577216565608978
4046 neurons visited

decision time: 0.32434606552124023
insert to domain / total batch time: 0.668562/1.057757
length of domains: 3024
Current lb:-0.3576946556568146
6046 neurons visited

decision time: 0.3196251392364502
insert to domain / total batch time: 0.702423/1.086508
length of domains: 4024
Current lb:-0.3576816916465759
8046 neurons visited

decision time: 0.31760430335998535
insert to domain / total batch time: 0.717953/1.100002
length of domains: 5024
Current lb:-0.3576749563217163
10046 neurons visited

decision time: 0.31732797622680664
insert to domain / total batch time: 0.751084/1.132926
length of domains: 6024
Current lb:-0.35767170786857605
12046 neurons visited

decision time: 0.3152332305908203
insert to domain / total batch time: 0.825580/1.206620
length of domains: 7024
Current lb:-0.35766953229904175
14046 neurons visited

decision time: 0.3148941993713379
insert to domain / total batch time: 0.754897/1.134237
length of domains: 8024
Current lb:-0.3576689064502716
16046 neurons visited

decision time: 0.3147156238555908
insert to domain / total batch time: 0.769825/1.148965
length of domains: 9024
Current lb:-0.3576684594154358
18046 neurons visited

decision time: 0.31661367416381836
insert to domain / total batch time: 0.796666/1.177952
length of domains: 10024
Current lb:-0.3576679229736328
20046 neurons visited

decision time: 0.31787586212158203
insert to domain / total batch time: 0.786069/1.168959
length of domains: 11024
Current lb:-0.3576679229736328
22046 neurons visited

decision time: 0.31705594062805176
insert to domain / total batch time: 0.802274/1.184164
length of domains: 12024
Current lb:-0.3576679229736328
24046 neurons visited

decision time: 0.31737709045410156
insert to domain / total batch time: 0.786228/1.168438
length of domains: 13024
Current lb:-0.3576679229736328
26046 neurons visited

decision time: 0.31774139404296875
insert to domain / total batch time: 0.877438/1.259811
length of domains: 14024
Current lb:-0.3576679229736328
28046 neurons visited

decision time: 0.31461596488952637
insert to domain / total batch time: 0.789850/1.169107
length of domains: 15024
Current lb:-0.3576679229736328
30046 neurons visited

calculate grad on input: False
alpha-CROWN optimizable variables initialized.
best_l after optimization: 0.1722225397825241 with beta sum per layer: []
alpha/beta optimization time: 3.5737967491149902
initial CROWN bounds: tensor([[-0.1722, -0.3360, -0.4342, -0.5472]], device='cuda:0',
       grad_fn=<AsStridedBackward>) None
decision time: 0.12106609344482422
best_l after optimization: 0.2961670160293579 with beta sum per layer: []
alpha/beta optimization time: 2.8287994861602783
insert to domain / total batch time: 0.000338/2.953220
length of domains: 2
Current lb:-0.14919570088386536
30048 neurons visited

decision time: 0.11995148658752441
best_l after optimization: 0.5208703875541687 with beta sum per layer: []
alpha/beta optimization time: 0.6689865589141846
insert to domain / total batch time: 0.000522/0.793138
length of domains: 4
Current lb:-0.13190215826034546
30052 neurons visited

decision time: 0.12302303314208984
best_l after optimization: 0.9351620078086853 with beta sum per layer: []
alpha/beta optimization time: 0.6616110801696777
insert to domain / total batch time: 0.000931/0.791111
length of domains: 8
Current lb:-0.119044728577137
30060 neurons visited

decision time: 0.12611699104309082
best_l after optimization: 1.756225824356079 with beta sum per layer: []
alpha/beta optimization time: 0.6683704853057861
insert to domain / total batch time: 0.002107/0.805635
length of domains: 16
Current lb:-0.11139415949583054
30076 neurons visited

decision time: 0.12661170959472656
best_l after optimization: 3.396766185760498 with beta sum per layer: []
alpha/beta optimization time: 0.6745007038116455
insert to domain / total batch time: 0.004793/0.822338
length of domains: 32
Current lb:-0.10790123045444489
30108 neurons visited

decision time: 0.12994694709777832
best_l after optimization: 6.636274337768555 with beta sum per layer: []
alpha/beta optimization time: 0.6770164966583252
insert to domain / total batch time: 0.010760/0.848491
length of domains: 64
Current lb:-0.10538133978843689
30172 neurons visited

decision time: 0.1413257122039795
best_l after optimization: 13.146052360534668 with beta sum per layer: []
alpha/beta optimization time: 0.6783123016357422
insert to domain / total batch time: 0.025126/0.904838
length of domains: 128
Current lb:-0.10461743921041489
30300 neurons visited

decision time: 0.1576850414276123
best_l after optimization: 25.982736587524414 with beta sum per layer: []
alpha/beta optimization time: 0.6863915920257568
insert to domain / total batch time: 0.056031/1.017564
length of domains: 256
Current lb:-0.10317716747522354
30556 neurons visited

decision time: 0.19295692443847656
best_l after optimization: 51.778564453125 with beta sum per layer: []
alpha/beta optimization time: 0.7019133567810059
insert to domain / total batch time: 0.127215/1.257771
length of domains: 512
Current lb:-0.10270292311906815
31068 neurons visited

decision time: 0.26273536682128906
best_l after optimization: 103.05868530273438 with beta sum per layer: []
alpha/beta optimization time: 0.7527804374694824
insert to domain / total batch time: 0.286466/1.776099
length of domains: 1024
Current lb:-0.10219497978687286
32092 neurons visited

decision time: 0.4426300525665283
best_l after optimization: 200.5787811279297 with beta sum per layer: []
alpha/beta optimization time: 0.883744478225708
insert to domain / total batch time: 0.614037/2.948370
length of domains: 2024
Current lb:-0.10193894058465958
34092 neurons visited

decision time: 0.4463663101196289
best_l after optimization: 201.5740966796875 with beta sum per layer: []
alpha/beta optimization time: 0.8657467365264893
insert to domain / total batch time: 0.685310/2.983866
length of domains: 3024
Current lb:-0.10182221978902817
36092 neurons visited

decision time: 0.44612812995910645
best_l after optimization: 201.81016540527344 with beta sum per layer: []
alpha/beta optimization time: 0.8642919063568115
insert to domain / total batch time: 0.716378/3.036490
length of domains: 4024
Current lb:-0.10181493312120438
38092 neurons visited

decision time: 0.4454796314239502
best_l after optimization: 201.82431030273438 with beta sum per layer: []
alpha/beta optimization time: 0.8692030906677246
insert to domain / total batch time: 0.717071/2.998124
length of domains: 5024
Current lb:-0.10181126743555069
40092 neurons visited

decision time: 0.4471609592437744
best_l after optimization: 201.68817138671875 with beta sum per layer: []
alpha/beta optimization time: 0.8680968284606934
insert to domain / total batch time: 0.747348/3.103950
length of domains: 6024
Current lb:-0.10180936753749847
42092 neurons visited

decision time: 0.4708526134490967
best_l after optimization: 201.7562255859375 with beta sum per layer: []
alpha/beta optimization time: 0.8858275413513184
insert to domain / total batch time: 0.742236/3.231380
length of domains: 7024
Current lb:-0.10180850327014923
44092 neurons visited

decision time: 0.43912363052368164
best_l after optimization: 202.12783813476562 with beta sum per layer: []
alpha/beta optimization time: 0.878462553024292
insert to domain / total batch time: 0.750538/3.210553
length of domains: 8024
Current lb:-0.10180806368589401
46092 neurons visited

decision time: 0.43385910987854004
best_l after optimization: 201.97732543945312 with beta sum per layer: []
alpha/beta optimization time: 0.8651666641235352
insert to domain / total batch time: 0.777615/3.020875
length of domains: 9024
Current lb:-0.10143663734197617
48092 neurons visited

decision time: 0.43477416038513184
best_l after optimization: 202.48512268066406 with beta sum per layer: []
alpha/beta optimization time: 0.8813369274139404
insert to domain / total batch time: 0.745160/3.214049
length of domains: 10024
Current lb:-0.10145682841539383
50092 neurons visited

decision time: 0.4424169063568115
best_l after optimization: 201.26036071777344 with beta sum per layer: []
alpha/beta optimization time: 0.8663468360900879
insert to domain / total batch time: 0.763524/3.005157
length of domains: 11024
Current lb:-0.10135675221681595
52092 neurons visited

decision time: 0.43263959884643555
best_l after optimization: 201.7882537841797 with beta sum per layer: []
alpha/beta optimization time: 0.883181095123291
insert to domain / total batch time: 0.796290/3.362648
length of domains: 12024
Current lb:-0.10109906643629074
54092 neurons visited

decision time: 0.43550896644592285
best_l after optimization: 201.49063110351562 with beta sum per layer: []
alpha/beta optimization time: 0.8807799816131592
insert to domain / total batch time: 0.790257/3.034067
length of domains: 13024
Current lb:-0.10096950829029083
56092 neurons visited

decision time: 0.442584753036499
best_l after optimization: 201.87594604492188 with beta sum per layer: []
alpha/beta optimization time: 0.8896634578704834
insert to domain / total batch time: 0.797503/3.351049
length of domains: 14024
Current lb:-0.10145547986030579
58092 neurons visited

decision time: 0.43876051902770996
best_l after optimization: 200.73648071289062 with beta sum per layer: []
alpha/beta optimization time: 0.8786580562591553
insert to domain / total batch time: 0.763538/3.019613
length of domains: 15024
Current lb:-0.1009707897901535
60092 neurons visited

attacking loss tensor(-0.0151, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0228, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0218, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0199, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0195, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0183, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0189, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0180, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0167, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0155, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0153, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0151, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0145, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0141, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0136, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0129, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0123, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0117, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0114, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0111, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0106, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0103, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0100, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0097, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0094, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0093, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0092, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0081, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0077, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0075, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0072, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0069, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0068, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0065, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0064, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0063, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0060, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0059, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0058, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0057, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0056, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0054, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0052, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0051, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0045, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0044, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0043, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0042, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0041, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0040, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0039, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0038, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0035, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0032, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0032, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0030, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0029, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0028, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0028, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0027, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0026, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0026, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0026, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0025, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0024, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0024, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0023, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0023, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0022, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0022, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0022, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0021, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0020, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0020, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0020, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0019, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0019, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0018, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0018, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0018, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0017, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0017, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0017, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0016, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0016, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0016, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0015, device='cuda:0', grad_fn=<MeanBackward0>)
decision time: 0.4596738815307617
best_l after optimization: 201.7032928466797 with beta sum per layer: []
alpha/beta optimization time: 0.891599178314209
insert to domain / total batch time: 0.810528/3.491795
length of domains: 16024
Current lb:-0.10138747096061707
62092 neurons visited

attacking loss tensor(-0.0151, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0227, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0218, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0198, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0196, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0183, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0189, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0180, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0167, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0156, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0153, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0152, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0145, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0141, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0136, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0129, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0123, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0117, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0114, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0111, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0106, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0102, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0100, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0097, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0094, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0093, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0092, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0081, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0077, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0075, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0072, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0069, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0068, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0065, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0064, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0063, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0061, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0059, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0058, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0057, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0056, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0054, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0053, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0051, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0045, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0044, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0043, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0042, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0041, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0040, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0039, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0038, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0035, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0032, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0032, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0030, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0029, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0028, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0028, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0027, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0026, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0026, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0026, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0025, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0024, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0024, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0023, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0023, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0022, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0022, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0022, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0021, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0020, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0020, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0020, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0019, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0019, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0018, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0018, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0018, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0017, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0017, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0017, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0016, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0016, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0016, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0015, device='cuda:0', grad_fn=<MeanBackward0>)
decision time: 0.44165778160095215
best_l after optimization: 201.42665100097656 with beta sum per layer: []
alpha/beta optimization time: 0.8860623836517334
insert to domain / total batch time: 0.805751/3.080695
length of domains: 17024
Current lb:-0.10092009603977203
64092 neurons visited

attacking loss tensor(-0.0151, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0227, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0218, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0198, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0196, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0183, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0189, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0180, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0167, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0155, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0153, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0151, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0145, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0141, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0136, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0129, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0123, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0117, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0114, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0110, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0106, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0102, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0100, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0097, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0094, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0093, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0092, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0081, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0077, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0075, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0072, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0069, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0067, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0065, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0064, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0063, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0060, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0059, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0058, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0057, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0056, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0054, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0052, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0051, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0047, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss /home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/convert/operations.py:110: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1627336316785/work/torch/csrc/utils/tensor_numpy.cpp:143.)
  weight = torch.from_numpy(numpy_helper.to_array(params[0]))
/mnt/data1/zhouxing/gputest/CROWN-GENERAL/tests/gpu_tests/vnncomp/acasxu/../../../../complete_verifier/bab_verification_input_split.py:208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  ret = np.array(ret)
/mnt/data1/zhouxing/gputest/CROWN-GENERAL/tests/gpu_tests/vnncomp/acasxu/../../../../complete_verifier/bab_verification_input_split.py:225: ResourceWarning: unclosed file <_io.TextIOWrapper name='../../../../../vnncomp2021/benchmarks/acasxu/acasxu_instances.csv' mode='r' encoding='UTF-8'>
  main()
ResourceWarning: Enable tracemalloc to get the object allocation traceback
tensor(-0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0045, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0044, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0043, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0042, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0041, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0040, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0039, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0038, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
attacking loss tensor(-0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
time out!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Image 118 against [1 2 3 4] verify end, Time cost: 117.65580439567566
[[118 -0.10092009603977203 64092 117.65580439567566 array([1, 2, 3, 4])]]
[['119' 'timeout']]
time mean: 117.65580439567566, branches mean: 64092.0
final verified acc: 18500.0%[1]
Total verification count: 1 total verified: 185
mean time [total:1]: 117.65580439567566
mean time [cnt:1]: 117.65580439567566
