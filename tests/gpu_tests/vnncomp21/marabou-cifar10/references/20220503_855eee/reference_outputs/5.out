Building native CUDA modules...
/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
CUDA modules have been built.
Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
  csv_name: marabou-cifar10_instances.csv
  onnx_path: null
  vnnlib_path: null
  results_file: null
  root_path: ../../../../../vnncomp2021/benchmarks/marabou-cifar10
model:
  path: null
  name: mnist_9_200
data:
  start: 51
  end: 52
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR
  data_filter_path: null
specification:
  type: lp
  norm: .inf
  epsilon: null
solver:
  no_float64_last_iter: false
  no_amp: false
  early_stop_patience: 10
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
  beta-crown:
    batch_size: 1000
    min_batch_size_ratio: 0.1
    lr_alpha: 0.01
    lr_beta: 0.5
    lr_decay: 0.98
    optimizer: adam
    iteration: 50
    beta: true
    beta_warmup: true
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
bab:
  max_domains: 200000
  decision_thresh: 0
  timeout: 360
  get_upper_bound: false
  dfs_percent: 0.0
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1
    iteration: 500
    lr_beta: 0.01
    number_cuts: 50
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    skip_bab: false
    max_num: 1000000000
    incomplete: false
  branching:
    method: kfsb
    candidates: 5
    reduceop: min
    input_split:
      enable: false
      use_alpha_patience: 20
      attack_patience: 80
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: before
  enable_mip_attack: false
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
debug:
  lp_test: null

Experiments at Mon May  2 21:34:40 2022 on ubuntu
saving results to vnn-comp_[marabou-cifar10_instances]_start=51_end=52_iter=50_b=1000_timeout=360_branching=kfsb-min-5_lra-init=0.1_lra=0.01_lrb=0.5_PGD=before.npz
customized start/end sample from 51 to 52

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Notice: this ONNX file has NHWC order. We assume the X in vnnlib is also flattend in in NHWC order (1, 32, 32, 3)
model output: [ 0.53121954  3.0272884  -3.6404533  -2.5361605  -3.4825277  -3.142046
 -5.0341187  -0.8614872  -1.9583007   7.594594  ]
##### PGD attack: True label: 9, Tested against: [0] ######
pgd prediction: tensor([ 0.7429,  1.4633, -2.5761, -1.9789, -2.4632, -2.0156, -4.6295,  0.1379,
        -1.9189,  5.4668], device='cuda:0', grad_fn=<SqueezeBackward1>)
attack margin 4.723903596401215
targeted pgd failed, margin 4.723903596401215
[ 0.53121954  3.0272884  -3.6404533  -2.5361605  -3.4825277  -3.142046
 -5.0341187  -0.8614872  -1.9583007   7.594594  ]
Model prediction is: tensor([[ 0.5312,  3.0273, -3.6405, -2.5362, -3.4825, -3.1420, -5.0341, -0.8615,
         -1.9583,  7.5946]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
initial CROWN bounds: tensor([[-26.1305, -20.6661, -28.2270, -26.7033, -27.9817, -27.6114, -25.7621,
         -27.8188, -21.7863]], device='cuda:0') None
best_l after optimization: 89.24439239501953 with beta sum per layer: []
alpha/beta optimization time: 10.822105169296265
initial alpha-CROWN bounds: tensor([[-11.6259,  -9.8816, -10.3257,  -9.1822,  -9.8960,  -9.7393,  -8.2154,
         -11.3371,  -9.0412]], device='cuda:0', grad_fn=<AsStridedBackward>)
worst class: tensor(-11.6259, device='cuda:0', grad_fn=<MinBackward1>)
##### [0] True label: 9, Tested against: 6, onnx_path: ./nets/cifar10_medium.onnx, vnnlib_path: ./specs/networkcifar10_medium_index9409_eps0.012_target0_orig9.vnnlib ######
Model prediction is: tensor([[ 0.5312,  3.0273, -3.6405, -2.5362, -3.4825, -3.1420, -5.0341, -0.8615,
         -1.9583,  7.5946]], device='cuda:0')
alpha-CROWN optimizable variables initialized.
setting alpha for layer /12 start_node /13
setting alpha for layer /12 start_node /16
setting alpha for layer /12 start_node /18
not setting layer /12 start_node /20 because shape mismatch (torch.Size([2, 1, 1, 16, 15, 15]) != torch.Size([2, 9, 1, 16, 15, 15]))
setting alpha for layer /14 start_node /16
setting alpha for layer /14 start_node /18
not setting layer /14 start_node /20 because shape mismatch (torch.Size([2, 1, 1, 32, 6, 6]) != torch.Size([2, 9, 1, 32, 6, 6]))
setting alpha for layer /17 start_node /18
not setting layer /17 start_node /20 because shape mismatch (torch.Size([2, 1, 1, 128]) != torch.Size([2, 9, 1, 128]))
not setting layer /19 start_node /20 because shape mismatch (torch.Size([2, 1, 1, 64]) != torch.Size([2, 9, 1, 64]))
0 /11 torch.Size([1, 16, 15, 15])
1 /13 torch.Size([1, 32, 6, 6])
2 /16 torch.Size([1, 128])
3 /18 torch.Size([1, 64])
best_l after optimization: 8.213793754577637 with beta sum per layer: []
alpha/beta optimization time: 2.1789145469665527
alpha-CROWN with fixed intermediate bounds: tensor([[-8.2138]], device='cuda:0', grad_fn=<AsStridedBackward>) None
-8.213793754577637
layer 0 size torch.Size([3600]) unstable 841
layer 1 size torch.Size([1152]) unstable 316
layer 2 size torch.Size([128]) unstable 79
layer 3 size torch.Size([64]) unstable 63
-----------------
# of unstable neurons: 1299
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 16, 15, 15]) pre split depth:  6
batch:  torch.Size([1, 16, 15, 15]) post split depth:  6
splitting decisions: 
split level 0: [3, 50] 
split level 1: [3, 33] 
split level 2: [3, 51] 
split level 3: [3, 35] 
split level 4: [3, 62] 
split level 5: [3, 12] 
regular batch size: 2*32, diving batch size 1*0
best_l after optimization: 205.5560760498047 with beta sum per layer: [0.0, 0.0, 0.0, 64.90118408203125]
alpha/beta optimization time: 0.8206472396850586
This batch time : update_bounds func: 0.8406	 prepare: 0.0100	 bound: 0.8211	 transfer: 0.0024	 finalize: 0.0069
Accumulated time: update_bounds func: 0.8406	 prepare: 0.0100	 bound: 0.8211	 transfer: 0.0024	 finalize: 0.0069
batch bounding time:  0.8409085273742676
Current worst splitting domains [lb, ub] (depth):
[-3.96965,   inf] (7), [-3.82498,   inf] (7), [-3.82044,   inf] (7), [-3.76694,   inf] (7), [-3.73778,   inf] (7), [-3.73721,   inf] (7), [-3.73383,   inf] (7), [-3.72391,   inf] (7), [-3.66079,   inf] (7), [-3.65484,   inf] (7), [-3.59895,   inf] (7), [-3.56633,   inf] (7), [-3.56554,   inf] (7), [-3.56134,   inf] (7), [-3.53353,   inf] (7), [-3.52669,   inf] (7), [-3.49591,   inf] (7), [-3.45597,   inf] (7), [-3.44682,   inf] (7), [-3.42344,   inf] (7), 
length of domains: 64
Total time: 0.9076	 pickout: 0.0013	 decision: 0.0500	 get_bound: 0.8531	 add_domain: 0.0031
Current lb:-3.969653844833374
64 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 4.045372247695923

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([64, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([64, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 31] [3, 31] [3, 31] [3, 31] [3, 31] [3, 31] [3, 31] [3, 31] [3, 31] [3, 31] 
regular batch size: 2*64, diving batch size 1*0
best_l after optimization: 334.23486328125 with beta sum per layer: [0.0, 0.0, 0.0, 177.61776733398438]
alpha/beta optimization time: 0.8313336372375488
This batch time : update_bounds func: 0.8753	 prepare: 0.0209	 bound: 0.8317	 transfer: 0.0087	 finalize: 0.0137
Accumulated time: update_bounds func: 1.7160	 prepare: 0.0308	 bound: 1.6528	 transfer: 0.0087	 finalize: 0.0206
batch bounding time:  0.8759465217590332
Current worst splitting domains [lb, ub] (depth):
[-3.43322,   inf] (9), [-3.38821,   inf] (9), [-3.29052,   inf] (9), [-3.27223,   inf] (9), [-3.27147,   inf] (9), [-3.26655,   inf] (9), [-3.21084,   inf] (9), [-3.20457,   inf] (9), [-3.18604,   inf] (9), [-3.17758,   inf] (9), [-3.17088,   inf] (9), [-3.16616,   inf] (9), [-3.16551,   inf] (9), [-3.16029,   inf] (9), [-3.15601,   inf] (9), [-3.14083,   inf] (9), [-3.13288,   inf] (9), [-3.08219,   inf] (9), [-3.07074,   inf] (9), [-3.04332,   inf] (9), 
length of domains: 128
Total time: 0.9626	 pickout: 0.0150	 decision: 0.0652	 get_bound: 0.8763	 add_domain: 0.0061
Current lb:-3.4332244396209717
192 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 5.00947904586792

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([128, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([128, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [2, 22] [2, 22] [2, 22] [2, 22] [2, 22] [2, 22] [2, 22] [2, 22] [3, 49] [2, 22] 
regular batch size: 2*128, diving batch size 1*0
best_l after optimization: 388.8658447265625 with beta sum per layer: [0.0, 0.0, 236.55014038085938, 784.9868774414062]
alpha/beta optimization time: 0.875852108001709
This batch time : update_bounds func: 0.9627	 prepare: 0.0409	 bound: 0.8763	 transfer: 0.0167	 finalize: 0.0282
Accumulated time: update_bounds func: 2.6787	 prepare: 0.0717	 bound: 2.5291	 transfer: 0.0167	 finalize: 0.0488
batch bounding time:  0.9632565975189209
Current worst splitting domains [lb, ub] (depth):
[-2.93624,   inf] (11), [-2.93446,   inf] (11), [-2.82816,   inf] (11), [-2.80800,   inf] (11), [-2.79360,   inf] (11), [-2.75905,   inf] (11), [-2.75551,   inf] (11), [-2.74983,   inf] (11), [-2.74595,   inf] (11), [-2.74296,   inf] (11), [-2.73529,   inf] (11), [-2.73101,   inf] (11), [-2.72892,   inf] (11), [-2.72702,   inf] (11), [-2.71889,   inf] (11), [-2.71356,   inf] (11), [-2.68920,   inf] (11), [-2.68338,   inf] (11), [-2.67651,   inf] (11), [-2.65490,   inf] (11), 
length of domains: 219
Total time: 1.0958	 pickout: 0.0296	 decision: 0.0910	 get_bound: 0.9638	 add_domain: 0.0114
Current lb:-2.9362385272979736
448 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 6.108367443084717

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([219, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([219, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 49] [3, 49] [3, 49] [3, 49] [3, 49] [3, 39] [3, 49] [3, 49] [3, 49] [3, 49] 
regular batch size: 2*219, diving batch size 1*0
best_l after optimization: 392.3055419921875 with beta sum per layer: [0.0, 0.0, 317.02972412109375, 1966.465087890625]
alpha/beta optimization time: 0.9208643436431885
This batch time : update_bounds func: 1.0467	 prepare: 0.0704	 bound: 0.9214	 transfer: 0.0233	 finalize: 0.0305
Accumulated time: update_bounds func: 3.7254	 prepare: 0.1421	 bound: 3.4505	 transfer: 0.0233	 finalize: 0.0794
batch bounding time:  1.047419786453247
Current worst splitting domains [lb, ub] (depth):
[-2.49826,   inf] (13), [-2.49313,   inf] (13), [-2.46148,   inf] (13), [-2.44348,   inf] (13), [-2.39782,   inf] (13), [-2.36925,   inf] (13), [-2.36657,   inf] (13), [-2.35551,   inf] (13), [-2.34609,   inf] (13), [-2.32640,   inf] (13), [-2.32090,   inf] (13), [-2.32090,   inf] (13), [-2.31860,   inf] (13), [-2.31771,   inf] (13), [-2.31754,   inf] (13), [-2.31534,   inf] (13), [-2.31346,   inf] (13), [-2.31076,   inf] (13), [-2.30307,   inf] (13), [-2.28623,   inf] (13), 
length of domains: 337
Total time: 1.2999	 pickout: 0.0493	 decision: 0.1290	 get_bound: 1.0482	 add_domain: 0.0734
Current lb:-2.498255729675293
886 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 7.413726568222046

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([337, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([337, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 39] [3, 39] [3, 39] [3, 39] [3, 39] [3, 39] [3, 39] [3, 39] [2, 22] [3, 39] 
regular batch size: 2*337, diving batch size 1*0
best_l after optimization: 182.097412109375 with beta sum per layer: [0.0, 0.0, 172.00730895996094, 3874.686767578125]
alpha/beta optimization time: 1.0174078941345215
This batch time : update_bounds func: 1.1662	 prepare: 0.0677	 bound: 1.0178	 transfer: 0.0301	 finalize: 0.0489
Accumulated time: update_bounds func: 4.8917	 prepare: 0.2099	 bound: 4.4683	 transfer: 0.0301	 finalize: 0.1283
batch bounding time:  1.1671431064605713
Current worst splitting domains [lb, ub] (depth):
[-2.08722,   inf] (15), [-2.08234,   inf] (15), [-2.05160,   inf] (15), [-2.02649,   inf] (15), [-1.98909,   inf] (15), [-1.97153,   inf] (15), [-1.96125,   inf] (15), [-1.94783,   inf] (15), [-1.93796,   inf] (15), [-1.92702,   inf] (15), [-1.92028,   inf] (15), [-1.91804,   inf] (15), [-1.91571,   inf] (15), [-1.90996,   inf] (15), [-1.90042,   inf] (15), [-1.90010,   inf] (15), [-1.89992,   inf] (15), [-1.89979,   inf] (15), [-1.89074,   inf] (15), [-1.87111,   inf] (15), 
length of domains: 443
Total time: 1.3956	 pickout: 0.0560	 decision: 0.1458	 get_bound: 1.1683	 add_domain: 0.0255
Current lb:-2.08721661567688
1560 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 8.81873083114624

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([443, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([443, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 13] [3, 13] [3, 13] [3, 13] [3, 13] [3, 13] [3, 13] [3, 13] [3, 13] [3, 13] 
regular batch size: 2*443, diving batch size 1*0
best_l after optimization: -422.014892578125 with beta sum per layer: [0.0, 0.0, 80.71062469482422, 4681.564453125]
alpha/beta optimization time: 1.1773629188537598
This batch time : update_bounds func: 1.3903	 prepare: 0.0899	 bound: 1.1778	 transfer: 0.0521	 finalize: 0.0684
Accumulated time: update_bounds func: 6.2820	 prepare: 0.2997	 bound: 5.6461	 transfer: 0.0521	 finalize: 0.1967
batch bounding time:  1.3915915489196777
Current worst splitting domains [lb, ub] (depth):
[-1.69660,   inf] (17), [-1.69364,   inf] (17), [-1.66565,   inf] (17), [-1.63192,   inf] (17), [-1.60242,   inf] (17), [-1.58320,   inf] (17), [-1.56966,   inf] (17), [-1.55893,   inf] (17), [-1.55109,   inf] (17), [-1.53521,   inf] (17), [-1.52627,   inf] (17), [-1.52537,   inf] (17), [-1.52514,   inf] (17), [-1.51819,   inf] (17), [-1.51683,   inf] (17), [-1.51160,   inf] (17), [-1.51130,   inf] (17), [-1.51051,   inf] (17), [-1.49703,   inf] (17), [-1.49293,   inf] (17), 
length of domains: 391
Total time: 1.7255	 pickout: 0.0748	 decision: 0.2316	 get_bound: 1.3934	 add_domain: 0.0257
Current lb:-1.6965970993041992
2446 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 10.558956146240234

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([391, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([391, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 18] [3, 18] [3, 1] [3, 18] [3, 1] [3, 1] [3, 1] [3, 1] [3, 1] [3, 18] 
regular batch size: 2*391, diving batch size 1*0
best_l after optimization: 143.8911895751953 with beta sum per layer: [0.0, 0.0, 16.087265014648438, 2148.76416015625]
alpha/beta optimization time: 1.0959808826446533
This batch time : update_bounds func: 1.2671	 prepare: 0.0806	 bound: 1.0964	 transfer: 0.0317	 finalize: 0.0565
Accumulated time: update_bounds func: 7.5490	 prepare: 0.3803	 bound: 6.7425	 transfer: 0.0317	 finalize: 0.2532
batch bounding time:  1.2681939601898193
Current worst splitting domains [lb, ub] (depth):
[-1.33093,   inf] (19), [-1.31963,   inf] (19), [-1.28870,   inf] (19), [-1.26692,   inf] (19), [-1.26430,   inf] (19), [-1.24699,   inf] (19), [-1.24021,   inf] (19), [-1.23960,   inf] (19), [-1.21537,   inf] (19), [-1.19667,   inf] (19), [-1.19257,   inf] (19), [-1.18880,   inf] (19), [-1.18087,   inf] (19), [-1.17752,   inf] (19), [-1.17629,   inf] (19), [-1.16535,   inf] (19)/home/zhouxingshi/conda/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/onnx2pytorch/convert/layer.py:25: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1627336316785/work/torch/csrc/utils/tensor_numpy.cpp:143.)
  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))
, [-1.16302,   inf] (19), [-1.16137,   inf] (19), [-1.15999,   inf] (19), [-1.15950,   inf] (19), 
length of domains: 533
Total time: 1.6028	 pickout: 0.0666	 decision: 0.2325	 get_bound: 1.2697	 add_domain: 0.0339
Current lb:-1.3309348821640015
3228 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 12.172563076019287

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([533, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([533, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 1] [3, 1] [3, 18] [3, 52] [3, 1] [3, 1] [3, 1] [3, 29] [3, 29] [3, 52] 
regular batch size: 2*533, diving batch size 1*0
best_l after optimization: 3.993631362915039 with beta sum per layer: [0.0, 0.0, 10.023120880126953, 2411.40625]
alpha/beta optimization time: 1.2956295013427734
This batch time : update_bounds func: 1.5230	 prepare: 0.1096	 bound: 1.2960	 transfer: 0.0364	 finalize: 0.0784
Accumulated time: update_bounds func: 9.0720	 prepare: 0.4899	 bound: 8.0385	 transfer: 0.0364	 finalize: 0.3316
batch bounding time:  1.5243196487426758
Current worst splitting domains [lb, ub] (depth):
[-0.95694,   inf] (21), [-0.95267,   inf] (21), [-0.92855,   inf] (21), [-0.92114,   inf] (21), [-0.91215,   inf] (21), [-0.89711,   inf] (21), [-0.88772,   inf] (21), [-0.87322,   inf] (21), [-0.87134,   inf] (21), [-0.85871,   inf] (21), [-0.85801,   inf] (21), [-0.85172,   inf] (21), [-0.84588,   inf] (21), [-0.83099,   inf] (21), [-0.82874,   inf] (21), [-0.82452,   inf] (21), [-0.82111,   inf] (21), [-0.81938,   inf] (21), [-0.81144,   inf] (21), [-0.80792,   inf] (21), 
length of domains: 578
Total time: 1.9661	 pickout: 0.0879	 decision: 0.2534	 get_bound: 1.5264	 add_domain: 0.0985
Current lb:-0.9569371342658997
4294 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 14.154969215393066

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([578, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([578, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 29] [3, 29] [3, 52] [3, 52] [3, 29] [3, 18] [3, 52] [3, 29] [3, 18] [3, 29] 
regular batch size: 2*578, diving batch size 1*0
best_l after optimization: -219.57733154296875 with beta sum per layer: [0.0, 0.0, 1.8431854248046875, 1801.5419921875]
alpha/beta optimization time: 1.3606386184692383
This batch time : update_bounds func: 1.6745	 prepare: 0.1184	 bound: 1.3611	 transfer: 0.0497	 finalize: 0.1426
Accumulated time: update_bounds func: 10.7465	 prepare: 0.6083	 bound: 9.3996	 transfer: 0.0497	 finalize: 0.4742
batch bounding time:  1.6761534214019775
Current worst splitting domains [lb, ub] (depth):
[-0.58976,   inf] (23), [-0.58221,   inf] (23), [-0.57818,   inf] (23), [-0.55368,   inf] (23), [-0.54599,   inf] (23), [-0.54501,   inf] (23), [-0.53059,   inf] (23), [-0.50857,   inf] (23), [-0.50775,   inf] (23), [-0.50575,   inf] (23), [-0.49875,   inf] (23), [-0.49266,   inf] (23), [-0.48055,   inf] (23), [-0.47955,   inf] (23), [-0.46466,   inf] (23), [-0.45831,   inf] (23), [-0.45584,   inf] (23), [-0.45479,   inf] (23), [-0.44390,   inf] (23), [-0.44069,   inf] (23), 
length of domains: 263
Total time: 2.0221	 pickout: 0.0998	 decision: 0.2238	 get_bound: 1.6786	 add_domain: 0.0198
Current lb:-0.5897560119628906
5450 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 16.197723627090454

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([263, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([263, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 52] [3, 52] [3, 29] [2, 67] [3, 29] [3, 52] [2, 67] [3, 29] [3, 52] [3, 52] 
regular batch size: 2*263, diving batch size 1*0
best_l after optimization: -142.46810913085938 with beta sum per layer: [0.0, 0.0, 0.0, 498.40673828125]
alpha/beta optimization time: 0.9070584774017334
This batch time : update_bounds func: 1.0128	 prepare: 0.0554	 bound: 0.9074	 transfer: 0.0107	 finalize: 0.0378
Accumulated time: update_bounds func: 11.7593	 prepare: 0.6638	 bound: 10.3070	 transfer: 0.0107	 finalize: 0.5121
batch bounding time:  1.0136597156524658
Current worst splitting domains [lb, ub] (depth):
[-0.23503,   inf] (25), [-0.22606,   inf] (25), [-0.21356,   inf] (25), [-0.18760,   inf] (25), [-0.17655,   inf] (25), [-0.15909,   inf] (25), [-0.15687,   inf] (25), [-0.15488,   inf] (25), [-0.15309,   inf] (25), [-0.14837,   inf] (25), [-0.14353,   inf] (25), [-0.14010,   inf] (25), [-0.11628,   inf] (25), [-0.11593,   inf] (25), [-0.11516,   inf] (25), [-0.11457,   inf] (25), [-0.10609,   inf] (25), [-0.10217,   inf] (25), [-0.10031,   inf] (25), [-0.09374,   inf] (25), 
length of domains: 66
Total time: 1.1853	 pickout: 0.0456	 decision: 0.1201	 get_bound: 1.0147	 add_domain: 0.0050
Current lb:-0.2350301742553711
5976 neurons visited
0 diving domains visited
Global ub: inf, batch ub: inf
Cumulative time: 17.396218299865723

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([66, 16, 15, 15]) pre split depth:  1
batch:  torch.Size([66, 16, 15, 15]) post split depth:  1
splitting decisions: 
split level 0: [3, 5] [3, 5] [3, 5] [2, 67] [2, 13] [3, 5] [3, 5] [2, 108] [3, 29] [3, 29] 
regular batch size: 2*66, diving batch size 1*0

all verified at 0th iter
best_l after optimization: -58.27257537841797 with beta sum per layer: [0.0, 0.0, 0.0, 43.04402160644531]
alpha/beta optimization time: 0.010637521743774414
This batch time : update_bounds func: 0.0399	 prepare: 0.0152	 bound: 0.0110	 transfer: 0.0035	 finalize: 0.0098
Accumulated time: update_bounds func: 11.7993	 prepare: 0.6790	 bound: 10.3180	 transfer: 0.0035	 finalize: 0.5219
batch bounding time:  0.04012775421142578
Current worst splitting domains [lb, ub] (depth):

length of domains: 0
Total time: 0.1062	 pickout: 0.0124	 decision: 0.0534	 get_bound: 0.0403	 add_domain: 0.0001
No domains left, verification finished!
Global ub: inf, batch ub: inf
Cumulative time: 17.50497531890869

Image 0 against label 6 verification end, Time cost: 17.570374011993408
Result: safe-bab in 34.5283 seconds


[[   0.            0.0000001  6108.           17.57037401    6.        ]]
############# Summary #############
Final verified acc: 100.0% [total 1 examples]
Total verification count: 1 , total verified safe: 1 , verified unsafe: 0 , timeout: 0
mean time [total:1]: 17.570374011993408
mean time [cnt:1]: 17.570374011993408
max time 34.528300523757935
safe-bab (total 1): [0]
