Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: matrix
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  sparse_alpha: true
  save_adv_example: false
  precompile_jit: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: false
  get_crown_verified_acc: false
  csv_name: instances.csv
  results_file: out.txt
  root_path: ../../vnncomp2022_benchmarks/benchmarks/reach_prob_density/
model:
  path: null
  cache_onnx_conversion: false
  onnx_quirks: null
  name: mnist_9_200
  onnx_path: null
  onnx_path_prefix: ''
  onnx_optimization_flags: none
data:
  start: 27
  end: 28
  select_instance: null
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: Reach_probability
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: null
  vnnlib_path: null
  vnnlib_path_prefix: ''
solver:
  batch_size: 8192
  no_float64_last_iter: true
  no_amp: false
  early_stop_patience: 10
  start_save_best: 2
  bound_prop_method: alpha-crown
  prune_after_crown: false
  crown:
    batch_size: 1000000000
    max_crown_size: 1000000000
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
    full_conv_alpha: true
  beta-crown:
    min_batch_size_ratio: 0.1
    lr_alpha: 0.03
    lr_beta: 0.08
    lr_decay: 0.98
    optimizer: adam
    iteration: 100
    beta: true
    beta_warmup: true
    enable_opt_interm_bounds: false
    all_node_split_LP: true
  forward:
    refine: false
    dynamic: false
    max_dim: 10000
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  multi_class:
    multi_class_method: allclass_domain
    label_batch_size: 32
    skip_with_refined_bound: true
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
    adv_warmup: true
bab:
  initial_max_domains: 1
  max_domains: .inf
  decision_thresh: 0
  timeout: 360
  timeout_scale: 1
  override_timeout: null
  get_upper_bound: false
  dfs_percent: 0.0
  pruning_in_iteration: true
  pruning_in_iteration_ratio: 0.2
  sort_targets: false
  batched_domain_list: true
  optimized_intermediate_layers: ''
  interm_transfer: true
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1.0
    iteration: 100
    bab_iteration: -1
    early_stop_patience: -1
    lr_beta: 0.02
    number_cuts: 50
    topk_cuts_in_filter: 100
    batch_size_primal: 100
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    max_num: 1000000000
    fixed_cuts: false
    patches_cut: false
    cplex_cuts: false
    cplex_cuts_wait: 0
    cplex_cuts_revpickup: true
    cut_reference_bounds: true
    fix_intermediate_bounds: false
    lr: 0.01
  branching:
    method: kfsb
    candidates: 3
    reduceop: min
    sb_coeff_thresh: 0.001
    input_split:
      enable: false
      enhanced_bound_prop_method: alpha-crown
      enhanced_branching_method: naive
      enhanced_bound_patience: 100000000.0
      attack_patience: 100000000.0
      adv_check: 0
      sort_domain_interval: -1
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: before
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
  pgd_loss_mode: null
  attack_mode: PGD
  gama_lambda: 10.0
  gama_decay: 0.9
  check_clean: false
  input_split:
    pgd_steps: 100
    pgd_restarts: 30
    pgd_alpha: auto
  input_split_enhanced:
    pgd_steps: 200
    pgd_restarts: 5000000
    pgd_alpha: auto
  input_split_check_adv:
    pgd_steps: 5
    pgd_restarts: 5
    pgd_alpha: auto
  enable_mip_attack: false
  cex_path: ./test_cex.txt
debug:
  lp_test: null

Experiments at Wed Aug 31 20:35:28 2022 on diablo.cs.ucla.edu
customized start/end sample from instance 27 to 28 in instances.csv
Internal results will be saved to a-b-crown_[instances]_start=27_end=28_iter=100_b=8192_timeout=360_branching=kfsb-min-3_lra-init=0.1_lra=0.03_lrb=0.08_PGD=before_cplex_cuts=False_initial_max_domains=1.npz.

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 27 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using onnx onnx/gcas.onnx
Using vnnlib vnnlib/gcas_3.vnnlib
Precompiled vnnlib file found at ../../vnncomp2022_benchmarks/benchmarks/reach_prob_density/vnnlib/gcas_3.vnnlib.compiled
Loading onnx ../../vnncomp2022_benchmarks/benchmarks/reach_prob_density/onnx/gcas.onnx wih quirks {}
Attack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.8551223874092102, initialization=uniform, GAMA=False
model output: tensor([[ 0.25884616, -1.29152334,  0.83221793, -0.25051379, -1.25049615,
          1.19042325, -0.26158494, -0.23011570, -0.37181473, -0.72184813,
          0.52231956, -0.08015931,  0.96334743,  0.35379049]], device='cuda:0')
pgd prediction: tensor([[[ 0.28061047,  0.23293264,  0.42544377,  0.17161581, -0.14736755,
           1.33588028,  0.51121294, -0.18002541, -0.01611795, -0.13562588,
           1.44448543,  0.68617624, -0.98067886,  0.84922451]]],
       device='cuda:0')
pgd attack margin tensor([[[ 1.30313253, -0.28061047]]], device='cuda:0')
number of violation:  1
Attack finished in 1.1112 seconds.
pgd attack failed
Total VNNLIB file length: 1, max property batch size: 1, total number of batches: 1

Properties batch 0, size 1
Remaining timeout: 596.8106150627136
##### Instance 0 first 10 spec matrices: [[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]
  [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]
thresholds: [-2.2838114  0.       ] ######
Model prediction is: tensor([ 0.25884616, -1.29152334,  0.83221793, -0.25051379, -1.25049615,
         1.19042325, -0.26158494, -0.23011570, -0.37181473, -0.72184813,
         0.52231956, -0.08015931,  0.96334743,  0.35379049], device='cuda:0')
layer /14 using full alpha with shape torch.Size([32]); unstable size 32; total size 32 (torch.Size([1, 32]))
layer /14 start_node /input.3 using full alpha with unstable size 32 total_size 32 output_shape torch.Size([32])
layer /14 start_node /17 using full alpha with unstable size None total_size 2 output_shape 2
layer /16 using full alpha with shape torch.Size([32]); unstable size 32; total size 32 (torch.Size([1, 32]))
layer /16 start_node /17 using full alpha with unstable size None total_size 2 output_shape 2
Optimizable variables initialized.
initial CROWN bounds: tensor([[-6.71143675, -0.51940560]], device='cuda:0') None
best_l after optimization: -5.713212490081787 with beta sum per layer: []
alpha/beta optimization time: 2.5976576805114746
initial alpha-CROWN bounds: tensor([[-5.29675913, -0.41645321]], device='cuda:0')
Worst class: (+ rhs) -5.296759128570557
preset mip_multi_proc as default setting: 40
Set parameter Username
Academic license - for non-commercial use only - expires 2023-08-09
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)
  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.
  "Using experimental implementation that allows 'batch_size > 1'."
mip_multi_proc: 40, mip_threads: 1, total threads used: 40
lp solver model built in 0.0232 seconds.
Keeping slopes for these layers: ['/17']
layer 0 size torch.Size([32]) unstable 32
layer 1 size torch.Size([32]) unstable 32
-----------------
# of unstable neurons: 64
-----------------

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([1, 32]) pre split depth:  9
batch:  torch.Size([1, 32]) post split depth:  9
splitting decisions: 
split level 0: [1, 30] 
split level 1: [1, 5] 
split level 2: [1, 27] 
split level 3: [1, 6] 
split level 4: [1, 1] 
split level 5: [1, 15] 
split level 6: [1, 8] 
split level 7: [1, 21] 
split level 8: [1, 14] 
regular batch size: 2*256, diving batch size 1*0
(512, 14) torch.Size([512, 2, 14]) torch.Size([512, 2])
best_l after optimization: -832.2100219726562 with beta sum per layer: [0.0, 0.17776882648468018]
alpha/beta optimization time: 0.8001129627227783
pruning_in_iteration open status: True
ratio of positive domain = 488 / 512 = 0.953125
pruning-in-iteration extra time: 0.053832054138183594
Tensors transferred: pre=0.0625M lA=0.0029M alpha=0.1250M beta=0.0044M
This batch time : update_bounds func: 0.8375	 prepare: 0.0229	 bound: 0.8008	 transfer: 0.0012	 finalize: 0.0117
Accumulated time: update_bounds func: 0.8375	 prepare: 0.0229	 bound: 0.8008	 transfer: 0.0012	 finalize: 0.0117
batch bounding time:  0.8375599384307861
Current worst splitting domains lb-rhs (depth):
-0.40459 (9), -0.40444 (9), -0.40069 (9), -0.40013 (9), -0.39924 (9), -0.39165 (9), -0.38335 (9), -0.37194 (9), -0.33968 (9), -0.24100 (9), -0.22195 (9), -0.20600 (9), -0.20422 (9), -0.19221 (9), -0.18332 (9), -0.17090 (9), -0.16465 (9), -0.12437 (9), -0.09408 (9), -0.06854 (9), 
length of domains: 24
Total time: 0.9465	 pickout: 0.0007	 decision: 0.0358	 get_bound: 0.8878	 add_domain: 0.0221
Accumulated time:	 pickout: 0.0007	 decision: 0.0358	 get_bound: 0.8878	 add_domain: 0.0221
Current (lb-rhs): -0.4045872986316681
488 domains visited
Cumulative time: 4.22416615486145

remaining dive domains: 0/-1, dive_rate:0.0
batch:  torch.Size([24, 32]) pre split depth:  5
batch:  torch.Size([24, 32]) post split depth:  5
splitting decisions: 
split level 0: [0, 26] [1, 22] [1, 22] [1, 22] [1, 22] [1, 22] [1, 22] [1, 22] [1, 22] [1, 22] 
split level 1: [1, 3] [1, 3] [1, 3] [1, 3] [1, 3] [1, 3] [1, 3] [1, 3] [1, 3] [0, 26] 
split level 2: [0, 31] [0, 9] [0, 9] [0, 9] [0, 9] [0, 26] [0, 9] [0, 9] [0, 9] [1, 3] 
split level 3: [1, 22] [0, 26] [0, 26] [0, 26] [0, 26] [0, 31] [0, 26] [0, 26] [0, 26] [0, 31] 
split level 4: [0, 9] [1, 11] [0, 31] [0, 31] [0, 31] [0, 9] [1, 11] [0, 31] [0, 31] [0, 9] 
regular batch size: 2*384, diving batch size 1*0
(768, 14) torch.Size([768, 2, 14]) torch.Size([768, 2])

all verified at 0th iter
best_l after optimization: -1098.09912109375 with beta sum per layer: [0.0, 5.688602447509766]
alpha/beta optimization time: 0.006078004837036133
pruning_in_iteration open status: False
ratio of positive domain = 768 / 768 = 1.0
pruning-in-iteration extra time: 0.00012731552124023438
Tensors transferred: pre=0.0938M lA=0.0938M alpha=0.1875M beta=0.0110M
This batch time : update_bounds func: 0.0601	 prepare: 0.0406	 bound: 0.0065	 transfer: 0.0010	 finalize: 0.0106
Accumulated time: update_bounds func: 0.8976	 prepare: 0.0635	 bound: 0.8072	 transfer: 0.0023	 finalize: 0.0224
batch bounding time:  0.0601806640625
length of domains: 0
Total time: 0.1981	 pickout: 0.0010	 decision: 0.0319	 get_bound: 0.1346	 add_domain: 0.0305
Accumulated time:	 pickout: 0.0017	 decision: 0.0678	 get_bound: 1.0224	 add_domain: 0.0527
No domains left, verification finished!
1256 domains visited
Cumulative time: 4.423187494277954

Result: safe in 7.6872 seconds
############# Summary #############
Final verified acc: 100.0% [total 1 examples]
Total verification count: 1 , total verified safe: 1 , verified unsafe: 0 , timeout: 0
mean time (bab) [total:1]: 4.497800350189209
mean time [1] 7.687226057052612 max time 7.687226057052612
safe (total 1): [0]
