Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  record_bounds: false
  sparse_alpha: true
  save_adv_example: false
  precompile_jit: false
  mode: verified-acc
  complete_verifier: bab
  enable_incomplete_verification: true
  get_crown_verified_acc: false
  csv_name: instances.csv
  results_file: null
  root_path: ../../vnncomp2022_benchmarks/benchmarks/cifar100_tinyimagenet_resnet
model:
  path: null
  cache_onnx_conversion: false
  onnx_quirks: null
  name: mnist_9_200
  onnx_path: null
  onnx_path_prefix: ''
  onnx_optimization_flags: merge_bn
data:
  start: 34
  end: 35
  select_instance: null
  num_outputs: 100
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: CIFAR100
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  norm: .inf
  epsilon: null
  vnnlib_path: null
  vnnlib_path_prefix: ''
solver:
  batch_size: 256
  no_float64_last_iter: true
  no_amp: false
  early_stop_patience: 10
  start_save_best: 2
  bound_prop_method: alpha-crown
  prune_after_crown: true
  crown:
    batch_size: 1000000000
    max_crown_size: 1000000000
  alpha-crown:
    alpha: true
    lr_alpha: 0.25
    iteration: 20
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
    full_conv_alpha: false
  beta-crown:
    min_batch_size_ratio: 0.1
    lr_alpha: 0.05
    lr_beta: 0.1
    lr_decay: 0.98
    optimizer: adam
    iteration: 5
    beta: true
    beta_warmup: true
    enable_opt_interm_bounds: false
    all_node_split_LP: false
  forward:
    refine: false
    dynamic: false
    max_dim: 10000
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
    solver_pkg: gurobi
  multi_class:
    multi_class_method: allclass_domain
    label_batch_size: 16
    skip_with_refined_bound: true
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
    adv_warmup: true
bab:
  initial_max_domains: 100
  max_domains: 200000
  decision_thresh: 0
  timeout: 360
  timeout_scale: 1
  override_timeout: null
  get_upper_bound: false
  dfs_percent: 0.0
  pruning_in_iteration: true
  pruning_in_iteration_ratio: 0.2
  sort_targets: false
  batched_domain_list: true
  optimized_intermediate_layers: ''
  interm_transfer: false
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr_decay: 1.0
    iteration: 100
    bab_iteration: -1
    early_stop_patience: -1
    lr_beta: 0.02
    number_cuts: 50
    topk_cuts_in_filter: 100
    batch_size_primal: 100
    add_implied_cuts: false
    add_input_cuts: false
    _tmp_cuts: null
    _eran_cuts: null
    max_num: 1000000000
    fixed_cuts: false
    patches_cut: false
    cplex_cuts: false
    cplex_cuts_wait: 0
    cplex_cuts_revpickup: true
    cut_reference_bounds: true
    fix_intermediate_bounds: false
    lr: 0.01
  branching:
    method: kfsb
    candidates: 7
    reduceop: max
    sb_coeff_thresh: 0.001
    input_split:
      enable: false
      enhanced_bound_prop_method: alpha-crown
      enhanced_branching_method: naive
      enhanced_bound_patience: 100000000.0
      attack_patience: 100000000.0
      adv_check: 0
      sort_domain_interval: -1
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_timeout: 30.0
    mip_start_iteration: 5
    max_dive_domains: -1
    num_dive_constraints: 50
    dive_rate: 0.2
    adv_dive: false
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: skip
  pgd_steps: 100
  pgd_restarts: 10
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
  pgd_loss_mode: null
  attack_mode: PGD
  gama_lambda: 10.0
  gama_decay: 0.9
  check_clean: false
  input_split:
    pgd_steps: 100
    pgd_restarts: 30
    pgd_alpha: auto
  input_split_enhanced:
    pgd_steps: 200
    pgd_restarts: 5000000
    pgd_alpha: auto
  input_split_check_adv:
    pgd_steps: 5
    pgd_restarts: 5
    pgd_alpha: auto
  enable_mip_attack: false
  cex_path: ./test_cex.txt
debug:
  lp_test: null

Experiments at Sun Aug 21 11:56:43 2022 on diablo.cs.ucla.edu
saving results to a-b-crown_[instances]_start=34_end=35_iter=5_b=256_timeout=360_branching=kfsb-max-7_lra-init=0.25_lra=0.05_lrb=0.1_PGD=skip_cplex_cuts=False_initial_max_domains=100.npz
customized start/end sample from 34 to 35

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 34 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using onnx onnx/CIFAR100_resnet_large.onnx
Using vnnlib vnnlib/CIFAR100_resnet_large_prop_idx_5586_sidx_80_eps_0.0039.vnnlib
Loading onnx ../../vnncomp2022_benchmarks/benchmarks/cifar100_tinyimagenet_resnet/onnx/CIFAR100_resnet_large.onnx wih quirks {}
Onnx optimization with flag merge_bn
Found existed optimized onnx model at ../../vnncomp2022_benchmarks/benchmarks/cifar100_tinyimagenet_resnet/onnx/CIFAR100_resnet_large.onnx.optimized
ConvertModel(
  (Conv_126): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_127): ReLU(inplace=True)
  (Conv_129): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_130): ReLU(inplace=True)
  (Conv_132): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Add_133): Add()
  (Conv_135): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_136): ReLU(inplace=True)
  (Conv_138): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Add_139): Add()
  (Conv_141): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (Relu_142): ReLU(inplace=True)
  (Conv_144): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Conv_146): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))
  (Add_147): Add()
  (Conv_149): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_150): ReLU(inplace=True)
  (Conv_152): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Add_153): Add()
  (Conv_155): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (Relu_156): ReLU(inplace=True)
  (Conv_158): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Conv_160): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2))
  (Add_161): Add()
  (Conv_163): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_164): ReLU(inplace=True)
  (Conv_166): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Add_167): Add()
  (Conv_169): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (Relu_170): ReLU(inplace=True)
  (Conv_172): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Conv_174): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))
  (Add_175): Add()
  (Conv_177): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Relu_178): ReLU(inplace=True)
  (Conv_180): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (Add_181): Add()
  (Flatten_182): Flatten()
  (Gemm_183): Linear(in_features=4096, out_features=100, bias=True)
  (Relu_184): ReLU(inplace=True)
  (Gemm_modelOutput): Linear(in_features=100, out_features=100, bias=True)
)
Precompiled vnnlib file found at ../../vnncomp2022_benchmarks/benchmarks/cifar100_tinyimagenet_resnet/vnnlib/CIFAR100_resnet_large_prop_idx_5586_sidx_80_eps_0.0039.vnnlib.compiled
Model prediction is: tensor([[-11.10499287,  -8.29730701,  -8.97668743,  -5.18692732,  -1.85886538,
         -10.91096306,  -6.34803009,  -3.23037910,  -6.23693323, -11.77017021,
         -10.07305813,  -8.55531025,  -6.75999641, -10.75261211,  -5.58124733,
          -6.24652767, -11.56424999, -10.03807640,  -4.39383316,  -7.81790876,
         -11.30146694,  -5.09575176,  -8.18941402, -11.28980160,  -7.99293518,
          -8.78129673,  -4.03442240,  -3.68476367, -11.38125515,  -5.02115726,
          -9.14384651,  -5.77611446,  -3.93516469,  -5.73003674,  -6.75352478,
          -8.16339779,  -7.77945471,  -7.21032524,  -4.74723244,  -9.55210781,
         -11.59322929,  -9.27762985,  -4.79605579,  -7.08118296,  -2.74563885,
          -5.28383303,  -8.83738995,  -8.27448559,  -9.70774841, -10.09659958,
          -2.16528201,  -5.05288792,  -6.74126005, -15.80838776,  -9.62883282,
          -3.75493288,  -6.43487453,  -9.17352295, -10.61621094,  -5.36170626,
         -15.24487495, -10.92763615, -10.64952946,  -1.50511122,  -3.74839807,
          -4.61512566,  -3.31671166,  -4.13531160,  -9.29153442, -12.90980339,
          -9.71146488, -14.46309566,  -4.90067148,  -9.04084301,  -0.10072792,
          -5.03283787, -13.01003551,  -4.35917568,  -3.31177974,  -4.30573368,
          -3.80682254,  -7.42843151,  -8.56064701,  -9.90867424,  -8.40246582,
          -6.14894867,  -9.39184475, -10.05353165,  -8.24860001,  -6.46777439,
          -6.38489532,  -4.58246613,  -9.82218456,  -3.07385254, -13.69301510,
         -12.57879066,  -5.37521172,  -6.70784903,  -9.04625225,  -4.60269976]],
       device='cuda:0')/home/zhouxingshi/onnx2pytorch/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.
  "Using experimental implementation that allows 'batch_size > 1'."
/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/abcrown.py:94: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_new.cpp:210.)
  arguments.Config["bab"]["decision_thresh"] = torch.tensor([item[1] for item in vnnlib[1]]).to(data)
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/utils.py:21: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return value.ndim == 0 or value.shape == torch.Size([1])
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/operations/add.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  elif all(x == 1 for x in input[0].shape):

layer /90 using sparse-features alpha with shape [1314]; unstable size 1314; total size 65536 (torch.Size([1, 64, 32, 32]))
layer /90 start_node /input.4 using sparse-spec alpha with unstable size 50 total_size 64 output_shape 64
layer /90 start_node /input.12 using sparse-spec alpha with unstable size 46 total_size 128 output_shape 128
layer /90 start_node /input.20 using sparse-spec alpha with unstable size 70 total_size 128 output_shape 128
layer /90 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /90 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /92 using sparse-features alpha with shape [2335]; unstable size 2335; total size 65536 (torch.Size([1, 64, 32, 32]))
layer /92 start_node /input.12 using sparse-spec alpha with unstable size 46 total_size 128 output_shape 128
layer /92 start_node /input.20 using sparse-spec alpha with unstable size 70 total_size 128 output_shape 128
layer /92 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /92 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /96 using sparse-features alpha with shape [0]; unstable size 0; total size 65536 (torch.Size([1, 64, 32, 32]))
layer /96 start_node /input.12 using sparse-spec alpha with unstable size 46 total_size 128 output_shape 128
layer /96 start_node /input.20 using sparse-spec alpha with unstable size 70 total_size 128 output_shape 128
layer /96 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /96 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /100 using sparse-features alpha with shape [479]; unstable size 479; total size 32768 (torch.Size([1, 128, 16, 16]))
layer /100 start_node /input.20 using sparse-spec alpha with unstable size 70 total_size 128 output_shape 128
layer /100 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /100 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /105 using sparse-features alpha with shape [0]; unstable size 0; total size 32768 (torch.Size([1, 128, 16, 16]))
layer /105 start_node /input.20 using sparse-spec alpha with unstable size 70 total_size 128 output_shape 128
layer /105 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /105 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /109 using sparse-features alpha with shape [437]; unstable size 437; total size 8192 (torch.Size([1, 128, 8, 8]))
layer /109 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /109 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /114 using sparse-features alpha with shape [0]; unstable size 0; total size 8192 (torch.Size([1, 128, 8, 8]))
layer /114 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /114 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /118 using sparse-features alpha with shape [0]; unstable size 0; total size 4096 (torch.Size([1, 256, 4, 4]))
layer /118 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /118 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /123 using sparse-features alpha with shape [0]; unstable size 0; total size 4096 (torch.Size([1, 256, 4, 4]))
layer /123 start_node /input.36 using sparse-spec alpha with unstable size 34 total_size 100 output_shape torch.Size([100])
layer /123 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
layer /128 using sparse-features alpha with shape [34]; unstable size 34; total size 100 (torch.Size([1, 100]))
layer /128 start_node /129 using full alpha with unstable size None total_size 99 output_shape 99
Optimizable variables initialized.
initial CROWN bounds: tensor([[ 8.21242428,  5.93845844,  6.16740894,  2.76408792,  0.42120016,
          7.65161133,  3.79056978,  0.98847890,  3.14286661,  8.86178112,
          6.89305115,  5.80896759,  4.14584637,  7.41907692,  3.23444605,
          3.92582655,  8.49932671,  7.08444214,  2.06343246,  5.03456497,
          7.90337467,  2.68281269,  5.35648727,  8.67234612,  5.19704723,
          6.11109304,  1.63550544,  1.70210361,  8.33361244,  2.76537752,
          6.50909376,  3.09750462,  1.97995543,  3.02694345,  4.34296608,
          5.45600510,  5.12797546,  4.51578236,  2.70450020,  6.41307735,
          8.41112041,  6.17084360,  2.58404255,  4.18732357,  0.77853227,
          2.80528069,  6.00254917,  4.93089581,  6.33630371,  7.68719149,
          0.85544062,  2.64956570,  3.57100630, 12.84461594,  7.25850964,
          2.05574965,  3.33326554,  6.58168459,  7.18362570,  2.34348392,
         12.07912064,  7.70661831,  8.08658028, -0.25881004,  1.85133386,
          2.79077959,  1.02033746,  2.05401659,  6.10633945, 10.12315941,
          6.87695503, 11.60861778,  3.10291719,  6.60178375,  2.68053508,
          9.47455978,  2.20894718,  1.00163913,  2.05721140,  1.99923992,
          4.60351944,  6.00059605,  6.88705492,  5.60451698,  2.98766875,
          6.53093815,  7.27630091,  5.35990143,  3.53848028,  3.54805708,
          2.14809823,  7.11637354,  1.09409213, 10.70286274,  9.74844170,
          2.36084509,  4.10304689,  6.41608047,  2.30988383]], device='cuda:0') None
prune_after_crown optimization in use: original label size = 99 pruned label size = 1
> /home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/beta_CROWN_solver.py(1271)build_the_model()
-> ret = self.net.compute_bounds(x=(x,), C=c_to_use, method='CROWN-Optimized',
(Pdb) 1266 	                prune_after_crown_used = True
1267 	                prune_after_crown_overhead += time.time() - stime
1268 	                print('prune_after_crown optimization in use: original label size =', final_layer_lb.shape[0], 'pruned label size =', len(unverified_label_mask))
1269 	
1270 	            import pdb; pdb.set_trace()
1271 ->	            ret = self.net.compute_bounds(x=(x,), C=c_to_use, method='CROWN-Optimized',
1272 	                return_A=self.return_A, needed_A_dict=self.needed_A_dict,
1273 	                bound_upper=False, aux_reference_bounds=aux_reference_bounds, cutter=self.cutter)
1274 	        elif bounding_method == 'alpha-forward':
1275 	            warnings.warn('alpha-forward can only be used with input split for now')
1276 	            self.net.bound_opts['optimize_bound_args']['ob_init'] = True
(Pdb) (1, 3, 32, 32)
(Pdb) torch.Size([1, 1, 100])
(Pdb) > /home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/beta_CROWN_solver.py(1272)build_the_model()
-> return_A=self.return_A, needed_A_dict=self.needed_A_dict,
(Pdb) > /home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/beta_CROWN_solver.py(1273)build_the_model()
-> bound_upper=False, aux_reference_bounds=aux_reference_bounds, cutter=self.cutter)
(Pdb) RuntimeError: CUDA out of memory. Tried to allocate 1.58 GiB (GPU 0; 10.92 GiB total capacity; 3.63 GiB already allocated; 502.44 MiB free; 6.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
> /home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/beta_CROWN_solver.py(1273)build_the_model()
-> bound_upper=False, aux_reference_bounds=aux_reference_bounds, cutter=self.cutter)
(Pdb) Traceback (most recent call last):
  File "/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/abcrown.py", line 713, in <module>
    main()
  File "/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/abcrown.py", line 614, in main
    incomplete_verifier(model_ori, x, data_ub=data_max, data_lb=data_min, vnnlib=vnnlib)
  File "/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/abcrown.py", line 108, in incomplete_verifier
    domain, x, data_lb, data_ub, vnnlib, stop_criterion_func=stop_criterion_min(arguments.Config["bab"]["decision_thresh"]))
  File "/home/zhouxingshi/gputest/CROWN-GENERAL/complete_verifier/beta_CROWN_solver.py", line 1273, in build_the_model
    bound_upper=False, aux_reference_bounds=aux_reference_bounds, cutter=self.cutter)
  File "/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/bdb.py", line 94, in trace_dispatch
    return self.dispatch_exception(frame, arg)
  File "/home/zhouxingshi/miniconda3/envs/alpha-beta-crown/lib/python3.7/bdb.py", line 174, in dispatch_exception
    if self.quitting: raise BdbQuit
bdb.BdbQuit
